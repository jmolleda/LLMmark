{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83a55c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching traces from project: LLMmark_determinism...\n",
      "Processing trace 1/22: run_022_gemma3:4b (0197ac8a-3214-7e13-8f27-903669ebe657)\n",
      "Processing trace 2/22: run_021_gemma3:4b (0197ac6b-ee77-7c4d-9b00-e26903d0972f)\n",
      "Processing trace 3/22: run_020_gemma3:4b (0197ac13-5ead-7983-89c4-848884a0a638)\n",
      "Processing trace 4/22: run_019_gemma3:4b (0197abf5-33af-73f1-be94-745b090979ec)\n",
      "Processing trace 5/22: run_018_gemma3:4b (0197ab9e-bb3f-70b2-8d80-c5d54967ff91)\n",
      "Processing trace 6/22: run_017_gemma3:4b (0197ab7e-bc2e-7ae9-b8ed-c3dea5c573bb)\n",
      "Processing trace 7/22: run_016_gemma3:4b (0197ab28-87c0-78af-880d-0da76d879b38)\n",
      "Processing trace 8/22: run_015_gemma3:4b (0197ab03-26c6-7f62-ae59-21e06ba7501d)\n",
      "Processing trace 9/22: run_014_gemma3:4b (0197aaa2-4a01-7eef-a7de-f1089b297d2d)\n",
      "Processing trace 10/22: run_013_gemma3:4b (0197aa8d-f3a0-74c1-8297-5f56524dd6eb)\n",
      "Processing trace 11/22: run_012_gemma3:4b (0197aa10-6b4b-75f9-b254-819978f0d479)\n",
      "Processing trace 12/22: run_011_gemma3:4b (0197aa0f-7476-7e74-8c8b-7f9c882f81ec)\n",
      "Processing trace 13/22: run_010_gemma3:4b (0197a9ac-9449-732c-bd84-74efec6c1435)\n",
      "Processing trace 14/22: run_009_gemma3:4b (0197a98e-fb60-794e-8172-b1f0c8de1449)\n",
      "Processing trace 15/22: run_008_gemma3:4b (0197a939-3cfd-7935-97a6-246a687a6c13)\n",
      "Processing trace 16/22: run_007_gemma3:4b (0197a8f1-c2af-75b2-bb8d-abf765481b48)\n",
      "Processing trace 17/22: run_006_gemma3:4b (0197a8ce-c5e6-7f74-8f15-2ba0b420ab45)\n",
      "Processing trace 18/22: run_005_gemma3:4b (0197a85f-099e-722a-8483-333c714054e4)\n",
      "Processing trace 19/22: run_004_gemma3:4b (0197a852-7a7a-7a18-a092-f9c9467124c8)\n",
      "Processing trace 20/22: run_003_gemma3:4b (0197a7e5-1005-74c9-8cb6-635b3d3185b4)\n",
      "Processing trace 21/22: run_002_gemma3:4b (0197a7d8-0ed3-727f-b572-c35cd4cf519f)\n",
      "Processing trace 22/22: run_001_gemma3:4b (0197a790-f02a-7019-a786-2df1d42ae2a5)\n",
      "\n",
      "Successfully extracted data and saved to ../../../data/determinism/opik_determinism_data.csv\n",
      "DataFrame head:\n",
      "                                span_id span_name  response_time_ms  \\\n",
      "0  0197ac92-a79f-7eaf-b555-e0885a24a8e0   q10_r10            20.149   \n",
      "1  0197ac92-58ea-79ac-922b-a48e7d35dc7e    q10_r9            20.280   \n",
      "2  0197ac92-09b2-7eb6-8f52-bc59395e7e66    q10_r8            20.324   \n",
      "3  0197ac91-ba4e-7a6c-9d0f-93ef3128a740    q10_r7            19.998   \n",
      "4  0197ac91-6c30-7eeb-896d-da3be9c463ca    q10_r6            20.029   \n",
      "\n",
      "                                 span_input_question span_output_answer  \\\n",
      "0  What is the sum of the numbers 111001011b and ...                [f]   \n",
      "1  What is the sum of the numbers 111001011b and ...                [f]   \n",
      "2  What is the sum of the numbers 111001011b and ...                [f]   \n",
      "3  What is the sum of the numbers 111001011b and ...                [f]   \n",
      "4  What is the sum of the numbers 111001011b and ...                [f]   \n",
      "\n",
      "                              span_output_raw_answer span_correct_answer  \\\n",
      "0  [[ 111001011b and 101111010b - What is the sum...                   c   \n",
      "1  [[ 111001011b and 101111010b - What is the sum...                   c   \n",
      "2  [[ 111001011b and 101111010b - What is the sum...                   c   \n",
      "3  [[ 111001011b and 101111010b - What is the sum...                   c   \n",
      "4  [[ 111001011b and 101111010b - What is the sum...                   c   \n",
      "\n",
      "     question_file                              trace_id           run_name  \\\n",
      "0  question_10.txt  0197ac8a-3214-7e13-8f27-903669ebe657  run_022_gemma3:4b   \n",
      "1  question_10.txt  0197ac8a-3214-7e13-8f27-903669ebe657  run_022_gemma3:4b   \n",
      "2  question_10.txt  0197ac8a-3214-7e13-8f27-903669ebe657  run_022_gemma3:4b   \n",
      "3  question_10.txt  0197ac8a-3214-7e13-8f27-903669ebe657  run_022_gemma3:4b   \n",
      "4  question_10.txt  0197ac8a-3214-7e13-8f27-903669ebe657  run_022_gemma3:4b   \n",
      "\n",
      "  model_display_name language prompting_tech  num_runs_per_question  \\\n",
      "0  run_022_gemma3:4b       en             R4                     10   \n",
      "1  run_022_gemma3:4b       en             R4                     10   \n",
      "2  run_022_gemma3:4b       en             R4                     10   \n",
      "3  run_022_gemma3:4b       en             R4                     10   \n",
      "4  run_022_gemma3:4b       en             R4                     10   \n",
      "\n",
      "  model_source  temperature  top_p    exercise    question_type  \n",
      "0        local          0.0    0.1  exam_01_mc  multiple_choice  \n",
      "1        local          0.0    0.1  exam_01_mc  multiple_choice  \n",
      "2        local          0.0    0.1  exam_01_mc  multiple_choice  \n",
      "3        local          0.0    0.1  exam_01_mc  multiple_choice  \n",
      "4        local          0.0    0.1  exam_01_mc  multiple_choice  \n",
      "\n",
      "DataFrame shape: (2200, 19)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "TARGET_MODEL_ID_SUBSTRING = \"gemma3:4b\"\n",
    "\n",
    "try:\n",
    "    import opik\n",
    "    from opik import Opik\n",
    "except ImportError:\n",
    "    print(\"Opik SDK not found. Please install it: pip install opik\")\n",
    "    exit()\n",
    "\n",
    "def parse_model_info(model_name: str, tags: List[str]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Parses model ID, display name, and size from the model name and tags.\n",
    "    Adjust this function based on your actual model naming conventions.\n",
    "    \"\"\"\n",
    "    model_id = model_name\n",
    "    model_display_name = model_name\n",
    "    model_size = \"N/A\"\n",
    "    question_type = \"N/A\"\n",
    "\n",
    "    match = re.search(r':([\\d\\.]+)b', model_name)\n",
    "    if match:\n",
    "        model_size = match.group(1) + \"B\"\n",
    "\n",
    "    if \"multiple_choice\" in tags:\n",
    "        question_type = \"multiple_choice\"\n",
    "    elif \"open_answer\" in tags:\n",
    "        question_type = \"open_answer\"\n",
    "\n",
    "    return {\n",
    "        \"model_id\": model_id,\n",
    "        \"model_display_name\": model_display_name,\n",
    "        \"model_size\": model_size,\n",
    "        \"question_type\": question_type\n",
    "    }\n",
    "\n",
    "def get_opik_flat_data_for_csv(project_name: str = \"LLMmark_determinism\") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Fetches detailed trace and span data from Opik and flattens it for CSV export.\n",
    "    Each dictionary in the returned list represents a single span,\n",
    "    including its parent trace's metadata.\n",
    "    \"\"\"\n",
    "    client = Opik()\n",
    "    flat_data = []\n",
    "\n",
    "    print(f\"Fetching traces from project: {project_name}...\")\n",
    "\n",
    "    traces = client.search_traces(\n",
    "        project_name=project_name,\n",
    "        filter_string=f'metadata.model_id contains \"{TARGET_MODEL_ID_SUBSTRING}\"',\n",
    "        max_results=1000\n",
    "    )\n",
    "\n",
    "    if not traces:\n",
    "        print(f\"No traces found in project '{project_name}'. Please check the project name and your Opik configuration.\")\n",
    "        return []\n",
    "\n",
    "    for i, trace in enumerate(traces):\n",
    "        print(f\"Processing trace {i+1}/{len(traces)}: {trace.name} ({trace.id})\")\n",
    "\n",
    "        trace_content = client.get_trace_content(trace.id)\n",
    "        spans = client.search_spans(project_name=project_name, trace_id=trace.id)\n",
    "\n",
    "        if not spans:\n",
    "            print(f\"  No spans found for trace {trace.id}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        model_info = parse_model_info(trace.name, trace.tags)\n",
    "        \n",
    "        model_source = \"N/A\"\n",
    "        if \"local\" in trace.tags:\n",
    "            model_source = \"local\"\n",
    "        elif \"online\" in trace.tags:\n",
    "            model_source = \"online\"\n",
    "\n",
    "        trace_flat_metadata = {\n",
    "            \"trace_id\": trace.id,\n",
    "            \"run_name\": trace.name,\n",
    "            # \"trace_llm_span_count\": trace.llm_span_count,\n",
    "            # \"trace_comments\": trace.comments, # Mantenido para traza, si se desea\n",
    "            # \"trace_model_id\": model_info[\"model_id\"],\n",
    "            \"model_display_name\": model_info[\"model_display_name\"],\n",
    "            \"language\": trace_content.metadata.get(\"language\", \"en\"),\n",
    "            \"prompting_tech\": trace_content.metadata.get(\"prompting_tech\", \"N/A\"),\n",
    "            \"num_runs_per_question\": trace_content.metadata.get(\"num_runs_per_question\", 1),\n",
    "            \"model_source\": model_source,\n",
    "            \"temperature\": trace_content.metadata.get(\"temperature\", \"N/A\"),\n",
    "            \"top_p\": 0.1,\n",
    "            \"exercise\": trace_content.metadata.get(\"exercise\", \"N/A\"),\n",
    "            \"question_type\": model_info[\"question_type\"],\n",
    "            **{f\"trace_meta_{k}\": v for k, v in trace_content.metadata.items()\n",
    "               if k not in [\"language\", \"prompting_tech\", \"num_runs_per_question\", \n",
    "                             \"model_source\", \"temperature\", \"top_p\", \"exercise\", \n",
    "                             \"prompt_tech\", \"question_type\", \"comments\", \"model_id\", \"model_display_name\", \"top-p\", \"run_name\"]}\n",
    "        }\n",
    "\n",
    "        # Process each span and combine with trace-level metadata\n",
    "        for j, span in enumerate(spans):\n",
    "            span_response_time_ms = span.output.get(\"response_time (s)\", \"N/A\")\n",
    "\n",
    "            span_input_question = span.input.get(\"question\", str(span.input)) if isinstance(span.input, dict) else str(span.input)\n",
    "            span_output_answer = span.output.get(\"answer\", str(span.output)) if isinstance(span.output, dict) else str(span.output)\n",
    "            span_output_raw_answer = span.output.get(\"raw_answer\", span_output_answer) if isinstance(span.output, dict) else span_output_answer\n",
    "\n",
    "            correct_answer = span.metadata.get(\"correct_answer\", \"PLACEHOLDER_CORRECT_ANSWER\")\n",
    "\n",
    "            span_data_row = {\n",
    "                \"span_id\": span.id,\n",
    "                \"span_name\": span.name,\n",
    "                # \"span_type\": span.type,\n",
    "                # \"span_start_time\": span.start_time,\n",
    "                # \"span_end_time\": span.end_time,\n",
    "                \"response_time_ms\": span_response_time_ms,\n",
    "                \"span_input_question\": span_input_question,\n",
    "                \"span_output_answer\": span_output_answer,\n",
    "                \"span_output_raw_answer\": span_output_raw_answer,\n",
    "                \"span_correct_answer\": correct_answer,\n",
    "                \"question_file\": span.metadata.get(\"question_file\", \"N/A\"),\n",
    "            }\n",
    "            \n",
    "            combined_row = {**span_data_row, **trace_flat_metadata}\n",
    "            flat_data.append(combined_row)\n",
    "\n",
    "    return flat_data\n",
    "\n",
    "my_project_name = \"LLMmark_determinism\" \n",
    "\n",
    "all_flat_data = []\n",
    "all_flat_data.extend(get_opik_flat_data_for_csv(project_name=my_project_name))\n",
    "\n",
    "if all_flat_data:\n",
    "    df = pd.DataFrame(all_flat_data)\n",
    "    output_filename = \"opik_determinism_data.csv\"\n",
    "    output_dir = '../../../data/determinism'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "    \n",
    "    df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "\n",
    "    print(f\"\\nSuccessfully extracted data and saved to {output_path}\")\n",
    "    print(\"DataFrame head:\")\n",
    "    print(df.head())\n",
    "    print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "else:\n",
    "    print(\"\\nNo data extracted to create a CSV file.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
