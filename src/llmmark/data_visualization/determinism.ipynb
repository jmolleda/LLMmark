{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b68a26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Any, List, Optional, Callable\n",
    "from opik import Opik\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "SENTENCE_TRANSFORMER_MODEL = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "OPIK_DETERMINISM_PROJECT_NAME = \"LLMmark_determinism\"\n",
    "\n",
    "EMPTY_PLACEHOLDER = '_EMPTY_ANSWER_FAILURE_'\n",
    "TIMEOUT_PLACEHOLDER = '_TIMEOUT_FAILURE_'\n",
    "OVERTHINK_PLACEHOLDER = '_OVERTHINK_FAILURE_'\n",
    "BAD_FORMAT_PLACEHOLDER = '_BAD_FORMAT_FAILURE_'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c83a55c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_identify_failures(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Identifies failed responses in the DataFrame based on timeout or token limit.\n",
    "    \"\"\"\n",
    "    timeout = 120\n",
    "    \n",
    "    is_empty = df['span_output_raw_answer'].fillna('').str.strip() == ''\n",
    "    \n",
    "    # It is considered timeout if the response time is > 120 seconds and the answer is empty\n",
    "    # Some models like gemma3n can take more than 120 seconds because of the memory limit (we use swap memory)\n",
    "    is_timeout = (df['response_time'] >= timeout).fillna(False) & is_empty\n",
    "    \n",
    "    \n",
    "    # If the raw_answer contains <think>, we consider that it has reached the token limit or an infinite loop\n",
    "    is_token_limit_hit = df['span_output_raw_answer'].str.contains('<think>', na=False)\n",
    "    \n",
    "    df['is_failure'] = is_timeout | is_token_limit_hit | is_empty\n",
    "    \n",
    "    conditions = [\n",
    "        is_timeout,\n",
    "        is_token_limit_hit,\n",
    "        is_empty\n",
    "    ]\n",
    "    \n",
    "    placeholders = [\n",
    "        TIMEOUT_PLACEHOLDER,\n",
    "        OVERTHINK_PLACEHOLDER,\n",
    "        EMPTY_PLACEHOLDER\n",
    "    ]\n",
    "    \n",
    "    # Apply the failure placeholders in the answer column, else keep the original raw answer\n",
    "    df['answer'] = np.select(conditions, placeholders, default=df['span_output_raw_answer'])\n",
    "\n",
    "    failure_count = df['is_failure'].sum()\n",
    "    if failure_count > 0:\n",
    "        print(f\"INFO: Total of {failure_count} failed responses (timeout, empty answer or infinite loop).\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def parse_model_info(model_name: str, tags: List[str]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Parses model ID, display name, and size from the model name and tags.\n",
    "    Adjust this function based on your actual model naming conventions.\n",
    "    \"\"\"\n",
    "    model_id = model_name\n",
    "    model_size = \"N/A\"\n",
    "    question_type = \"N/A\"\n",
    "\n",
    "    match = re.search(r':([\\d\\.]+)b', model_name)\n",
    "    if match:\n",
    "        model_size = match.group(1) + \"B\"\n",
    "\n",
    "    if \"multiple_choice\" in tags:\n",
    "        question_type = \"multiple_choice\"\n",
    "    elif \"open_answer\" in tags:\n",
    "        question_type = \"open_answer\"\n",
    "\n",
    "    return {\n",
    "        \"model_id\": model_id,\n",
    "        \"model_size\": model_size,\n",
    "        \"question_type\": question_type\n",
    "    }\n",
    "\n",
    "def get_opik_flat_data_for_csv(project_name: str = OPIK_DETERMINISM_PROJECT_NAME) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Fetches detailed trace and span data from Opik and flattens it for CSV export.\n",
    "    Each dictionary in the returned list represents a single span,\n",
    "    including its parent trace's metadata.\n",
    "    \"\"\"\n",
    "    client = Opik()\n",
    "    flat_data = []\n",
    "\n",
    "    print(f\"Fetching traces from project: {project_name}...\")\n",
    "\n",
    "    traces = client.search_traces(\n",
    "        project_name=project_name,\n",
    "        max_results=25000\n",
    "    )\n",
    "\n",
    "    if not traces:\n",
    "        print(f\"No traces found in project '{project_name}'. Please check the project name and your Opik configuration.\")\n",
    "        return []\n",
    "    \n",
    "    # Delete traces with None values\n",
    "    traces = [trace for trace in traces if trace.name is not None]\n",
    "    \n",
    "    # :TODO: REMOVE: - Delete traces with gemma3n model\n",
    "    traces = [trace for trace in traces if 'gemma3n' not in trace.name]\n",
    "\n",
    "    for i, trace in enumerate(traces):\n",
    "        \n",
    "        print(f\"Processing trace {i+1}/{len(traces)}: {trace.name} ({trace.id})\")\n",
    "\n",
    "        trace_content = client.get_trace_content(trace.id)\n",
    "        spans = client.search_spans(project_name=project_name, trace_id=trace.id)\n",
    "\n",
    "        if not spans:\n",
    "            print(f\"  No spans found for trace {trace.id}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        model_info = parse_model_info(trace.name, trace.tags)\n",
    "        \n",
    "        model_source = \"N/A\"\n",
    "        if \"local\" in trace.tags:\n",
    "            model_source = \"local\"\n",
    "        elif \"online\" in trace.tags:\n",
    "            model_source = \"online\"\n",
    "\n",
    "        trace_flat_metadata = {\n",
    "            \"trace_id\": trace.id,\n",
    "            \"run_name\": trace.name,\n",
    "            \"model_display_name\": trace_content.metadata.get(\"model_display_name\"),\n",
    "            \"language\": trace_content.metadata.get(\"language\", \"en\"),\n",
    "            \"prompting_tech\": trace_content.metadata.get(\"prompting_tech\", \"N/A\"),\n",
    "            \"num_runs_per_question\": trace_content.metadata.get(\"num_runs_per_question\", 1),\n",
    "            \"model_source\": model_source,\n",
    "            \"temperature\": trace_content.metadata.get(\"temperature\", \"N/A\"),\n",
    "            \"top_p\": trace_content.metadata.get(\"top_p\", 0.1),\n",
    "            \"exercise\": trace_content.metadata.get(\"exercise\", \"N/A\"),\n",
    "            \"question_type\": model_info[\"question_type\"],\n",
    "            **{f\"trace_meta_{k.replace('.', '_')}\": v for k, v in trace_content.metadata.items() # Replace '.' in keys for valid column names\n",
    "            if k not in [\"language\", \"prompting_tech\", \"num_runs_per_question\", \n",
    "                         \"model_source\", \"temperature\", \"top_p\", \"exercise\", \n",
    "                         \"prompt_tech\", \"question_type\", \"comments\", \"model_id\", \"model_display_name\", \"top-p\", \"run_name\"]}\n",
    "        }\n",
    "\n",
    "        # Process each span and combine with trace-level metadata\n",
    "        for j, span in enumerate(spans):\n",
    "            span_response_time = span.output.get(\"response_time (s)\", \"N/A\")\n",
    "\n",
    "            span_input_question = span.input.get(\"question\", str(span.input)) if isinstance(span.input, dict) else str(span.input)\n",
    "            span_output_answer = span.output.get(\"answer\", str(span.output)) if isinstance(span.output, dict) else str(span.output)\n",
    "            span_output_raw_answer = span.output.get(\"raw_answer\", span_output_answer) if isinstance(span.output, dict) else span_output_answer\n",
    "\n",
    "            correct_answer = span.metadata.get(\"correct_answer\", \"PLACEHOLDER_CORRECT_ANSWER\")\n",
    "            \n",
    "            \n",
    "            span_usage = span.usage or {} # Usamos un diccionario vacÃ­o si 'usage' es None\n",
    "            completion_tokens = span_usage.get(\"completion_tokens\", 0)\n",
    "\n",
    "            span_data_row = {\n",
    "                \"span_id\": span.id,\n",
    "                \"span_name\": span.name,\n",
    "                \"response_time\": span_response_time,\n",
    "                \"span_input_question\": span_input_question,\n",
    "                \"span_output_answer\": span_output_answer,\n",
    "                \"span_output_raw_answer\": span_output_raw_answer,\n",
    "                \"span_correct_answer\": correct_answer,\n",
    "                \"completion_tokens\": completion_tokens,\n",
    "                \"question_file\": span.metadata.get(\"question_file\", \"N/A\"),\n",
    "            }\n",
    "            \n",
    "            combined_row = {**span_data_row, **trace_flat_metadata}\n",
    "            flat_data.append(combined_row)\n",
    "\n",
    "    return flat_data\n",
    "\n",
    "def filter_and_save_dataframe(\n",
    "    df: pd.DataFrame, \n",
    "    csv_filename: str = \"opik_determinism_data.csv\", \n",
    "    temperature_filter: Optional[float] = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Filters an existing DataFrame by temperature and saves it to a CSV file.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        print(\"\\nInput DataFrame is empty. No CSV file will be created.\")\n",
    "        return\n",
    "\n",
    "    filtered_df = df.copy()\n",
    "    \n",
    "    print(\"DataFrame after preprocessing:\")\n",
    "    print(filtered_df.head())\n",
    "\n",
    "    if temperature_filter is not None:\n",
    "        filtered_df['temperature'] = pd.to_numeric(filtered_df['temperature'], errors='coerce')\n",
    "        filtered_df = filtered_df[filtered_df['temperature'] == temperature_filter].copy()\n",
    "        print(f\"\\nFiltered DataFrame for temperature = {temperature_filter}:\")\n",
    "\n",
    "    if filtered_df.empty:\n",
    "        print(f\"No data after filtering for temperature = {temperature_filter}.\")\n",
    "        return\n",
    "\n",
    "    output_filename = csv_filename\n",
    "    output_dir = '../../../data/determinism'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "    \n",
    "    filtered_df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(\"DataFrame head:\")\n",
    "    print(filtered_df.head())\n",
    "    print(f\"\\nDataFrame shape: {filtered_df.shape}\")\n",
    "\n",
    "    print(f\"\\nSuccessfully extracted data and saved to {output_path}\")\n",
    "      \n",
    "def get_dataframe_from_csv(csv_filename: str = \"opik_determinism_data.csv\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads a CSV file into a DataFrame.\n",
    "    \"\"\"\n",
    "    output_dir = '../../../data/determinism'\n",
    "    output_path = os.path.join(output_dir, csv_filename)\n",
    "    \n",
    "    if not os.path.exists(output_path):\n",
    "        print(f\"CSV file {output_path} does not exist.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = pd.read_csv(output_path, encoding='utf-8')\n",
    "    print(f\"DataFrame loaded from {output_path} with shape: {df.shape}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Analyse multiple_choice answers\n",
    "def calculate_determinism_mc(answers):\n",
    "    \"\"\"Calculates the determinism of a list of answers (multiple choice).\n",
    "\n",
    "    Args:\n",
    "        answers (list): A list of answers to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        float: The number of most frequent answer.\n",
    "    \"\"\"\n",
    "    unique_answers = set(answers)\n",
    "\n",
    "    if len(unique_answers) == 1:\n",
    "        return len(answers)  # Completely deterministic\n",
    "    else:\n",
    "        # Calculate the proportion of the most frequent answer\n",
    "        counter = Counter(answers)\n",
    "        most_frequent_answer = counter.most_common(1)[0][1]\n",
    "        return most_frequent_answer\n",
    "\n",
    "def extract_answer_letter_mc(answer_text: str) -> Optional[str]: \n",
    "    \"\"\"\n",
    "    Extracts the answer letter from the given text.\n",
    "    The expected format is: [a]\n",
    "    If multiple bracketed letters follow, only the first one is returned.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Match the pattern [letter]\n",
    "        match = re.search(r'\\[([a-zA-Z])\\]', answer_text)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        # print(f\"Error extracting answer letter: {e}\")\n",
    "        return None\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Callable, Dict, Any\n",
    "\n",
    "def process_determinism_and_store(df: pd.DataFrame, determinism_function: Callable[[List[str]], float]) -> pd.DataFrame:\n",
    "    \"\"\"Processes the DataFrame to calculate determinism scores and store the results.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing response data.\n",
    "        determinism_function (Callable[[List[str]], float]): The function to use for calculating determinism\n",
    "                                                             (e.g., calculate_determinism_mc or calculate_determinism_oa).\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the input DataFrame is missing required columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame summarizing the determinism results.\n",
    "    \"\"\"\n",
    "\n",
    "    results_list = []\n",
    "\n",
    "    required_columns = ['model_display_name', 'question_file', 'prompting_tech', 'answer', 'is_failure']\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        missing = [col for col in required_columns if col not in df.columns]\n",
    "        raise ValueError(f\"The DataFrame does not have the columns: {required_columns}. Missing: {missing}\")\n",
    "\n",
    "    grouping_keys = ['model_display_name', 'question_file', 'prompting_tech']\n",
    "    df_grouped = df.groupby(grouping_keys)\n",
    "\n",
    "    for group_id, group in df_grouped:        \n",
    "        all_answers = group['answer'].tolist()\n",
    "        successful_answers = group.loc[~group['is_failure'], 'answer'].tolist()\n",
    "        \n",
    "        num_total_runs = len(group)\n",
    "        \n",
    "        # get number of is_failures with value True\n",
    "        num_failures = group['is_failure'].value_counts().get(True, 0)\n",
    "        \n",
    "        num_successful = num_total_runs - num_failures\n",
    "        \n",
    "        if (num_successful == 0):\n",
    "            print(f\"No successful runs for group {group_id}\")\n",
    "            determinism = 0.0\n",
    "        else:\n",
    "            determinism_score = determinism_function(successful_answers)\n",
    "            determinism = round((determinism_score / num_successful), 1)\n",
    "\n",
    "\n",
    "        results_list.append({\n",
    "            'Model Display Name': group_id[0],\n",
    "            'Question File': group_id[1],\n",
    "            'Prompting Tech': group_id[2],\n",
    "            'Determinism Score': determinism,\n",
    "            'Number of Runs': num_total_runs,\n",
    "            'Number of Failures': num_failures,\n",
    "            'All Answers': all_answers,\n",
    "        })\n",
    "        \n",
    "    if not results_list:\n",
    "        print(\"No determinism results found. The input DataFrame may be empty or not contain valid data.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "\n",
    "    results_df = results_df.sort_values(by=['Model Display Name', 'Question File', 'Prompting Tech']).reset_index(drop=True)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def calculate_determinism_oa(answers: List[str], umbral=0.8) -> float:\n",
    "    \"\"\"Calculate the determinism of open answer questions.\n",
    "\n",
    "    Args:\n",
    "        answers (List[str]): The list of answers to evaluate.\n",
    "        umbral (float, optional): The similarity threshold for considering answers as similar. Defaults to 0.8.\n",
    "\n",
    "    Returns:\n",
    "        float: The determinism score between 0 and 1.\n",
    "    \"\"\"\n",
    "    if not answers:\n",
    "        return 0.0  # No answers, no determinism\n",
    "\n",
    "    cleaned_answers = [str(ans) if pd.notna(ans) else '' for ans in answers]\n",
    "    \n",
    "    # Remove failure answers\n",
    "    cleaned_answers = [ans for ans in cleaned_answers if ans != '_GENERATION_FAILURE_']\n",
    "    \n",
    "    if len(cleaned_answers) < 2:\n",
    "        return 0.0\n",
    "     \n",
    "    embeddings = SENTENCE_TRANSFORMER_MODEL.encode(cleaned_answers)\n",
    "\n",
    "    similarities = []\n",
    "    for i in range(len(cleaned_answers)):\n",
    "        for j in range(i+1, len(cleaned_answers)):\n",
    "            sim = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]\n",
    "            similarities.append(sim)\n",
    "\n",
    "    return np.mean(similarities) if similarities else 0.0\n",
    "\n",
    "def create_determinism_table(df, filename=\"determinism_summary_table.csv\", function=calculate_determinism_oa):\n",
    "    \"\"\"Creates a summary table of determinism scores and saves it to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing LLM experiment results.\n",
    "        filename (str, optional): The name of the output CSV file. Defaults to \"determinism_summary_table.csv\".\n",
    "    \"\"\"\n",
    "    print(f\"\\nColumns in {df}:\")\n",
    "    print(df.columns.tolist())\n",
    "    \n",
    "    df = df.sort_values(by=['model_display_name', 'question_file', 'prompting_tech'])\n",
    "\n",
    "\n",
    "    determinism_table = process_determinism_and_store(df, function)\n",
    "\n",
    "    print(\"\\n--- Determinism Results Table ---\")\n",
    "    print(determinism_table)\n",
    "    \n",
    "    # Save the determinism table to a CSV file\n",
    "    output_dir_tables = '../../../data/determinism_tables'\n",
    "    os.makedirs(output_dir_tables, exist_ok=True)\n",
    "    table_filepath = os.path.join(output_dir_tables, filename)\n",
    "    determinism_table.to_csv(table_filepath, index=False)\n",
    "    print(f\"\\nDeterminism table saved to: {table_filepath}\")\n",
    "    \n",
    "\n",
    "\n",
    "def generate_determinism_table_mc(mc_df, filename):\n",
    "    \"\"\"Generates a determinism table for multiple choice questions.\n",
    "\n",
    "    Args:\n",
    "        mc_df (pd.DataFrame): DataFrame containing multiple choice question data.\n",
    "        filename (str): Name of the output CSV file.\n",
    "    \"\"\"\n",
    "    # Filter by question_type\n",
    "    mc_df = mc_df[mc_df['question_type'] == 'multiple_choice'].copy()\n",
    "\n",
    "    # Extract the answer letter from multiple choice answers only if they do not is_failure=True    \n",
    "    mc_df['answer'] = mc_df['span_output_answer'].apply(extract_answer_letter_mc)\n",
    "\n",
    "    # Rename span_correct_answer to correct_answer\n",
    "    mc_df.rename(columns={'span_correct_answer': 'correct_answer'}, inplace=True)\n",
    "    # Drop all columns except run_name, question_file, answer, span_correct_answer, promtpting_tech\n",
    "    mc_df = mc_df[['run_name', 'model_display_name', 'question_file', 'answer', 'correct_answer', 'prompting_tech', 'is_failure']]\n",
    "\n",
    "    # Order by question_file\n",
    "    mc_df = mc_df.sort_values(by=['question_file', 'answer']).reset_index(drop=True)\n",
    "\n",
    "    mc_df.head(5).style.set_table_styles(\n",
    "        [{'selector': 'th', 'props': [('background-color', '#f2f2f2'), ('color', 'black')]}]\n",
    "    ).set_properties(**{'text-align': 'center'})\n",
    "    \n",
    "    \n",
    "    # Get unique model_display_name values\n",
    "    unique_models = mc_df['model_display_name'].unique()\n",
    "    print(\"MODELS: \", unique_models)\n",
    "    \n",
    "    # Create CSV determinism table\n",
    "    create_determinism_table(mc_df, filename=filename, function=calculate_determinism_mc)\n",
    "\n",
    "\n",
    "def generate_determinism_table_oa(mc_df, filename):\n",
    "    \"\"\"Generates a determinism table for open answer questions.\n",
    "\n",
    "    Args:\n",
    "        mc_df (pd.DataFrame): DataFrame containing open answer question data.\n",
    "        filename (str): Name of the output CSV file.\n",
    "    \"\"\"\n",
    "    # Filter by question_type\n",
    "    mc_df = mc_df[mc_df['question_type'] == 'open_answer'].copy()\n",
    "\n",
    "    #Extract the answer letter from multiple choice answers\n",
    "    mc_df['answer'] = mc_df['span_output_answer']\n",
    "\n",
    "    # Rename span_correct_answer to correct_answer\n",
    "    mc_df.rename(columns={'span_correct_answer': 'correct_answer'}, inplace=True)\n",
    "    # Drop all columns except run_name, question_file, answer, span_correct_answer, promtpting_tech\n",
    "    mc_df = mc_df[['run_name', 'model_display_name', 'question_file', 'answer', 'correct_answer', 'prompting_tech', 'is_failure']]\n",
    "\n",
    "    # Order by question_file\n",
    "    mc_df = mc_df.sort_values(by=['question_file', 'answer']).reset_index(drop=True)\n",
    "\n",
    "    mc_df.head(5).style.set_table_styles(\n",
    "        [{'selector': 'th', 'props': [('background-color', '#f2f2f2'), ('color', 'black')]}]\n",
    "    ).set_properties(**{'text-align': 'center'})\n",
    "    \n",
    "    \n",
    "    # Get unique model_display_name values\n",
    "    unique_models = mc_df['model_display_name'].unique()\n",
    "    print(\"MODELS: \", unique_models)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Create CSV determinism table\n",
    "    create_determinism_table(mc_df, filename=filename, function=calculate_determinism_oa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bad62bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching traces from project: LLMmark_determinism...\n",
      "Processing trace 1/240: run_049_qwen3:4b_0.2 (0197c174-ebcd-7404-97e0-3ba8fa383e50)\n",
      "Processing trace 2/240: run_049_qwen3:1.7b_0.2 (0197c16b-ebc8-7aa4-b737-d650e97f8916)\n",
      "Processing trace 3/240: run_049_qwen3:0.6b_0.2 (0197c15a-d25e-7881-a4db-21d293f536fe)\n",
      "Processing trace 4/240: run_048_qwen3:4b_0.2 (0197c14f-4be2-7d1e-86e4-743f7c49f93f)\n",
      "Processing trace 5/240: run_049_tinyllama:1.1b_0.2 (0197c14e-44f1-71ef-a8ab-255f8f776f5a)\n",
      "Processing trace 6/240: run_049_smollm2:1.7b_0.2 (0197c14a-40af-7385-a226-2c9b9ed9d809)\n",
      "Processing trace 7/240: run_049_moondream:1.8b_0.2 (0197c14a-309e-7ec8-a55c-c27e6b230250)\n",
      "Processing trace 8/240: run_049_llama3.2:1b_0.2 (0197c147-32a1-76dc-b04c-d006dbd74b8b)\n",
      "Processing trace 9/240: run_048_qwen3:1.7b_0.2 (0197c13e-b877-729a-b73f-c6d6fd4cd48c)\n",
      "Processing trace 10/240: run_048_qwen3:0.6b_0.2 (0197c134-c8ae-7c68-a166-45961d80acd5)\n",
      "Processing trace 11/240: run_049_gemma3:4b_0.2 (0197c134-4202-76ce-9b06-2429467edb3a)\n",
      "Processing trace 12/240: run_048_tinyllama:1.1b_0.2 (0197c12e-2454-7390-9af5-65f1993273e0)\n",
      "Processing trace 13/240: run_049_gemma3:1b_0.2 (0197c12a-cee7-74bc-a40d-d36a5669c068)\n",
      "Processing trace 14/240: run_048_smollm2:1.7b_0.2 (0197c12a-7535-7c68-b46f-2b5658b3a6af)\n",
      "Processing trace 15/240: run_048_moondream:1.8b_0.2 (0197c12a-5b3a-78b6-8a8c-63cbddc9b05a)\n",
      "Processing trace 16/240: run_048_llama3.2:1b_0.2 (0197c127-76c0-7123-b1f9-2ebe63aea32a)\n",
      "Processing trace 17/240: run_048_gemma3:4b_0.2 (0197c116-68a1-7689-a43e-6a42473df4ff)\n",
      "Processing trace 18/240: run_049_deepseek-r1:1.5b_0.2 (0197c111-8fb6-767c-b2fc-c7d6204e5e8e)\n",
      "Processing trace 19/240: run_048_gemma3:1b_0.2 (0197c111-8578-7e4f-922c-d27cb092b8d8)\n",
      "Processing trace 20/240: run_047_qwen3:4b_0.0 (0197c0fe-279b-7341-8504-eaf89ee2bdf2)\n",
      "Processing trace 21/240: run_048_deepseek-r1:1.5b_0.2 (0197c0f3-d545-7bb2-a4a4-e3290ad5dc41)\n",
      "Processing trace 22/240: run_047_qwen3:1.7b_0.0 (0197c0eb-40a8-70db-bc99-2686e796cc14)\n",
      "Processing trace 23/240: run_046_qwen3:4b_0.0 (0197c0dd-d7f2-75eb-bd92-2572f9e850cc)\n",
      "Processing trace 24/240: run_047_qwen3:0.6b_0.0 (0197c0d7-3b32-7a36-baae-04748b1fde41)\n",
      "Processing trace 25/240: run_046_qwen3:1.7b_0.0 (0197c0d1-a2a7-7257-94cc-32ebe1ff6937)\n",
      "Processing trace 26/240: run_047_tinyllama:1.1b_0.0 (0197c0cf-34fc-7734-a6a1-4ba2d2244334)\n",
      "Processing trace 27/240: run_047_smollm2:1.7b_0.0 (0197c0cb-ec45-745b-a997-c1b7b43dfc64)\n",
      "Processing trace 28/240: run_047_moondream:1.8b_0.0 (0197c0cb-dee2-733f-9249-ac72b87c5909)\n",
      "Processing trace 29/240: run_046_qwen3:0.6b_0.0 (0197c0cb-42b0-781c-b5f6-caf091754121)\n",
      "Processing trace 30/240: run_047_llama3.2:1b_0.0 (0197c0c9-6d6a-7f70-8a12-5d7d3dca1acf)\n",
      "Processing trace 31/240: run_046_tinyllama:1.1b_0.0 (0197c0bf-cda7-7c1e-aaed-fc0d79517d6c)\n",
      "Processing trace 32/240: run_046_smollm2:1.7b_0.0 (0197c0ba-80ed-7528-a7b6-d47449a5d4f0)\n",
      "Processing trace 33/240: run_046_moondream:1.8b_0.0 (0197c0ba-5f16-74ba-b61f-ad011a2c05c0)\n",
      "Processing trace 34/240: run_047_gemma3:4b_0.0 (0197c0b6-9efd-79aa-bf99-8e5c071c0f50)\n",
      "Processing trace 35/240: run_046_llama3.2:1b_0.0 (0197c0b6-63d3-7fa4-9ad8-c0056ababbb7)\n",
      "Processing trace 36/240: run_047_gemma3:1b_0.0 (0197c0a8-0cde-74e6-9208-376070720765)\n",
      "Processing trace 37/240: run_046_gemma3:4b_0.0 (0197c0a5-b92c-7188-9872-4a22312d164f)\n",
      "Processing trace 38/240: run_046_gemma3:1b_0.0 (0197c0a0-9539-7b64-b06f-f94c4e01856e)\n",
      "Processing trace 39/240: run_047_deepseek-r1:1.5b_0.0 (0197c094-b5ff-7bae-b42d-fba8a7766214)\n",
      "Processing trace 40/240: run_046_deepseek-r1:1.5b_0.0 (0197c08b-a64e-79f5-b1d5-870b0d895f07)\n",
      "Processing trace 41/240: run_018_qwen3:4b_0.2 (0197c080-321d-7a4f-b939-00ace8052584)\n",
      "Processing trace 42/240: run_018_qwen3:1.7b_0.2 (0197c07c-b95a-7262-9268-d2eaf6824bce)\n",
      "Processing trace 43/240: run_017_qwen3:4b_0.2 (0197c074-474d-712f-acac-12d5e637c1ca)\n",
      "Processing trace 44/240: run_017_qwen3:1.7b_0.2 (0197c06a-3e69-7a9e-a171-6d79fa570304)\n",
      "Processing trace 45/240: run_018_qwen3:0.6b_0.2 (0197c06a-296d-768b-a863-bc6b8943fc6d)\n",
      "Processing trace 46/240: run_018_tinyllama:1.1b_0.2 (0197c065-2f7d-789d-bde2-b82516c82958)\n",
      "Processing trace 47/240: run_017_qwen3:0.6b_0.2 (0197c060-a3a7-7c8d-bd41-903541cc7e8b)\n",
      "Processing trace 48/240: run_017_tinyllama:1.1b_0.2 (0197c058-e0f4-71d1-aaa5-c30cf458cf61)\n",
      "Processing trace 49/240: run_018_smollm2:1.7b_0.2 (0197c057-fd1f-710d-9d7e-555860ae6798)\n",
      "Processing trace 50/240: run_018_moondream:1.8b_0.2 (0197c057-ef12-7e0a-8dfe-679ae34f73c2)\n",
      "Processing trace 51/240: run_018_llama3.2:1b_0.2 (0197c054-5f8f-7af5-aaf7-72acf79bf030)\n",
      "Processing trace 52/240: run_017_smollm2:1.7b_0.2 (0197c050-3303-76bd-914e-d75fb9ff0122)\n",
      "Processing trace 53/240: run_017_moondream:1.8b_0.2 (0197c050-144e-74c5-abb0-0581bb2b007c)\n",
      "Processing trace 54/240: run_017_llama3.2:1b_0.2 (0197c04b-89e9-7fd0-bd8f-aeb1f7e00869)\n",
      "Processing trace 55/240: run_018_gemma3:4b_0.2 (0197c044-67dd-76bb-842b-3dd2076d98d4)\n",
      "Processing trace 56/240: run_018_gemma3:1b_0.2 (0197c040-9b84-7f46-9f0b-ef03c054d2b8)\n",
      "Processing trace 57/240: run_017_gemma3:4b_0.2 (0197c03d-d8f7-7d02-b462-13b749e99236)\n",
      "Processing trace 58/240: run_017_gemma3:1b_0.2 (0197c039-8057-7c62-9a51-11dddaf4beeb)\n",
      "Processing trace 59/240: run_018_deepseek-r1:1.5b_0.2 (0197c02d-e812-7472-ae25-e47a1db85afb)\n",
      "Processing trace 60/240: run_017_deepseek-r1:1.5b_0.2 (0197c021-1dbf-79f3-b0fb-2e3e66a80a19)\n",
      "Processing trace 61/240: run_016_qwen3:4b_0.0 (0197c018-771c-7329-845a-3e6aa2f7b364)\n",
      "Processing trace 62/240: run_016_qwen3:1.7b_0.0 (0197c015-0121-75e2-a338-38c0120ba744)\n",
      "Processing trace 63/240: run_015_qwen3:4b_0.0 (0197c00a-5885-71ae-95aa-833c903a2235)\n",
      "Processing trace 64/240: run_016_qwen3:0.6b_0.0 (0197c001-fe14-7617-88de-0ef75c92d1c7)\n",
      "Processing trace 65/240: run_015_qwen3:1.7b_0.0 (0197c001-215c-7333-acb7-a4faa7e07f54)\n",
      "Processing trace 66/240: run_016_tinyllama:1.1b_0.0 (0197bffb-4eea-7d8b-bc21-68a467240211)\n",
      "Processing trace 67/240: run_016_smollm2:1.7b_0.0 (0197bff8-71b8-73ae-adad-010b2d6b9678)\n",
      "Processing trace 68/240: run_016_moondream:1.8b_0.0 (0197bff8-648d-79f4-9612-b8cdb27cb67b)\n",
      "Processing trace 69/240: run_015_qwen3:0.6b_0.0 (0197bff8-4d96-720d-bca0-80194049226e)\n",
      "Processing trace 70/240: run_016_llama3.2:1b_0.0 (0197bff6-11fc-7cc3-8da7-11e858a107d4)\n",
      "Processing trace 71/240: run_015_tinyllama:1.1b_0.0 (0197bff2-3100-7c5c-9d97-3d05faf3ab1a)\n",
      "Processing trace 72/240: run_016_gemma3:4b_0.0 (0197bfee-1021-7234-8843-279ca52d1c5c)\n",
      "Processing trace 73/240: run_016_gemma3:1b_0.0 (0197bfea-bd65-7190-8627-8b93134d9c77)\n",
      "Processing trace 74/240: run_015_smollm2:1.7b_0.0 (0197bfe9-7eea-7635-90f1-49c08e763b80)\n",
      "Processing trace 75/240: run_015_moondream:1.8b_0.0 (0197bfe9-660d-767b-9dbb-a7467f0c2269)\n",
      "Processing trace 76/240: run_015_llama3.2:1b_0.0 (0197bfe6-5899-7ba2-ac23-2b41c67dfee8)\n",
      "Processing trace 77/240: run_015_gemma3:4b_0.0 (0197bfd7-44ba-7b30-b1fa-2281a126efc3)\n",
      "Processing trace 78/240: run_016_deepseek-r1:1.5b_0.0 (0197bfd4-29da-788d-b6eb-d811ec9ade72)\n",
      "Processing trace 79/240: run_015_gemma3:1b_0.0 (0197bfd3-2b8f-7ba6-8e1a-3205bfbd08e3)\n",
      "Processing trace 80/240: run_014_qwen3:4b_0.2 (0197bfc2-cc8c-7b38-8a9b-70033cfa3ce0)\n",
      "Processing trace 81/240: run_015_deepseek-r1:1.5b_0.0 (0197bfb6-72ef-72a2-990d-94cc621d0d56)\n",
      "Processing trace 82/240: run_014_qwen3:1.7b_0.2 (0197bfb3-2430-7fd6-973a-2e1000f31cb6)\n",
      "Processing trace 83/240: run_013_qwen3:4b_0.2 (0197bf9a-85f2-7b8c-82df-71d224d39bc7)\n",
      "Processing trace 84/240: run_014_qwen3:0.6b_0.2 (0197bf94-f63c-7e53-9773-c91f1054b717)\n",
      "Processing trace 85/240: run_014_tinyllama:1.1b_0.2 (0197bf87-f9cd-782a-991c-2dde813a9f1f)\n",
      "Processing trace 86/240: run_014_smollm2:1.7b_0.2 (0197bf81-d265-7df1-a5c1-6eabc4bd4275)\n",
      "Processing trace 87/240: run_014_moondream:1.8b_0.2 (0197bf81-c01a-74af-8554-1466c10e9a7a)\n",
      "Processing trace 88/240: run_013_qwen3:1.7b_0.2 (0197bf7c-d081-769f-ba3e-2ebed32063e4)\n",
      "Processing trace 89/240: run_011_qwen3:4b_0.4 (0197bf7c-6d43-75f9-9122-e9392c1c7728)\n",
      "Processing trace 90/240: run_014_llama3.2:1b_0.2 (0197bf74-f1a2-7679-b620-e8da23fb366f)\n",
      "Processing trace 91/240: run_012_qwen3:4b_0.4 (0197bf71-94a8-7c2d-bc78-fa96751224e9)\n",
      "Processing trace 92/240: run_013_qwen3:0.6b_0.2 (0197bf6e-4ac1-7b6d-b46d-6425b1f81124)\n",
      "Processing trace 93/240: run_014_gemma3:4b_0.2 (0197bf62-9aa6-7b2f-9456-f185203eb01b)\n",
      "Processing trace 94/240: run_011_qwen3:1.7b_0.4 (0197bf61-9a21-78bc-b29c-b088fe5c86a4)\n",
      "Processing trace 95/240: run_013_tinyllama:1.1b_0.2 (0197bf58-f848-7f63-b4a0-19669f5f81cc)\n",
      "Processing trace 96/240: run_012_qwen3:1.7b_0.4 (0197bf55-592c-7716-ad6c-7dc7feecb2df)\n",
      "Processing trace 97/240: run_014_gemma3:1b_0.2 (0197bf53-8a78-7ec4-83ed-d915af15713e)\n",
      "Processing trace 98/240: run_013_smollm2:1.7b_0.2 (0197bf50-08a1-7724-a3f7-9546991afb25)\n",
      "Processing trace 99/240: run_013_moondream:1.8b_0.2 (0197bf4f-f441-7d99-93ad-272cd52ac3d6)\n",
      "Processing trace 100/240: run_011_qwen3:0.6b_0.4 (0197bf46-1bf5-7196-bab2-5aa2530a6aaf)\n",
      "Processing trace 101/240: run_013_llama3.2:1b_0.2 (0197bf45-3a37-7fb9-a9e1-4289adfa81d4)\n",
      "Processing trace 102/240: run_012_qwen3:0.6b_0.4 (0197bf42-8966-738c-a5b2-bbeadd461fac)\n",
      "Processing trace 103/240: run_013_gemma3:4b_0.2 (0197bf36-d727-7bb7-ab5a-b4f5c55a6c72)\n",
      "Processing trace 104/240: run_011_tinyllama:1.1b_0.4 (0197bf36-27d4-739d-a95a-b7208ed02cc6)\n",
      "Processing trace 105/240: run_012_tinyllama:1.1b_0.4 (0197bf35-1317-7e1d-9712-cf7e3bf701fb)\n",
      "Processing trace 106/240: run_014_deepseek-r1:1.5b_0.2 (0197bf34-fc57-704d-a4a8-7d6393f76737)\n",
      "Processing trace 107/240: run_012_smollm2:1.7b_0.4 (0197bf2b-129d-7708-9937-a7413e49e24a)\n",
      "Processing trace 108/240: run_013_gemma3:1b_0.2 (0197bf25-4b65-7bec-b9f8-49e42ad6ea85)\n",
      "Processing trace 109/240: run_011_smollm2:1.7b_0.4 (0197bf23-46ab-721d-8a0c-e05470870bcf)\n",
      "Processing trace 110/240: run_012_moondream:1.8b_0.4 (0197bf23-3305-787f-9963-e3efede66884)\n",
      "Processing trace 111/240: run_011_moondream:1.8b_0.4 (0197bf20-44f2-72e9-9c8d-957d63c08f6c)\n",
      "Processing trace 112/240: run_011_llama3.2:1b_0.4 (0197bf18-8e3e-74aa-ab4c-a78d46842823)\n",
      "Processing trace 113/240: run_010_qwen3:4b_0.0 (0197bf18-5468-7022-87bd-325d487b8ccb)\n",
      "Processing trace 114/240: run_012_llama3.2:1b_0.4 (0197bf0e-9c4c-70e1-9689-df79ae8a5803)\n",
      "Processing trace 115/240: run_011_gemma3:4b_0.4 (0197befe-75cc-7786-808c-83af03dc716d)\n",
      "Processing trace 116/240: run_010_qwen3:1.7b_0.0 (0197befd-28f6-77b1-b14a-941e3216be90)\n",
      "Processing trace 117/240: run_013_deepseek-r1:1.5b_0.2 (0197befd-1906-75d9-a373-a18d5dfcc889)\n",
      "Processing trace 118/240: run_012_gemma3:4b_0.4 (0197bef4-5d40-75bb-8b70-ca6750afcb1c)\n",
      "Processing trace 119/240: run_001_qwen3:4b_0.4 (0197beef-2217-7e4a-9cf6-9a6a4b6c2710)\n",
      "Processing trace 120/240: run_011_gemma3:1b_0.4 (0197bee9-8299-7b84-95ad-6d0923ffd0b8)\n",
      "Processing trace 121/240: run_012_gemma3:1b_0.4 (0197bedc-b687-7680-bc60-c64c5d7266f1)\n",
      "Processing trace 122/240: run_001_qwen3:1.7b_0.4 (0197bedb-dc96-7e09-bc9b-041ba4ac0870)\n",
      "Processing trace 123/240: run_010_qwen3:0.6b_0.0 (0197bed9-3a86-780e-b911-46153e77cd47)\n",
      "Processing trace 124/240: run_009_qwen3:4b_0.0 (0197bed7-c8fc-7af6-93cf-0f72038b6976)\n",
      "Processing trace 125/240: run_010_tinyllama:1.1b_0.0 (0197bece-4f37-7ad8-b01d-5b2fc4d02b7a)\n",
      "Processing trace 126/240: run_010_smollm2:1.7b_0.0 (0197bec9-4cf8-7a1c-8acf-94f4c22fdb97)\n",
      "Processing trace 127/240: run_010_moondream:1.8b_0.0 (0197bec9-3daf-72e8-9626-45c2573c9d8c)\n",
      "Processing trace 128/240: run_010_llama3.2:1b_0.0 (0197bec2-c3e0-7f61-b7ca-51873e382830)\n",
      "Processing trace 129/240: run_009_qwen3:1.7b_0.0 (0197bebe-b8e7-733d-83f4-32502e0a3d5d)\n",
      "Processing trace 130/240: run_012_deepseek-r1:1.5b_0.4 (0197bebb-311a-7959-9c4b-c1caefea286e)\n",
      "Processing trace 131/240: run_011_deepseek-r1:1.5b_0.4 (0197beb4-7d2f-795b-bb0d-2a19432a1097)\n",
      "Processing trace 132/240: run_009_qwen3:0.6b_0.0 (0197beaa-ff6d-79b5-bedf-db2f3e3f8520)\n",
      "Processing trace 133/240: run_001_qwen3:4b_0.4 (0197bea9-0b46-703d-9603-3a83ecf2e320)\n",
      "Processing trace 134/240: run_010_gemma3:4b_0.0 (0197bea8-3e8b-77b9-ae65-e661142c1065)\n",
      "Processing trace 135/240: run_010_gemma3:1b_0.0 (0197be9e-3275-70f7-bd62-21338a61a4cf)\n",
      "Processing trace 136/240: run_009_tinyllama:1.1b_0.0 (0197be95-6bc8-704d-81d6-2ac134e451e3)\n",
      "Processing trace 137/240: run_008_qwen3:4b_0.4 (0197be91-e662-7e71-bcb2-8e65aed3a9d5)\n",
      "Processing trace 138/240: run_001_qwen3:0.6b_0.4 (0197be8f-b477-729e-8684-0370a268debe)\n",
      "Processing trace 139/240: run_007_qwen3:4b_0.4 (0197be8c-7a23-77b7-bab1-920b74c4655c)\n",
      "Processing trace 140/240: run_009_smollm2:1.7b_0.0 (0197be8a-9535-7d96-9b7d-e533cba9ddc3)\n",
      "Processing trace 141/240: run_009_moondream:1.8b_0.0 (0197be8a-7da0-7d25-a3bd-d6a36738fdaf)\n",
      "Processing trace 142/240: run_007_qwen3:1.7b_0.4 (0197be88-bc37-782a-b597-79d728ce9bfc)\n",
      "Processing trace 143/240: run_009_llama3.2:1b_0.0 (0197be76-fa07-7861-8a5e-a23fa4bb609a)\n",
      "Processing trace 144/240: run_008_qwen3:1.7b_0.4 (0197be76-7939-7f2b-9d68-bebc61c31a6b)\n",
      "Processing trace 145/240: run_010_deepseek-r1:1.5b_0.0 (0197be73-caaf-767f-bee3-e4c20fd16096)\n",
      "Processing trace 146/240: run_007_qwen3:0.6b_0.4 (0197be67-4568-7a9b-9f83-2a873635741b)\n",
      "Processing trace 147/240: run_009_gemma3:4b_0.0 (0197be65-034e-77ab-b45d-4e5e5ccd06f1)\n",
      "Processing trace 148/240: run_008_qwen3:0.6b_0.4 (0197be5f-01d7-7107-ba8c-69ffe845dc5a)\n",
      "Processing trace 149/240: run_001_qwen3:1.7b_0.4 (0197be5c-ba88-7273-9c7d-591c7a9b76ca)\n",
      "Processing trace 150/240: run_001_tinyllama:1.1b_0.4 (0197be5a-61c5-7a47-a50e-7d81b24a5583)\n",
      "Processing trace 151/240: run_009_gemma3:1b_0.0 (0197be56-26ba-7329-a759-252e102f0475)\n",
      "Processing trace 152/240: run_006_qwen3:4b_0.2 (0197be54-6b1b-7d00-bee1-38541c8d861f)\n",
      "Processing trace 153/240: run_007_tinyllama:1.1b_0.4 (0197be54-04a1-7285-a0e5-977e7308ed34)\n",
      "Processing trace 154/240: run_008_tinyllama:1.1b_0.4 (0197be51-1a59-7c23-a17d-065f9579fbcf)\n",
      "Processing trace 155/240: run_006_qwen3:1.7b_0.2 (0197be4d-64de-729e-991c-7aad9f355aca)\n",
      "Processing trace 156/240: run_008_smollm2:1.7b_0.4 (0197be46-5c70-72c5-8a4c-f1712d887932)\n",
      "Processing trace 157/240: run_007_smollm2:1.7b_0.4 (0197be45-a99a-7f5f-8443-f7d2270d75c4)\n",
      "Processing trace 158/240: run_007_moondream:1.8b_0.4 (0197be45-9299-79a9-8a07-407ce816ba3c)\n",
      "Processing trace 159/240: run_008_moondream:1.8b_0.4 (0197be41-b852-70f1-930a-9f49e4eea235)\n",
      "Processing trace 160/240: run_007_llama3.2:1b_0.4 (0197be3e-2f8f-74a4-b1e0-0cb2ac9290b7)\n",
      "Processing trace 161/240: run_009_deepseek-r1:1.5b_0.0 (0197be3b-7bf8-7487-9e0a-97e3f911d6c4)\n",
      "Processing trace 162/240: run_001_qwen3:0.6b_0.4 (0197be32-1dc5-70d8-bf7f-2d6b68c3cb0e)\n",
      "Processing trace 163/240: run_008_llama3.2:1b_0.4 (0197be2e-561a-7878-b28f-8c52f189a521)\n",
      "Processing trace 164/240: run_001_smollm2:1.7b_0.4 (0197be2b-f91e-7012-8755-11bc56f97e48)\n",
      "Processing trace 165/240: run_001_moondream:1.8b_0.4 (0197be2b-20f3-76c3-ba3c-9d79d3f8a3e1)\n",
      "Processing trace 166/240: run_006_qwen3:0.6b_0.2 (0197be2a-f7d2-7233-9181-ace29ce91c22)\n",
      "Processing trace 167/240: run_001_tinyllama:1.1b_0.4 (0197be1c-0653-7406-8e05-cf2dd8421a11)\n",
      "Processing trace 168/240: run_006_tinyllama:1.1b_0.2 (0197be1a-5653-7b83-8a91-82da59bb08b6)\n",
      "Processing trace 169/240: run_007_gemma3:4b_0.4 (0197be18-acac-7637-abd4-eca32c7fdee7)\n",
      "Processing trace 170/240: run_005_qwen3:4b_0.2 (0197be11-cbe2-72e1-b007-f77c2f928477)\n",
      "Processing trace 171/240: run_001_llama3.2:1b_0.4 (0197be0f-9c30-7fe9-bd3a-7ac8f2d83a50)\n",
      "Processing trace 172/240: run_008_gemma3:4b_0.4 (0197be0f-6ef9-7c5a-bfa6-d6937b0dd77c)\n",
      "Processing trace 173/240: run_006_smollm2:1.7b_0.2 (0197be0c-8414-795c-863e-3b7400bfdd14)\n",
      "Processing trace 174/240: run_006_moondream:1.8b_0.2 (0197be0c-760e-7c43-823a-408b8a78f2b2)\n",
      "Processing trace 175/240: run_006_llama3.2:1b_0.2 (0197be07-3fb3-7228-b50f-90cf7050b13a)\n",
      "Processing trace 176/240: run_001_smollm2:1.7b_0.4 (0197be03-ebec-7485-a228-15b79cfffae6)\n",
      "Processing trace 177/240: run_007_gemma3:1b_0.4 (0197be03-c1d2-7034-ac7b-07ac1d23ddd8)\n",
      "Processing trace 178/240: run_001_moondream:1.8b_0.4 (0197be03-1298-75f9-9e19-9982a782cac6)\n",
      "Processing trace 179/240: run_008_gemma3:1b_0.4 (0197be00-9ff3-7e90-a0c2-0a9bbd396d52)\n",
      "Processing trace 180/240: run_005_qwen3:1.7b_0.2 (0197bdf8-75df-7a79-991d-6af90fcacbb2)\n",
      "Processing trace 181/240: run_001_llama3.2:1b_0.4 (0197bdeb-9802-7db5-8cc1-f297f9539bab)\n",
      "Processing trace 182/240: run_005_qwen3:0.6b_0.2 (0197bdea-2c72-7173-bca0-179d7955828e)\n",
      "Processing trace 183/240: run_006_gemma3:4b_0.2 (0197bde4-9745-7007-862f-34d2bbba9fb1)\n",
      "Processing trace 184/240: run_005_tinyllama:1.1b_0.2 (0197bde4-17c7-7ec1-8af7-766934c95998)\n",
      "Processing trace 185/240: run_006_gemma3:1b_0.2 (0197bdde-3462-74ce-bb17-e555522d8d2c)\n",
      "Processing trace 186/240: run_005_smollm2:1.7b_0.2 (0197bddd-7b67-7dd9-9529-b25519a23ddb)\n",
      "Processing trace 187/240: run_005_moondream:1.8b_0.2 (0197bddd-6988-7016-b47a-0e69b5dcc8b0)\n",
      "Processing trace 188/240: run_008_deepseek-r1:1.5b_0.4 (0197bdda-2014-775d-a63c-237d2515e083)\n",
      "Processing trace 189/240: run_005_llama3.2:1b_0.2 (0197bdd7-d95c-7414-8650-8c19124ab050)\n",
      "Processing trace 190/240: run_007_deepseek-r1:1.5b_0.4 (0197bdd3-da1e-7ea8-abed-ac045ac0cbdc)\n",
      "Processing trace 191/240: run_005_gemma3:4b_0.2 (0197bdb8-38be-7736-bfb5-9513184a50b6)\n",
      "Processing trace 192/240: run_006_deepseek-r1:1.5b_0.2 (0197bdb7-9cdb-77c1-87cf-18699ada7cc0)\n",
      "Processing trace 193/240: run_005_gemma3:1b_0.2 (0197bdae-82a7-7229-af4d-00818791efdc)\n",
      "Processing trace 194/240: run_004_qwen3:4b_0.4 (0197bdaa-463f-78f0-a83c-a2ce8d3260a4)\n",
      "Processing trace 195/240: run_003_qwen3:4b_0.4 (0197bda8-c217-70a1-9d32-db8744d8b97a)\n",
      "Processing trace 196/240: run_001_gemma3:4b_0.4 (0197bda4-d075-7f85-b0ba-8cfc92fdf22f)\n",
      "Processing trace 197/240: run_001_gemma3:4b_0.4 (0197bd97-96d6-7be2-9299-aff96a9a7e0f)\n",
      "Processing trace 198/240: run_001_qwen3:4b_0.0 (0197bd97-2a8d-7dc0-8dae-81bc931933bf)\n",
      "Processing trace 199/240: run_004_qwen3:1.7b_0.4 (0197bd95-eaca-7b7b-bf6b-a00bc6841494)\n",
      "Processing trace 200/240: run_001_qwen3:1.7b_0.0 (0197bd90-bc6a-73af-8309-fcb101a0c082)\n",
      "Processing trace 201/240: run_005_deepseek-r1:1.5b_0.2 (0197bd8c-f0f3-7a35-acc2-7741f0089cbc)\n",
      "Processing trace 202/240: run_003_qwen3:1.7b_0.4 (0197bd88-a498-76b8-a26e-5bcced5d0a45)\n",
      "Processing trace 203/240: run_001_gemma3:1b_0.4 (0197bd7a-ab21-7583-b1b5-6d9b1da17299)\n",
      "Processing trace 204/240: run_002_qwen3:4b_0.0 (0197bd70-2287-7518-b754-3d9622811114)\n",
      "Processing trace 205/240: run_004_qwen3:0.6b_0.4 (0197bd6f-de2c-746f-9213-f4a7c85f03fa)\n",
      "Processing trace 206/240: run_003_qwen3:0.6b_0.4 (0197bd6e-533a-78e9-8f8d-bc419041f277)\n",
      "Processing trace 207/240: run_001_qwen3:0.6b_0.0 (0197bd6a-23a9-7934-bac2-d81eebfea5ee)\n",
      "Processing trace 208/240: run_001_gemma3:1b_0.4 (0197bd62-05f2-7068-b488-9e07003a16f4)\n",
      "Processing trace 209/240: run_003_tinyllama:1.1b_0.4 (0197bd61-b321-7fd7-b090-9fe8a17e4a81)\n",
      "Processing trace 210/240: run_004_tinyllama:1.1b_0.4 (0197bd61-62f4-78da-a41d-7cc1d84f48d1)\n",
      "Processing trace 211/240: run_002_qwen3:1.7b_0.0 (0197bd5a-adb5-7976-bf4b-48418415993d)\n",
      "Processing trace 212/240: run_001_tinyllama:1.1b_0.0 (0197bd59-2307-7689-a47e-350f482c3d1c)\n",
      "Processing trace 213/240: run_004_smollm2:1.7b_0.4 (0197bd58-1de4-7187-bc44-392e24c3ae1b)\n",
      "Processing trace 214/240: run_003_smollm2:1.7b_0.4 (0197bd58-1993-7ea1-af15-3d672ab18db3)\n",
      "Processing trace 215/240: run_004_moondream:1.8b_0.4 (0197bd57-f9e2-765f-8c6e-80f848ab17d6)\n",
      "Processing trace 216/240: run_003_moondream:1.8b_0.4 (0197bd51-2595-7ee9-a7ca-d5b6c80e4656)\n",
      "Processing trace 217/240: run_001_smollm2:1.7b_0.0 (0197bd4e-a6a7-712f-a500-c93cf496ce85)\n",
      "Processing trace 218/240: run_001_moondream:1.8b_0.0 (0197bd4e-8563-7bbb-8029-837ae5600490)\n",
      "Processing trace 219/240: run_004_llama3.2:1b_0.4 (0197bd4d-bb0b-7350-a496-9c00051fcd2b)\n",
      "Processing trace 220/240: run_002_qwen3:0.6b_0.0 (0197bd4c-7bac-7a1a-ad1a-45e3754da907)\n",
      "Processing trace 221/240: run_003_llama3.2:1b_0.4 (0197bd47-ca95-723b-92d0-8eb2dac3d437)\n",
      "Processing trace 222/240: run_002_tinyllama:1.1b_0.0 (0197bd46-73a5-7d77-8b3a-c570885edfef)\n",
      "Processing trace 223/240: run_001_llama3.2:1b_0.0 (0197bd45-e1db-7b19-8abe-8f6a3f1d6042)\n",
      "Processing trace 224/240: run_002_smollm2:1.7b_0.0 (0197bd3f-d807-7d70-b714-a2749d564571)\n",
      "Processing trace 225/240: run_002_moondream:1.8b_0.0 (0197bd3f-c541-7651-b0e9-6910a7b8e140)\n",
      "Processing trace 226/240: run_002_llama3.2:1b_0.0 (0197bd39-7825-771f-b523-658bbda080f4)\n",
      "Processing trace 227/240: run_004_gemma3:4b_0.4 (0197bd39-0c64-7b26-b934-b6aeca064560)\n",
      "Processing trace 228/240: run_003_gemma3:4b_0.4 (0197bd31-7db4-7205-a012-91c8da9a04fb)\n",
      "Processing trace 229/240: run_004_gemma3:1b_0.4 (0197bd30-b40e-79cd-adc0-c3f1ba814c7f)\n",
      "Processing trace 230/240: run_001_gemma3:4b_0.0 (0197bd2b-effb-7c5f-baa1-978412209aac)\n",
      "Processing trace 231/240: run_003_gemma3:1b_0.4 (0197bd28-37e7-7077-94cc-378a4442fe23)\n",
      "Processing trace 232/240: run_002_gemma3:4b_0.0 (0197bd27-84d2-7ec9-adfd-1de4a917e159)\n",
      "Processing trace 233/240: run_001_gemma3:1b_0.0 (0197bd24-bf96-76b2-a67c-aff3882ddfbc)\n",
      "Processing trace 234/240: run_002_gemma3:1b_0.0 (0197bd22-0c97-79b3-8ebe-ae385d1ce98d)\n",
      "Processing trace 235/240: run_004_deepseek-r1:1.5b_0.4 (0197bd06-b2c3-7f4b-9ad8-81f6dd9b27b2)\n",
      "Processing trace 236/240: run_003_deepseek-r1:1.5b_0.4 (0197bd06-69b0-7bf8-a7e4-b810caef1a73)\n",
      "Processing trace 237/240: run_001_deepseek-r1:1.5b_0.4 (0197bd05-09aa-77bf-9ede-b0c5b0b6d762)\n",
      "Processing trace 238/240: run_001_deepseek-r1:1.5b_0.4 (0197bd04-f7c5-7971-a387-d34a5bab4a4c)\n",
      "Processing trace 239/240: run_002_deepseek-r1:1.5b_0.0 (0197bd04-7801-767b-9617-208ae407f61b)\n",
      "Processing trace 240/240: run_001_deepseek-r1:1.5b_0.0 (0197bd04-3e8f-7ca6-a9a2-71201bb23e96)\n"
     ]
    }
   ],
   "source": [
    "# Get all the data of Opik determinism project\n",
    "all_opik_data = get_opik_flat_data_for_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff02533f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full DataFrame loaded with shape: (24000, 20)\n",
      "INFO: Total of 2726 failed responses (timeout, empty answer or infinite loop).\n",
      "DataFrame after preprocessing:\n",
      "                                span_id span_name  response_time  \\\n",
      "0  0197c181-c40a-773e-a48c-f2fd9a6caf63   q10_r10         11.648   \n",
      "1  0197c181-968a-7f04-91f8-d694be080ada    q10_r9         28.038   \n",
      "2  0197c181-2903-722b-98c9-7d680961893a    q10_r8         11.675   \n",
      "3  0197c180-fb68-7c6c-bb8a-865d9e9577ac    q10_r7         28.049   \n",
      "4  0197c180-8dd7-7187-ac09-5b52f5458ab0    q10_r6         11.668   \n",
      "\n",
      "                                 span_input_question span_output_answer  \\\n",
      "0  What is the sum of the binary numbers 11100101...             [c][c]   \n",
      "1  What is the sum of the binary numbers 11100101...          [c][c][c]   \n",
      "2  What is the sum of the binary numbers 11100101...             [c][c]   \n",
      "3  What is the sum of the binary numbers 11100101...          [c][c][c]   \n",
      "4  What is the sum of the binary numbers 11100101...             [c][c]   \n",
      "\n",
      "                              span_output_raw_answer span_correct_answer  \\\n",
      "0  [[ ## thought ## ]] To add the binary numbers ...                   c   \n",
      "1  <think> Okay, let's see. The user is asking fo...                   c   \n",
      "2  [[ ## thought ## ]] To add the binary numbers ...                   c   \n",
      "3  <think> Okay, let's see. The user is asking fo...                   c   \n",
      "4  [[ ## thought ## ]] To add the binary numbers ...                   c   \n",
      "\n",
      "   completion_tokens    question_file                              trace_id  \\\n",
      "0                  0  question_10.txt  0197c174-ebcd-7404-97e0-3ba8fa383e50   \n",
      "1                  0  question_10.txt  0197c174-ebcd-7404-97e0-3ba8fa383e50   \n",
      "2                  0  question_10.txt  0197c174-ebcd-7404-97e0-3ba8fa383e50   \n",
      "3                  0  question_10.txt  0197c174-ebcd-7404-97e0-3ba8fa383e50   \n",
      "4                  0  question_10.txt  0197c174-ebcd-7404-97e0-3ba8fa383e50   \n",
      "\n",
      "   ... language prompting_tech num_runs_per_question model_source  \\\n",
      "0  ...       en             R4                    10        local   \n",
      "1  ...       en             R4                    10        local   \n",
      "2  ...       en             R4                    10        local   \n",
      "3  ...       en             R4                    10        local   \n",
      "4  ...       en             R4                    10        local   \n",
      "\n",
      "   temperature top_p    exercise    question_type is_failure  \\\n",
      "0          0.2   0.1  exam_01_mc  multiple_choice      False   \n",
      "1          0.2   0.1  exam_01_mc  multiple_choice       True   \n",
      "2          0.2   0.1  exam_01_mc  multiple_choice      False   \n",
      "3          0.2   0.1  exam_01_mc  multiple_choice       True   \n",
      "4          0.2   0.1  exam_01_mc  multiple_choice      False   \n",
      "\n",
      "                                              answer  \n",
      "0  [[ ## thought ## ]] To add the binary numbers ...  \n",
      "1                                _OVERTHINK_FAILURE_  \n",
      "2  [[ ## thought ## ]] To add the binary numbers ...  \n",
      "3                                _OVERTHINK_FAILURE_  \n",
      "4  [[ ## thought ## ]] To add the binary numbers ...  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "DataFrame head:\n",
      "                                span_id span_name  response_time  \\\n",
      "0  0197c181-c40a-773e-a48c-f2fd9a6caf63   q10_r10         11.648   \n",
      "1  0197c181-968a-7f04-91f8-d694be080ada    q10_r9         28.038   \n",
      "2  0197c181-2903-722b-98c9-7d680961893a    q10_r8         11.675   \n",
      "3  0197c180-fb68-7c6c-bb8a-865d9e9577ac    q10_r7         28.049   \n",
      "4  0197c180-8dd7-7187-ac09-5b52f5458ab0    q10_r6         11.668   \n",
      "\n",
      "                                 span_input_question span_output_answer  \\\n",
      "0  What is the sum of the binary numbers 11100101...             [c][c]   \n",
      "1  What is the sum of the binary numbers 11100101...          [c][c][c]   \n",
      "2  What is the sum of the binary numbers 11100101...             [c][c]   \n",
      "3  What is the sum of the binary numbers 11100101...          [c][c][c]   \n",
      "4  What is the sum of the binary numbers 11100101...             [c][c]   \n",
      "\n",
      "                              span_output_raw_answer span_correct_answer  \\\n",
      "0  [[ ## thought ## ]] To add the binary numbers ...                   c   \n",
      "1  <think> Okay, let's see. The user is asking fo...                   c   \n",
      "2  [[ ## thought ## ]] To add the binary numbers ...                   c   \n",
      "3  <think> Okay, let's see. The user is asking fo...                   c   \n",
      "4  [[ ## thought ## ]] To add the binary numbers ...                   c   \n",
      "\n",
      "   completion_tokens    question_file                              trace_id  \\\n",
      "0                  0  question_10.txt  0197c174-ebcd-7404-97e0-3ba8fa383e50   \n",
      "1                  0  question_10.txt  0197c174-ebcd-7404-97e0-3ba8fa383e50   \n",
      "2                  0  question_10.txt  0197c174-ebcd-7404-97e0-3ba8fa383e50   \n",
      "3                  0  question_10.txt  0197c174-ebcd-7404-97e0-3ba8fa383e50   \n",
      "4                  0  question_10.txt  0197c174-ebcd-7404-97e0-3ba8fa383e50   \n",
      "\n",
      "   ... language prompting_tech num_runs_per_question model_source  \\\n",
      "0  ...       en             R4                    10        local   \n",
      "1  ...       en             R4                    10        local   \n",
      "2  ...       en             R4                    10        local   \n",
      "3  ...       en             R4                    10        local   \n",
      "4  ...       en             R4                    10        local   \n",
      "\n",
      "   temperature top_p    exercise    question_type is_failure  \\\n",
      "0          0.2   0.1  exam_01_mc  multiple_choice      False   \n",
      "1          0.2   0.1  exam_01_mc  multiple_choice       True   \n",
      "2          0.2   0.1  exam_01_mc  multiple_choice      False   \n",
      "3          0.2   0.1  exam_01_mc  multiple_choice       True   \n",
      "4          0.2   0.1  exam_01_mc  multiple_choice      False   \n",
      "\n",
      "                                              answer  \n",
      "0  [[ ## thought ## ]] To add the binary numbers ...  \n",
      "1                                _OVERTHINK_FAILURE_  \n",
      "2  [[ ## thought ## ]] To add the binary numbers ...  \n",
      "3                                _OVERTHINK_FAILURE_  \n",
      "4  [[ ## thought ## ]] To add the binary numbers ...  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "\n",
      "DataFrame shape: (24000, 22)\n",
      "\n",
      "Successfully extracted data and saved to ../../../data/determinism/opik_determinism_data.csv\n",
      "DataFrame after preprocessing:\n",
      "                                span_id span_name  response_time  \\\n",
      "0  0197c181-c40a-773e-a48c-f2fd9a6caf63   q10_r10         11.648   \n",
      "1  0197c181-968a-7f04-91f8-d694be080ada    q10_r9         28.038   \n",
      "2  0197c181-2903-722b-98c9-7d680961893a    q10_r8         11.675   \n",
      "3  0197c180-fb68-7c6c-bb8a-865d9e9577ac    q10_r7         28.049   \n",
      "4  0197c180-8dd7-7187-ac09-5b52f5458ab0    q10_r6         11.668   \n",
      "\n",
      "                                 span_input_question span_output_answer  \\\n",
      "0  What is the sum of the binary numbers 11100101...             [c][c]   \n",
      "1  What is the sum of the binary numbers 11100101...          [c][c][c]   \n",
      "2  What is the sum of the binary numbers 11100101...             [c][c]   \n",
      "3  What is the sum of the binary numbers 11100101...          [c][c][c]   \n",
      "4  What is the sum of the binary numbers 11100101...             [c][c]   \n",
      "\n",
      "                              span_output_raw_answer span_correct_answer  \\\n",
      "0  [[ ## thought ## ]] To add the binary numbers ...                   c   \n",
      "1  <think> Okay, let's see. The user is asking fo...                   c   \n",
      "2  [[ ## thought ## ]] To add the binary numbers ...                   c   \n",
      "3  <think> Okay, let's see. The user is asking fo...                   c   \n",
      "4  [[ ## thought ## ]] To add the binary numbers ...                   c   \n",
      "\n",
      "   completion_tokens    question_file                              trace_id  \\\n",
      "0                  0  question_10.txt  0197c174-ebcd-7404-97e0-3ba8fa383e50   \n",
      "1                  0  question_10.txt  0197c174-ebcd-7404-97e0-3ba8fa383e50   \n",
      "2                  0  question_10.txt  0197c174-ebcd-7404-97e0-3ba8fa383e50   \n",
      "3                  0  question_10.txt  0197c174-ebcd-7404-97e0-3ba8fa383e50   \n",
      "4                  0  question_10.txt  0197c174-ebcd-7404-97e0-3ba8fa383e50   \n",
      "\n",
      "   ... language prompting_tech num_runs_per_question model_source  \\\n",
      "0  ...       en             R4                    10        local   \n",
      "1  ...       en             R4                    10        local   \n",
      "2  ...       en             R4                    10        local   \n",
      "3  ...       en             R4                    10        local   \n",
      "4  ...       en             R4                    10        local   \n",
      "\n",
      "   temperature top_p    exercise    question_type is_failure  \\\n",
      "0          0.2   0.1  exam_01_mc  multiple_choice      False   \n",
      "1          0.2   0.1  exam_01_mc  multiple_choice       True   \n",
      "2          0.2   0.1  exam_01_mc  multiple_choice      False   \n",
      "3          0.2   0.1  exam_01_mc  multiple_choice       True   \n",
      "4          0.2   0.1  exam_01_mc  multiple_choice      False   \n",
      "\n",
      "                                              answer  \n",
      "0  [[ ## thought ## ]] To add the binary numbers ...  \n",
      "1                                _OVERTHINK_FAILURE_  \n",
      "2  [[ ## thought ## ]] To add the binary numbers ...  \n",
      "3                                _OVERTHINK_FAILURE_  \n",
      "4  [[ ## thought ## ]] To add the binary numbers ...  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "\n",
      "Filtered DataFrame for temperature = 0.0:\n",
      "DataFrame head:\n",
      "                                   span_id span_name  response_time  \\\n",
      "1900  0197c111-5b5d-77f4-a981-5e25d8b3c4ae   q10_r10         17.469   \n",
      "1901  0197c111-171f-73b9-817b-863fe74dbf95    q10_r9         41.601   \n",
      "1902  0197c110-749e-7b8f-b16f-4244272f0768    q10_r8         17.534   \n",
      "1903  0197c110-3020-79cf-9593-9188002cbfbf    q10_r7         41.700   \n",
      "1904  0197c10f-8d3c-7dee-bee9-ba8d730efbd2    q10_r6         17.379   \n",
      "\n",
      "                                    span_input_question span_output_answer  \\\n",
      "1900  What is the sum of the binary numbers 11100101...             [c][c]   \n",
      "1901  What is the sum of the binary numbers 11100101...          [c][c][c]   \n",
      "1902  What is the sum of the binary numbers 11100101...             [c][c]   \n",
      "1903  What is the sum of the binary numbers 11100101...          [c][c][c]   \n",
      "1904  What is the sum of the binary numbers 11100101...             [c][c]   \n",
      "\n",
      "                                 span_output_raw_answer span_correct_answer  \\\n",
      "1900  [[ ## thought ## ]] To add the binary numbers ...                   c   \n",
      "1901  <think> Okay, let's see. The user is asking fo...                   c   \n",
      "1902  [[ ## thought ## ]] To add the binary numbers ...                   c   \n",
      "1903  <think> Okay, let's see. The user is asking fo...                   c   \n",
      "1904  [[ ## thought ## ]] To add the binary numbers ...                   c   \n",
      "\n",
      "      completion_tokens    question_file  \\\n",
      "1900                  0  question_10.txt   \n",
      "1901                  0  question_10.txt   \n",
      "1902                  0  question_10.txt   \n",
      "1903                  0  question_10.txt   \n",
      "1904                  0  question_10.txt   \n",
      "\n",
      "                                  trace_id  ... language prompting_tech  \\\n",
      "1900  0197c0fe-279b-7341-8504-eaf89ee2bdf2  ...       en             R4   \n",
      "1901  0197c0fe-279b-7341-8504-eaf89ee2bdf2  ...       en             R4   \n",
      "1902  0197c0fe-279b-7341-8504-eaf89ee2bdf2  ...       en             R4   \n",
      "1903  0197c0fe-279b-7341-8504-eaf89ee2bdf2  ...       en             R4   \n",
      "1904  0197c0fe-279b-7341-8504-eaf89ee2bdf2  ...       en             R4   \n",
      "\n",
      "     num_runs_per_question model_source  temperature top_p    exercise  \\\n",
      "1900                    10        local          0.0   0.1  exam_01_mc   \n",
      "1901                    10        local          0.0   0.1  exam_01_mc   \n",
      "1902                    10        local          0.0   0.1  exam_01_mc   \n",
      "1903                    10        local          0.0   0.1  exam_01_mc   \n",
      "1904                    10        local          0.0   0.1  exam_01_mc   \n",
      "\n",
      "        question_type is_failure  \\\n",
      "1900  multiple_choice      False   \n",
      "1901  multiple_choice       True   \n",
      "1902  multiple_choice      False   \n",
      "1903  multiple_choice       True   \n",
      "1904  multiple_choice      False   \n",
      "\n",
      "                                                 answer  \n",
      "1900  [[ ## thought ## ]] To add the binary numbers ...  \n",
      "1901                                _OVERTHINK_FAILURE_  \n",
      "1902  [[ ## thought ## ]] To add the binary numbers ...  \n",
      "1903                                _OVERTHINK_FAILURE_  \n",
      "1904  [[ ## thought ## ]] To add the binary numbers ...  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "\n",
      "DataFrame shape: (8000, 22)\n",
      "\n",
      "Successfully extracted data and saved to ../../../data/determinism/opik_determinism_data_temp_00.csv\n",
      "DataFrame after preprocessing:\n",
      "                                span_id span_name  response_time  \\\n",
      "0  0197c181-c40a-773e-a48c-f2fd9a6caf63   q10_r10         11.648   \n",
      "1  0197c181-968a-7f04-91f8-d694be080ada    q10_r9         28.038   \n",
      "2  0197c181-2903-722b-98c9-7d680961893a    q10_r8         11.675   \n",
      "3  0197c180-fb68-7c6c-bb8a-865d9e9577ac    q10_r7         28.049   \n",
      "4  0197c180-8dd7-7187-ac09-5b52f5458ab0    q10_r6         11.668   \n",
      "\n",
      "                                 span_input_question span_output_answer  \\\n",
      "0  What is the sum of the binary numbers 11100101...             [c][c]   \n",
      "1  What is the sum of the binary numbers 11100101...          [c][c][c]   \n",
      "2  What is the sum of the binary numbers 11100101...             [c][c]   \n",
      "3  What is the sum of the binary numbers 11100101...          [c][c][c]   \n",
      "4  What is the sum of the binary numbers 11100101...             [c][c]   \n",
      "\n",
      "                              span_output_raw_answer span_correct_answer  \\\n",
      "0  [[ ## thought ## ]] To add the binary numbers ...                   c   \n",
      "1  <think> Okay, let's see. The user is asking fo...                   c   \n",
      "2  [[ ## thought ## ]] To add the binary numbers ...                   c   \n",
      "3  <think> Okay, let's see. The user is asking fo...                   c   \n",
      "4  [[ ## thought ## ]] To add the binary numbers ...                   c   \n",
      "\n",
      "   completion_tokens    question_file                              trace_id  \\\n",
      "0                  0  question_10.txt  0197c174-ebcd-7404-97e0-3ba8fa383e50   \n",
      "1                  0  question_10.txt  0197c174-ebcd-7404-97e0-3ba8fa383e50   \n",
      "2                  0  question_10.txt  0197c174-ebcd-7404-97e0-3ba8fa383e50   \n",
      "3                  0  question_10.txt  0197c174-ebcd-7404-97e0-3ba8fa383e50   \n",
      "4                  0  question_10.txt  0197c174-ebcd-7404-97e0-3ba8fa383e50   \n",
      "\n",
      "   ... language prompting_tech num_runs_per_question model_source  \\\n",
      "0  ...       en             R4                    10        local   \n",
      "1  ...       en             R4                    10        local   \n",
      "2  ...       en             R4                    10        local   \n",
      "3  ...       en             R4                    10        local   \n",
      "4  ...       en             R4                    10        local   \n",
      "\n",
      "   temperature top_p    exercise    question_type is_failure  \\\n",
      "0          0.2   0.1  exam_01_mc  multiple_choice      False   \n",
      "1          0.2   0.1  exam_01_mc  multiple_choice       True   \n",
      "2          0.2   0.1  exam_01_mc  multiple_choice      False   \n",
      "3          0.2   0.1  exam_01_mc  multiple_choice       True   \n",
      "4          0.2   0.1  exam_01_mc  multiple_choice      False   \n",
      "\n",
      "                                              answer  \n",
      "0  [[ ## thought ## ]] To add the binary numbers ...  \n",
      "1                                _OVERTHINK_FAILURE_  \n",
      "2  [[ ## thought ## ]] To add the binary numbers ...  \n",
      "3                                _OVERTHINK_FAILURE_  \n",
      "4  [[ ## thought ## ]] To add the binary numbers ...  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "\n",
      "Filtered DataFrame for temperature = 0.2:\n",
      "DataFrame head:\n",
      "                                span_id span_name  response_time  \\\n",
      "0  0197c181-c40a-773e-a48c-f2fd9a6caf63   q10_r10         11.648   \n",
      "1  0197c181-968a-7f04-91f8-d694be080ada    q10_r9         28.038   \n",
      "2  0197c181-2903-722b-98c9-7d680961893a    q10_r8         11.675   \n",
      "3  0197c180-fb68-7c6c-bb8a-865d9e9577ac    q10_r7         28.049   \n",
      "4  0197c180-8dd7-7187-ac09-5b52f5458ab0    q10_r6         11.668   \n",
      "\n",
      "                                 span_input_question span_output_answer  \\\n",
      "0  What is the sum of the binary numbers 11100101...             [c][c]   \n",
      "1  What is the sum of the binary numbers 11100101...          [c][c][c]   \n",
      "2  What is the sum of the binary numbers 11100101...             [c][c]   \n",
      "3  What is the sum of the binary numbers 11100101...          [c][c][c]   \n",
      "4  What is the sum of the binary numbers 11100101...             [c][c]   \n",
      "\n",
      "                              span_output_raw_answer span_correct_answer  \\\n",
      "0  [[ ## thought ## ]] To add the binary numbers ...                   c   \n",
      "1  <think> Okay, let's see. The user is asking fo...                   c   \n",
      "2  [[ ## thought ## ]] To add the binary numbers ...                   c   \n",
      "3  <think> Okay, let's see. The user is asking fo...                   c   \n",
      "4  [[ ## thought ## ]] To add the binary numbers ...                   c   \n",
      "\n",
      "   completion_tokens    question_file                              trace_id  \\\n",
      "0                  0  question_10.txt  0197c174-ebcd-7404-97e0-3ba8fa383e50   \n",
      "1                  0  question_10.txt  0197c174-ebcd-7404-97e0-3ba8fa383e50   \n",
      "2                  0  question_10.txt  0197c174-ebcd-7404-97e0-3ba8fa383e50   \n",
      "3                  0  question_10.txt  0197c174-ebcd-7404-97e0-3ba8fa383e50   \n",
      "4                  0  question_10.txt  0197c174-ebcd-7404-97e0-3ba8fa383e50   \n",
      "\n",
      "   ... language prompting_tech num_runs_per_question model_source  \\\n",
      "0  ...       en             R4                    10        local   \n",
      "1  ...       en             R4                    10        local   \n",
      "2  ...       en             R4                    10        local   \n",
      "3  ...       en             R4                    10        local   \n",
      "4  ...       en             R4                    10        local   \n",
      "\n",
      "   temperature top_p    exercise    question_type is_failure  \\\n",
      "0          0.2   0.1  exam_01_mc  multiple_choice      False   \n",
      "1          0.2   0.1  exam_01_mc  multiple_choice       True   \n",
      "2          0.2   0.1  exam_01_mc  multiple_choice      False   \n",
      "3          0.2   0.1  exam_01_mc  multiple_choice       True   \n",
      "4          0.2   0.1  exam_01_mc  multiple_choice      False   \n",
      "\n",
      "                                              answer  \n",
      "0  [[ ## thought ## ]] To add the binary numbers ...  \n",
      "1                                _OVERTHINK_FAILURE_  \n",
      "2  [[ ## thought ## ]] To add the binary numbers ...  \n",
      "3                                _OVERTHINK_FAILURE_  \n",
      "4  [[ ## thought ## ]] To add the binary numbers ...  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "\n",
      "DataFrame shape: (8000, 22)\n",
      "\n",
      "Successfully extracted data and saved to ../../../data/determinism/opik_determinism_data_temp_02.csv\n",
      "DataFrame after preprocessing:\n",
      "                                span_id span_name  response_time  \\\n",
      "0  0197c181-c40a-773e-a48c-f2fd9a6caf63   q10_r10         11.648   \n",
      "1  0197c181-968a-7f04-91f8-d694be080ada    q10_r9         28.038   \n",
      "2  0197c181-2903-722b-98c9-7d680961893a    q10_r8         11.675   \n",
      "3  0197c180-fb68-7c6c-bb8a-865d9e9577ac    q10_r7         28.049   \n",
      "4  0197c180-8dd7-7187-ac09-5b52f5458ab0    q10_r6         11.668   \n",
      "\n",
      "                                 span_input_question span_output_answer  \\\n",
      "0  What is the sum of the binary numbers 11100101...             [c][c]   \n",
      "1  What is the sum of the binary numbers 11100101...          [c][c][c]   \n",
      "2  What is the sum of the binary numbers 11100101...             [c][c]   \n",
      "3  What is the sum of the binary numbers 11100101...          [c][c][c]   \n",
      "4  What is the sum of the binary numbers 11100101...             [c][c]   \n",
      "\n",
      "                              span_output_raw_answer span_correct_answer  \\\n",
      "0  [[ ## thought ## ]] To add the binary numbers ...                   c   \n",
      "1  <think> Okay, let's see. The user is asking fo...                   c   \n",
      "2  [[ ## thought ## ]] To add the binary numbers ...                   c   \n",
      "3  <think> Okay, let's see. The user is asking fo...                   c   \n",
      "4  [[ ## thought ## ]] To add the binary numbers ...                   c   \n",
      "\n",
      "   completion_tokens    question_file                              trace_id  \\\n",
      "0                  0  question_10.txt  0197c174-ebcd-7404-97e0-3ba8fa383e50   \n",
      "1                  0  question_10.txt  0197c174-ebcd-7404-97e0-3ba8fa383e50   \n",
      "2                  0  question_10.txt  0197c174-ebcd-7404-97e0-3ba8fa383e50   \n",
      "3                  0  question_10.txt  0197c174-ebcd-7404-97e0-3ba8fa383e50   \n",
      "4                  0  question_10.txt  0197c174-ebcd-7404-97e0-3ba8fa383e50   \n",
      "\n",
      "   ... language prompting_tech num_runs_per_question model_source  \\\n",
      "0  ...       en             R4                    10        local   \n",
      "1  ...       en             R4                    10        local   \n",
      "2  ...       en             R4                    10        local   \n",
      "3  ...       en             R4                    10        local   \n",
      "4  ...       en             R4                    10        local   \n",
      "\n",
      "   temperature top_p    exercise    question_type is_failure  \\\n",
      "0          0.2   0.1  exam_01_mc  multiple_choice      False   \n",
      "1          0.2   0.1  exam_01_mc  multiple_choice       True   \n",
      "2          0.2   0.1  exam_01_mc  multiple_choice      False   \n",
      "3          0.2   0.1  exam_01_mc  multiple_choice       True   \n",
      "4          0.2   0.1  exam_01_mc  multiple_choice      False   \n",
      "\n",
      "                                              answer  \n",
      "0  [[ ## thought ## ]] To add the binary numbers ...  \n",
      "1                                _OVERTHINK_FAILURE_  \n",
      "2  [[ ## thought ## ]] To add the binary numbers ...  \n",
      "3                                _OVERTHINK_FAILURE_  \n",
      "4  [[ ## thought ## ]] To add the binary numbers ...  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "\n",
      "Filtered DataFrame for temperature = 0.4:\n",
      "DataFrame head:\n",
      "                                   span_id span_name  response_time  \\\n",
      "8800  0197bfa2-22e0-7158-9914-369c496a6164   q10_r10         47.023   \n",
      "8801  0197bfa1-6b31-73b7-a020-7572a1f78bee    q10_r9         44.168   \n",
      "8802  0197bfa0-bea9-7c45-9b8d-e7edc3434b4a    q10_r8         40.583   \n",
      "8803  0197bfa0-2022-7ab0-b9b4-c2af5ff68c38    q10_r7         58.287   \n",
      "8804  0197bf9f-3c73-7c84-b055-7ee0388ca2e2    q10_r6         58.033   \n",
      "\n",
      "                                    span_input_question  \\\n",
      "8800  What is the sum of the binary numbers 11100101...   \n",
      "8801  What is the sum of the binary numbers 11100101...   \n",
      "8802  What is the sum of the binary numbers 11100101...   \n",
      "8803  What is the sum of the binary numbers 11100101...   \n",
      "8804  What is the sum of the binary numbers 11100101...   \n",
      "\n",
      "                                 span_output_answer  \\\n",
      "8800                                         [c][c]   \n",
      "8801                                            [c]   \n",
      "8802                                         [c][c]   \n",
      "8803                 [a][b][c][d][e][f][c][e][c][c]   \n",
      "8804  [a][b][c][d][e][f][c][a][b][c][d][e][f][c][c]   \n",
      "\n",
      "                                 span_output_raw_answer span_correct_answer  \\\n",
      "8800  [[ ## thought ## ]] To find the sum of the bin...                   c   \n",
      "8801  [[ ## thought ## ]] To find the sum of the bin...                   c   \n",
      "8802  [[ ## thought ## ]] To find the sum of the bin...                   c   \n",
      "8803  <think> Okay, let's see. The user is asking fo...                   c   \n",
      "8804  <think> Okay, let's see. The user is asking fo...                   c   \n",
      "\n",
      "      completion_tokens    question_file  \\\n",
      "8800                  0  question_10.txt   \n",
      "8801                  0  question_10.txt   \n",
      "8802                  0  question_10.txt   \n",
      "8803                  0  question_10.txt   \n",
      "8804                  0  question_10.txt   \n",
      "\n",
      "                                  trace_id  ... language prompting_tech  \\\n",
      "8800  0197bf7c-6d43-75f9-9122-e9392c1c7728  ...       en             R4   \n",
      "8801  0197bf7c-6d43-75f9-9122-e9392c1c7728  ...       en             R4   \n",
      "8802  0197bf7c-6d43-75f9-9122-e9392c1c7728  ...       en             R4   \n",
      "8803  0197bf7c-6d43-75f9-9122-e9392c1c7728  ...       en             R4   \n",
      "8804  0197bf7c-6d43-75f9-9122-e9392c1c7728  ...       en             R4   \n",
      "\n",
      "     num_runs_per_question model_source  temperature top_p    exercise  \\\n",
      "8800                    10        local          0.4   0.1  exam_01_mc   \n",
      "8801                    10        local          0.4   0.1  exam_01_mc   \n",
      "8802                    10        local          0.4   0.1  exam_01_mc   \n",
      "8803                    10        local          0.4   0.1  exam_01_mc   \n",
      "8804                    10        local          0.4   0.1  exam_01_mc   \n",
      "\n",
      "        question_type is_failure  \\\n",
      "8800  multiple_choice      False   \n",
      "8801  multiple_choice      False   \n",
      "8802  multiple_choice      False   \n",
      "8803  multiple_choice       True   \n",
      "8804  multiple_choice       True   \n",
      "\n",
      "                                                 answer  \n",
      "8800  [[ ## thought ## ]] To find the sum of the bin...  \n",
      "8801  [[ ## thought ## ]] To find the sum of the bin...  \n",
      "8802  [[ ## thought ## ]] To find the sum of the bin...  \n",
      "8803                                _OVERTHINK_FAILURE_  \n",
      "8804                                _OVERTHINK_FAILURE_  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "\n",
      "DataFrame shape: (8000, 22)\n",
      "\n",
      "Successfully extracted data and saved to ../../../data/determinism/opik_determinism_data_temp_04.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if all_opik_data:\n",
    "    full_df = pd.DataFrame(all_opik_data).copy()\n",
    "    print(f\"\\nFull DataFrame loaded with shape: {full_df.shape}\")\n",
    "    \n",
    "    full_df = preprocess_and_identify_failures(full_df)\n",
    "    \n",
    "    # Save full dataframe\n",
    "    filter_and_save_dataframe(full_df)\n",
    "\n",
    "    # Dataframe with temperature=0.0\n",
    "    filter_and_save_dataframe(full_df, csv_filename=\"opik_determinism_data_temp_00.csv\", temperature_filter=0.0)\n",
    "\n",
    "    # Dataframe with temperature=0.2\n",
    "    filter_and_save_dataframe(full_df, csv_filename=\"opik_determinism_data_temp_02.csv\", temperature_filter=0.2)\n",
    "    \n",
    "    # Dataframe with temperature=0.4\n",
    "    filter_and_save_dataframe(full_df, csv_filename=\"opik_determinism_data_temp_04.csv\", temperature_filter=0.4)\n",
    "\n",
    "else:\n",
    "    print(\"No data fetched from Opik to create any CSV files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84f01819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame loaded from ../../../data/determinism/opik_determinism_data_temp_00.csv with shape: (8000, 22)\n",
      "MODELS:  ['Qwen3:1.7b' 'Qwen3:0.6b' 'smollm2:1.7b' 'DeepSeek R1:1.5b' 'Gemma3:4b'\n",
      " 'Gemma3:1b' 'Qwen3:4b' 'TinyLlama:1.1b' 'Llama3.2:1b' 'Moondream 2']\n",
      "\n",
      "Columns in                           run_name model_display_name    question_file answer  \\\n",
      "0           run_047_qwen3:1.7b_0.0         Qwen3:1.7b  question_01.txt      a   \n",
      "1           run_047_qwen3:1.7b_0.0         Qwen3:1.7b  question_01.txt      a   \n",
      "2           run_047_qwen3:1.7b_0.0         Qwen3:1.7b  question_01.txt      a   \n",
      "3           run_047_qwen3:1.7b_0.0         Qwen3:1.7b  question_01.txt      a   \n",
      "4           run_047_qwen3:1.7b_0.0         Qwen3:1.7b  question_01.txt      a   \n",
      "...                            ...                ...              ...    ...   \n",
      "3995  run_001_deepseek-r1:1.5b_0.0   DeepSeek R1:1.5b  question_10.txt   None   \n",
      "3996  run_001_deepseek-r1:1.5b_0.0   DeepSeek R1:1.5b  question_10.txt   None   \n",
      "3997  run_001_deepseek-r1:1.5b_0.0   DeepSeek R1:1.5b  question_10.txt   None   \n",
      "3998  run_001_deepseek-r1:1.5b_0.0   DeepSeek R1:1.5b  question_10.txt   None   \n",
      "3999  run_001_deepseek-r1:1.5b_0.0   DeepSeek R1:1.5b  question_10.txt   None   \n",
      "\n",
      "     correct_answer prompting_tech  is_failure  \n",
      "0                 b             R4       False  \n",
      "1                 b             R4       False  \n",
      "2                 b             R4       False  \n",
      "3                 b             R4       False  \n",
      "4                 b             R4       False  \n",
      "...             ...            ...         ...  \n",
      "3995              c             R1        True  \n",
      "3996              c             R1        True  \n",
      "3997              c             R1        True  \n",
      "3998              c             R1        True  \n",
      "3999              c             R1        True  \n",
      "\n",
      "[4000 rows x 7 columns]:\n",
      "['run_name', 'model_display_name', 'question_file', 'answer', 'correct_answer', 'prompting_tech', 'is_failure']\n",
      "No successful runs for group ('DeepSeek R1:1.5b', 'question_01.txt', 'R2')\n",
      "No successful runs for group ('DeepSeek R1:1.5b', 'question_04.txt', 'R3')\n",
      "No successful runs for group ('DeepSeek R1:1.5b', 'question_09.txt', 'R2')\n",
      "No successful runs for group ('DeepSeek R1:1.5b', 'question_10.txt', 'R4')\n",
      "No successful runs for group ('Moondream 2', 'question_01.txt', 'R1')\n",
      "No successful runs for group ('Moondream 2', 'question_01.txt', 'R2')\n",
      "No successful runs for group ('Moondream 2', 'question_01.txt', 'R3')\n",
      "No successful runs for group ('Moondream 2', 'question_01.txt', 'R4')\n",
      "No successful runs for group ('Moondream 2', 'question_03.txt', 'R1')\n",
      "No successful runs for group ('Moondream 2', 'question_03.txt', 'R2')\n",
      "No successful runs for group ('Moondream 2', 'question_05.txt', 'R1')\n",
      "No successful runs for group ('Moondream 2', 'question_05.txt', 'R2')\n",
      "No successful runs for group ('Moondream 2', 'question_05.txt', 'R3')\n",
      "No successful runs for group ('Moondream 2', 'question_05.txt', 'R4')\n",
      "No successful runs for group ('Moondream 2', 'question_06.txt', 'R1')\n",
      "No successful runs for group ('Moondream 2', 'question_06.txt', 'R2')\n",
      "No successful runs for group ('Moondream 2', 'question_07.txt', 'R1')\n",
      "No successful runs for group ('Moondream 2', 'question_07.txt', 'R2')\n",
      "No successful runs for group ('Moondream 2', 'question_07.txt', 'R3')\n",
      "No successful runs for group ('Moondream 2', 'question_07.txt', 'R4')\n",
      "No successful runs for group ('Moondream 2', 'question_09.txt', 'R1')\n",
      "No successful runs for group ('Moondream 2', 'question_09.txt', 'R2')\n",
      "No successful runs for group ('Moondream 2', 'question_09.txt', 'R3')\n",
      "No successful runs for group ('Moondream 2', 'question_09.txt', 'R4')\n",
      "No successful runs for group ('Moondream 2', 'question_10.txt', 'R1')\n",
      "No successful runs for group ('Moondream 2', 'question_10.txt', 'R2')\n",
      "No successful runs for group ('Moondream 2', 'question_10.txt', 'R3')\n",
      "No successful runs for group ('Moondream 2', 'question_10.txt', 'R4')\n",
      "No successful runs for group ('Qwen3:0.6b', 'question_04.txt', 'R3')\n",
      "No successful runs for group ('Qwen3:0.6b', 'question_04.txt', 'R4')\n",
      "No successful runs for group ('Qwen3:0.6b', 'question_07.txt', 'R3')\n",
      "No successful runs for group ('Qwen3:0.6b', 'question_08.txt', 'R3')\n",
      "No successful runs for group ('Qwen3:0.6b', 'question_09.txt', 'R2')\n",
      "No successful runs for group ('Qwen3:0.6b', 'question_10.txt', 'R4')\n",
      "No successful runs for group ('Qwen3:1.7b', 'question_10.txt', 'R4')\n",
      "\n",
      "--- Determinism Results Table ---\n",
      "    Model Display Name    Question File Prompting Tech  Determinism Score  \\\n",
      "0     DeepSeek R1:1.5b  question_01.txt             R1                0.6   \n",
      "1     DeepSeek R1:1.5b  question_01.txt             R2                0.0   \n",
      "2     DeepSeek R1:1.5b  question_01.txt             R3                1.0   \n",
      "3     DeepSeek R1:1.5b  question_01.txt             R4                0.6   \n",
      "4     DeepSeek R1:1.5b  question_02.txt             R1                0.8   \n",
      "..                 ...              ...            ...                ...   \n",
      "395       smollm2:1.7b  question_09.txt             R4                0.9   \n",
      "396       smollm2:1.7b  question_10.txt             R1                0.7   \n",
      "397       smollm2:1.7b  question_10.txt             R2                1.0   \n",
      "398       smollm2:1.7b  question_10.txt             R3                1.0   \n",
      "399       smollm2:1.7b  question_10.txt             R4                1.0   \n",
      "\n",
      "     Number of Runs  Number of Failures  \\\n",
      "0                10                   1   \n",
      "1                10                  10   \n",
      "2                10                   5   \n",
      "3                10                   2   \n",
      "4                10                   0   \n",
      "..              ...                 ...   \n",
      "395              10                   0   \n",
      "396              10                   0   \n",
      "397              10                   0   \n",
      "398              10                   0   \n",
      "399              10                   0   \n",
      "\n",
      "                                           All Answers  \n",
      "0                 [a, a, b, b, b, b, b, f, None, None]  \n",
      "1    [None, None, None, None, None, None, None, Non...  \n",
      "2                       [a, a, a, a, a, b, b, b, b, b]  \n",
      "3                 [a, a, b, b, b, b, b, f, None, None]  \n",
      "4                    [a, d, d, d, d, d, d, d, d, None]  \n",
      "..                                                 ...  \n",
      "395                     [a, a, a, a, a, a, a, a, a, e]  \n",
      "396            [f, f, f, f, f, f, f, None, None, None]  \n",
      "397  [None, None, None, None, None, None, None, Non...  \n",
      "398                     [f, f, f, f, f, f, f, f, f, f]  \n",
      "399                     [a, a, a, a, a, a, a, a, a, a]  \n",
      "\n",
      "[400 rows x 7 columns]\n",
      "\n",
      "Determinism table saved to: ../../../data/determinism_tables/determinism_table_temp_00_mc.csv\n",
      "MODELS:  ['Llama3.2:1b' 'Moondream 2' 'DeepSeek R1:1.5b' 'Gemma3:1b'\n",
      " 'TinyLlama:1.1b' 'smollm2:1.7b' 'Gemma3:4b' 'Qwen3:4b' 'Qwen3:1.7b'\n",
      " 'Qwen3:0.6b']\n",
      "\n",
      "Columns in                      run_name model_display_name    question_file  \\\n",
      "0     run_015_llama3.2:1b_0.0        Llama3.2:1b  question_01.txt   \n",
      "1     run_002_llama3.2:1b_0.0        Llama3.2:1b  question_01.txt   \n",
      "2     run_002_llama3.2:1b_0.0        Llama3.2:1b  question_01.txt   \n",
      "3     run_002_llama3.2:1b_0.0        Llama3.2:1b  question_01.txt   \n",
      "4     run_002_llama3.2:1b_0.0        Llama3.2:1b  question_01.txt   \n",
      "...                       ...                ...              ...   \n",
      "3995   run_046_qwen3:0.6b_0.0         Qwen3:0.6b  question_10.txt   \n",
      "3996   run_046_qwen3:0.6b_0.0         Qwen3:0.6b  question_10.txt   \n",
      "3997   run_046_qwen3:0.6b_0.0         Qwen3:0.6b  question_10.txt   \n",
      "3998   run_009_qwen3:0.6b_0.0         Qwen3:0.6b  question_10.txt   \n",
      "3999   run_046_qwen3:0.6b_0.0         Qwen3:0.6b  question_10.txt   \n",
      "\n",
      "                                                 answer correct_answer  \\\n",
      "0     ## Step 1: Understand what natural binary is N...       00111101   \n",
      "1     ## Step 1: Understand what natural binary is N...       00111101   \n",
      "2     ## Step 1: Understand what natural binary is N...       00111101   \n",
      "3     ## Step 1: Understand what natural binary is N...       00111101   \n",
      "4     ## Step 1: Understand what natural binary is N...       00111101   \n",
      "...                                                 ...            ...   \n",
      "3995  [[ ## thought ## ]] thought  To find the sum o...     1101000101   \n",
      "3996  [[ ## thought ## ]] thought  To find the sum o...     1101000101   \n",
      "3997  [[ ## thought ## ]] thought  To find the sum o...     1101000101   \n",
      "3998  [[ ## thought ## ]] thought  To find the sum o...     1101000101   \n",
      "3999  [[ ## thought ## ]] thought  To find the sum o...     1101000101   \n",
      "\n",
      "     prompting_tech  is_failure  \n",
      "0                R3       False  \n",
      "1                R1       False  \n",
      "2                R1       False  \n",
      "3                R1       False  \n",
      "4                R1       False  \n",
      "...             ...         ...  \n",
      "3995             R4       False  \n",
      "3996             R4       False  \n",
      "3997             R4       False  \n",
      "3998             R2       False  \n",
      "3999             R4       False  \n",
      "\n",
      "[4000 rows x 7 columns]:\n",
      "['run_name', 'model_display_name', 'question_file', 'answer', 'correct_answer', 'prompting_tech', 'is_failure']\n",
      "No successful runs for group ('DeepSeek R1:1.5b', 'question_09.txt', 'R3')\n",
      "No successful runs for group ('DeepSeek R1:1.5b', 'question_10.txt', 'R3')\n",
      "No successful runs for group ('Moondream 2', 'question_02.txt', 'R1')\n",
      "No successful runs for group ('Moondream 2', 'question_02.txt', 'R2')\n",
      "No successful runs for group ('Moondream 2', 'question_02.txt', 'R3')\n",
      "No successful runs for group ('Moondream 2', 'question_03.txt', 'R1')\n",
      "No successful runs for group ('Moondream 2', 'question_03.txt', 'R2')\n",
      "No successful runs for group ('Moondream 2', 'question_07.txt', 'R1')\n",
      "No successful runs for group ('Moondream 2', 'question_07.txt', 'R2')\n",
      "No successful runs for group ('Moondream 2', 'question_07.txt', 'R3')\n",
      "No successful runs for group ('Qwen3:1.7b', 'question_04.txt', 'R2')\n",
      "No successful runs for group ('Qwen3:1.7b', 'question_09.txt', 'R1')\n",
      "No successful runs for group ('Qwen3:1.7b', 'question_10.txt', 'R1')\n",
      "No successful runs for group ('Qwen3:1.7b', 'question_10.txt', 'R3')\n",
      "No successful runs for group ('Qwen3:1.7b', 'question_10.txt', 'R4')\n",
      "No successful runs for group ('Qwen3:4b', 'question_04.txt', 'R3')\n",
      "No successful runs for group ('Qwen3:4b', 'question_10.txt', 'R2')\n",
      "No successful runs for group ('Qwen3:4b', 'question_10.txt', 'R4')\n",
      "\n",
      "--- Determinism Results Table ---\n",
      "    Model Display Name    Question File Prompting Tech  Determinism Score  \\\n",
      "0     DeepSeek R1:1.5b  question_01.txt             R1                0.1   \n",
      "1     DeepSeek R1:1.5b  question_01.txt             R2                0.1   \n",
      "2     DeepSeek R1:1.5b  question_01.txt             R3                0.1   \n",
      "3     DeepSeek R1:1.5b  question_01.txt             R4                0.1   \n",
      "4     DeepSeek R1:1.5b  question_02.txt             R1                0.1   \n",
      "..                 ...              ...            ...                ...   \n",
      "395       smollm2:1.7b  question_09.txt             R4                0.1   \n",
      "396       smollm2:1.7b  question_10.txt             R1                0.1   \n",
      "397       smollm2:1.7b  question_10.txt             R2                0.1   \n",
      "398       smollm2:1.7b  question_10.txt             R3                0.1   \n",
      "399       smollm2:1.7b  question_10.txt             R4                0.1   \n",
      "\n",
      "     Number of Runs  Number of Failures  \\\n",
      "0                10                   1   \n",
      "1                10                   0   \n",
      "2                10                   0   \n",
      "3                10                   0   \n",
      "4                10                   0   \n",
      "..              ...                 ...   \n",
      "395              10                   0   \n",
      "396              10                   0   \n",
      "397              10                   0   \n",
      "398              10                   0   \n",
      "399              10                   0   \n",
      "\n",
      "                                           All Answers  \n",
      "0    [<think> Okay, so I need to figure out how to ...  \n",
      "1    [The decimal number 61 in natural binary is re...  \n",
      "2    [The binary representation of the decimal numb...  \n",
      "3    [The decimal number 61 in natural binary is re...  \n",
      "4    [The decimal number 61 converts to **3D** in h...  \n",
      "..                                                 ...  \n",
      "395  [[[ ## question ## ]] The question asks for th...  \n",
      "396  [[[ ## question ## ]] The question asks for th...  \n",
      "397  [[[ ## question ## ]] The question asks for th...  \n",
      "398  [[[ ## question ## ]] question  [[ ## thought ...  \n",
      "399  [[## question ##] The problem requires us to f...  \n",
      "\n",
      "[400 rows x 7 columns]\n",
      "\n",
      "Determinism table saved to: ../../../data/determinism_tables/determinism_table_temp_00_oa.csv\n",
      "DataFrame loaded from ../../../data/determinism/opik_determinism_data_temp_02.csv with shape: (8000, 22)\n",
      "MODELS:  ['Qwen3:1.7b' 'Qwen3:0.6b' 'smollm2:1.7b' 'Gemma3:4b' 'DeepSeek R1:1.5b'\n",
      " 'Qwen3:4b' 'TinyLlama:1.1b' 'Llama3.2:1b' 'Moondream 2' 'Gemma3:1b']\n",
      "\n",
      "Columns in                           run_name model_display_name    question_file answer  \\\n",
      "0           run_049_qwen3:1.7b_0.2         Qwen3:1.7b  question_01.txt      a   \n",
      "1           run_049_qwen3:1.7b_0.2         Qwen3:1.7b  question_01.txt      a   \n",
      "2           run_049_qwen3:1.7b_0.2         Qwen3:1.7b  question_01.txt      a   \n",
      "3           run_049_qwen3:1.7b_0.2         Qwen3:1.7b  question_01.txt      a   \n",
      "4           run_049_qwen3:1.7b_0.2         Qwen3:1.7b  question_01.txt      a   \n",
      "...                            ...                ...              ...    ...   \n",
      "3995  run_006_deepseek-r1:1.5b_0.2   DeepSeek R1:1.5b  question_10.txt   None   \n",
      "3996  run_006_deepseek-r1:1.5b_0.2   DeepSeek R1:1.5b  question_10.txt   None   \n",
      "3997  run_006_deepseek-r1:1.5b_0.2   DeepSeek R1:1.5b  question_10.txt   None   \n",
      "3998  run_006_deepseek-r1:1.5b_0.2   DeepSeek R1:1.5b  question_10.txt   None   \n",
      "3999  run_006_deepseek-r1:1.5b_0.2   DeepSeek R1:1.5b  question_10.txt   None   \n",
      "\n",
      "     correct_answer prompting_tech  is_failure  \n",
      "0                 b             R4       False  \n",
      "1                 b             R4       False  \n",
      "2                 b             R4       False  \n",
      "3                 b             R4       False  \n",
      "4                 b             R4       False  \n",
      "...             ...            ...         ...  \n",
      "3995              c             R1        True  \n",
      "3996              c             R1        True  \n",
      "3997              c             R1        True  \n",
      "3998              c             R1        True  \n",
      "3999              c             R1        True  \n",
      "\n",
      "[4000 rows x 7 columns]:\n",
      "['run_name', 'model_display_name', 'question_file', 'answer', 'correct_answer', 'prompting_tech', 'is_failure']\n",
      "No successful runs for group ('DeepSeek R1:1.5b', 'question_01.txt', 'R2')\n",
      "No successful runs for group ('DeepSeek R1:1.5b', 'question_01.txt', 'R4')\n",
      "No successful runs for group ('DeepSeek R1:1.5b', 'question_04.txt', 'R1')\n",
      "No successful runs for group ('DeepSeek R1:1.5b', 'question_04.txt', 'R2')\n",
      "No successful runs for group ('DeepSeek R1:1.5b', 'question_04.txt', 'R4')\n",
      "No successful runs for group ('DeepSeek R1:1.5b', 'question_10.txt', 'R3')\n",
      "No successful runs for group ('DeepSeek R1:1.5b', 'question_10.txt', 'R4')\n",
      "No successful runs for group ('Moondream 2', 'question_01.txt', 'R1')\n",
      "No successful runs for group ('Moondream 2', 'question_01.txt', 'R2')\n",
      "No successful runs for group ('Moondream 2', 'question_01.txt', 'R3')\n",
      "No successful runs for group ('Moondream 2', 'question_01.txt', 'R4')\n",
      "No successful runs for group ('Moondream 2', 'question_03.txt', 'R1')\n",
      "No successful runs for group ('Moondream 2', 'question_03.txt', 'R2')\n",
      "No successful runs for group ('Moondream 2', 'question_05.txt', 'R1')\n",
      "No successful runs for group ('Moondream 2', 'question_05.txt', 'R2')\n",
      "No successful runs for group ('Moondream 2', 'question_05.txt', 'R3')\n",
      "No successful runs for group ('Moondream 2', 'question_05.txt', 'R4')\n",
      "No successful runs for group ('Moondream 2', 'question_06.txt', 'R1')\n",
      "No successful runs for group ('Moondream 2', 'question_06.txt', 'R2')\n",
      "No successful runs for group ('Moondream 2', 'question_07.txt', 'R1')\n",
      "No successful runs for group ('Moondream 2', 'question_07.txt', 'R2')\n",
      "No successful runs for group ('Moondream 2', 'question_07.txt', 'R3')\n",
      "No successful runs for group ('Moondream 2', 'question_07.txt', 'R4')\n",
      "No successful runs for group ('Moondream 2', 'question_09.txt', 'R1')\n",
      "No successful runs for group ('Moondream 2', 'question_09.txt', 'R2')\n",
      "No successful runs for group ('Moondream 2', 'question_09.txt', 'R3')\n",
      "No successful runs for group ('Moondream 2', 'question_09.txt', 'R4')\n",
      "No successful runs for group ('Moondream 2', 'question_10.txt', 'R1')\n",
      "No successful runs for group ('Moondream 2', 'question_10.txt', 'R2')\n",
      "No successful runs for group ('Moondream 2', 'question_10.txt', 'R3')\n",
      "No successful runs for group ('Moondream 2', 'question_10.txt', 'R4')\n",
      "No successful runs for group ('Qwen3:0.6b', 'question_04.txt', 'R1')\n",
      "No successful runs for group ('Qwen3:0.6b', 'question_04.txt', 'R3')\n",
      "No successful runs for group ('Qwen3:0.6b', 'question_04.txt', 'R4')\n",
      "No successful runs for group ('Qwen3:0.6b', 'question_05.txt', 'R4')\n",
      "No successful runs for group ('Qwen3:0.6b', 'question_06.txt', 'R4')\n",
      "No successful runs for group ('Qwen3:0.6b', 'question_07.txt', 'R1')\n",
      "No successful runs for group ('Qwen3:0.6b', 'question_07.txt', 'R2')\n",
      "No successful runs for group ('Qwen3:0.6b', 'question_07.txt', 'R4')\n",
      "No successful runs for group ('Qwen3:0.6b', 'question_09.txt', 'R2')\n",
      "No successful runs for group ('Qwen3:0.6b', 'question_10.txt', 'R4')\n",
      "No successful runs for group ('Qwen3:1.7b', 'question_10.txt', 'R2')\n",
      "No successful runs for group ('Qwen3:1.7b', 'question_10.txt', 'R4')\n",
      "No successful runs for group ('Qwen3:4b', 'question_10.txt', 'R3')\n",
      "\n",
      "--- Determinism Results Table ---\n",
      "    Model Display Name    Question File Prompting Tech  Determinism Score  \\\n",
      "0     DeepSeek R1:1.5b  question_01.txt             R1                1.0   \n",
      "1     DeepSeek R1:1.5b  question_01.txt             R2                0.0   \n",
      "2     DeepSeek R1:1.5b  question_01.txt             R3                0.7   \n",
      "3     DeepSeek R1:1.5b  question_01.txt             R4                0.0   \n",
      "4     DeepSeek R1:1.5b  question_02.txt             R1                1.0   \n",
      "..                 ...              ...            ...                ...   \n",
      "395       smollm2:1.7b  question_09.txt             R4                0.9   \n",
      "396       smollm2:1.7b  question_10.txt             R1                1.0   \n",
      "397       smollm2:1.7b  question_10.txt             R2                1.0   \n",
      "398       smollm2:1.7b  question_10.txt             R3                1.0   \n",
      "399       smollm2:1.7b  question_10.txt             R4                1.0   \n",
      "\n",
      "     Number of Runs  Number of Failures  \\\n",
      "0                10                   9   \n",
      "1                10                  10   \n",
      "2                10                   4   \n",
      "3                10                  10   \n",
      "4                10                   0   \n",
      "..              ...                 ...   \n",
      "395              10                   0   \n",
      "396              10                   0   \n",
      "397              10                   0   \n",
      "398              10                   0   \n",
      "399              10                   0   \n",
      "\n",
      "                                           All Answers  \n",
      "0                       [a, a, a, a, a, a, a, a, a, b]  \n",
      "1    [None, None, None, None, None, None, None, Non...  \n",
      "2              [a, b, b, b, b, b, f, None, None, None]  \n",
      "3                       [b, b, b, b, b, b, b, b, b, f]  \n",
      "4    [None, None, None, None, None, None, None, Non...  \n",
      "..                                                 ...  \n",
      "395                     [a, a, a, a, a, a, a, a, a, e]  \n",
      "396                     [f, f, f, f, f, f, f, f, f, f]  \n",
      "397  [None, None, None, None, None, None, None, Non...  \n",
      "398                     [f, f, f, f, f, f, f, f, f, f]  \n",
      "399                     [a, a, a, a, a, a, a, a, a, a]  \n",
      "\n",
      "[400 rows x 7 columns]\n",
      "\n",
      "Determinism table saved to: ../../../data/determinism_tables/determinism_table_temp_02_mc.csv\n",
      "MODELS:  ['Llama3.2:1b' 'Moondream 2' 'Gemma3:1b' 'TinyLlama:1.1b'\n",
      " 'DeepSeek R1:1.5b' 'smollm2:1.7b' 'Gemma3:4b' 'Qwen3:4b' 'Qwen3:1.7b'\n",
      " 'Qwen3:0.6b']\n",
      "\n",
      "Columns in                      run_name model_display_name    question_file  \\\n",
      "0     run_017_llama3.2:1b_0.2        Llama3.2:1b  question_01.txt   \n",
      "1     run_005_llama3.2:1b_0.2        Llama3.2:1b  question_01.txt   \n",
      "2     run_005_llama3.2:1b_0.2        Llama3.2:1b  question_01.txt   \n",
      "3     run_005_llama3.2:1b_0.2        Llama3.2:1b  question_01.txt   \n",
      "4     run_005_llama3.2:1b_0.2        Llama3.2:1b  question_01.txt   \n",
      "...                       ...                ...              ...   \n",
      "3995   run_048_qwen3:0.6b_0.2         Qwen3:0.6b  question_10.txt   \n",
      "3996   run_048_qwen3:0.6b_0.2         Qwen3:0.6b  question_10.txt   \n",
      "3997   run_048_qwen3:0.6b_0.2         Qwen3:0.6b  question_10.txt   \n",
      "3998   run_013_qwen3:0.6b_0.2         Qwen3:0.6b  question_10.txt   \n",
      "3999   run_048_qwen3:0.6b_0.2         Qwen3:0.6b  question_10.txt   \n",
      "\n",
      "                                                 answer correct_answer  \\\n",
      "0     ## Step 1: Understand what natural binary is N...       00111101   \n",
      "1     ## Step 1: Understand what natural binary is N...       00111101   \n",
      "2     ## Step 1: Understand what natural binary is N...       00111101   \n",
      "3     ## Step 1: Understand what natural binary is N...       00111101   \n",
      "4     ## Step 1: Understand what natural binary is N...       00111101   \n",
      "...                                                 ...            ...   \n",
      "3995  [[ ## thought ## ]] thought  To find the sum o...     1101000101   \n",
      "3996  [[ ## thought ## ]] thought  To find the sum o...     1101000101   \n",
      "3997  [[ ## thought ## ]] thought  To find the sum o...     1101000101   \n",
      "3998  [[ ## thought ## ]] thought  To find the sum o...     1101000101   \n",
      "3999  [[ ## thought ## ]] thought  To find the sum o...     1101000101   \n",
      "\n",
      "     prompting_tech  is_failure  \n",
      "0                R3       False  \n",
      "1                R1       False  \n",
      "2                R1       False  \n",
      "3                R1       False  \n",
      "4                R1       False  \n",
      "...             ...         ...  \n",
      "3995             R4       False  \n",
      "3996             R4       False  \n",
      "3997             R4       False  \n",
      "3998             R2       False  \n",
      "3999             R4       False  \n",
      "\n",
      "[4000 rows x 7 columns]:\n",
      "['run_name', 'model_display_name', 'question_file', 'answer', 'correct_answer', 'prompting_tech', 'is_failure']\n",
      "No successful runs for group ('DeepSeek R1:1.5b', 'question_04.txt', 'R2')\n",
      "No successful runs for group ('DeepSeek R1:1.5b', 'question_05.txt', 'R4')\n",
      "No successful runs for group ('DeepSeek R1:1.5b', 'question_09.txt', 'R4')\n",
      "No successful runs for group ('DeepSeek R1:1.5b', 'question_10.txt', 'R1')\n",
      "No successful runs for group ('DeepSeek R1:1.5b', 'question_10.txt', 'R4')\n",
      "No successful runs for group ('Moondream 2', 'question_02.txt', 'R1')\n",
      "No successful runs for group ('Moondream 2', 'question_02.txt', 'R2')\n",
      "No successful runs for group ('Moondream 2', 'question_02.txt', 'R3')\n",
      "No successful runs for group ('Moondream 2', 'question_03.txt', 'R1')\n",
      "No successful runs for group ('Moondream 2', 'question_03.txt', 'R2')\n",
      "No successful runs for group ('Moondream 2', 'question_07.txt', 'R1')\n",
      "No successful runs for group ('Moondream 2', 'question_07.txt', 'R2')\n",
      "No successful runs for group ('Moondream 2', 'question_07.txt', 'R3')\n",
      "No successful runs for group ('Qwen3:1.7b', 'question_04.txt', 'R2')\n",
      "No successful runs for group ('Qwen3:1.7b', 'question_09.txt', 'R1')\n",
      "No successful runs for group ('Qwen3:1.7b', 'question_10.txt', 'R3')\n",
      "No successful runs for group ('Qwen3:1.7b', 'question_10.txt', 'R4')\n",
      "No successful runs for group ('Qwen3:4b', 'question_04.txt', 'R3')\n",
      "No successful runs for group ('Qwen3:4b', 'question_10.txt', 'R1')\n",
      "No successful runs for group ('Qwen3:4b', 'question_10.txt', 'R2')\n",
      "No successful runs for group ('Qwen3:4b', 'question_10.txt', 'R4')\n",
      "\n",
      "--- Determinism Results Table ---\n",
      "    Model Display Name    Question File Prompting Tech  Determinism Score  \\\n",
      "0     DeepSeek R1:1.5b  question_01.txt             R1                0.1   \n",
      "1     DeepSeek R1:1.5b  question_01.txt             R2                0.1   \n",
      "2     DeepSeek R1:1.5b  question_01.txt             R3                0.1   \n",
      "3     DeepSeek R1:1.5b  question_01.txt             R4                0.1   \n",
      "4     DeepSeek R1:1.5b  question_02.txt             R1                0.1   \n",
      "..                 ...              ...            ...                ...   \n",
      "395       smollm2:1.7b  question_09.txt             R4                0.1   \n",
      "396       smollm2:1.7b  question_10.txt             R1                0.1   \n",
      "397       smollm2:1.7b  question_10.txt             R2                0.1   \n",
      "398       smollm2:1.7b  question_10.txt             R3                0.1   \n",
      "399       smollm2:1.7b  question_10.txt             R4                0.1   \n",
      "\n",
      "     Number of Runs  Number of Failures  \\\n",
      "0                10                   0   \n",
      "1                10                   0   \n",
      "2                10                   0   \n",
      "3                10                   0   \n",
      "4                10                   0   \n",
      "..              ...                 ...   \n",
      "395              10                   0   \n",
      "396              10                   0   \n",
      "397              10                   0   \n",
      "398              10                   0   \n",
      "399              10                   0   \n",
      "\n",
      "                                           All Answers  \n",
      "0    [The decimal number 61 in natural binary is re...  \n",
      "1    [The decimal number 61 in natural binary is re...  \n",
      "2    [The binary representation of the decimal numb...  \n",
      "3    [The decimal number 61 in natural binary is re...  \n",
      "4    [The decimal number 61 converts to hexadecimal...  \n",
      "..                                                 ...  \n",
      "395  [[[ ## question ## ]] The question asks for th...  \n",
      "396  [[[ ## question ## ]] The question asks for th...  \n",
      "397  [[## question ##] The problem requires us to f...  \n",
      "398  [[[ ## question ## ]] The question asks for th...  \n",
      "399  [[## question ##] The problem requires us to f...  \n",
      "\n",
      "[400 rows x 7 columns]\n",
      "\n",
      "Determinism table saved to: ../../../data/determinism_tables/determinism_table_temp_02_oa.csv\n",
      "DataFrame loaded from ../../../data/determinism/opik_determinism_data_temp_04.csv with shape: (8000, 22)\n",
      "MODELS:  ['Qwen3:1.7b' 'Qwen3:0.6b' 'smollm2:1.7b' 'Llama3.2:1b' 'Gemma3:1b'\n",
      " 'DeepSeek R1:1.5b' 'Gemma3:4b' 'Qwen3:4b' 'TinyLlama:1.1b' 'Moondream 2']\n",
      "\n",
      "Columns in                           run_name model_display_name    question_file answer  \\\n",
      "0           run_011_qwen3:1.7b_0.4         Qwen3:1.7b  question_01.txt      a   \n",
      "1           run_011_qwen3:0.6b_0.4         Qwen3:0.6b  question_01.txt      a   \n",
      "2           run_011_qwen3:0.6b_0.4         Qwen3:0.6b  question_01.txt      a   \n",
      "3           run_011_qwen3:0.6b_0.4         Qwen3:0.6b  question_01.txt      a   \n",
      "4           run_011_qwen3:0.6b_0.4         Qwen3:0.6b  question_01.txt      a   \n",
      "...                            ...                ...              ...    ...   \n",
      "3995  run_001_deepseek-r1:1.5b_0.4   DeepSeek R1:1.5b  question_10.txt   None   \n",
      "3996  run_001_deepseek-r1:1.5b_0.4   DeepSeek R1:1.5b  question_10.txt   None   \n",
      "3997  run_001_deepseek-r1:1.5b_0.4   DeepSeek R1:1.5b  question_10.txt   None   \n",
      "3998  run_001_deepseek-r1:1.5b_0.4   DeepSeek R1:1.5b  question_10.txt   None   \n",
      "3999  run_001_deepseek-r1:1.5b_0.4   DeepSeek R1:1.5b  question_10.txt   None   \n",
      "\n",
      "     correct_answer prompting_tech  is_failure  \n",
      "0                 b             R4       False  \n",
      "1                 b             R4        True  \n",
      "2                 b             R4        True  \n",
      "3                 b             R4        True  \n",
      "4                 b             R4        True  \n",
      "...             ...            ...         ...  \n",
      "3995              c             R1        True  \n",
      "3996              c             R1        True  \n",
      "3997              c             R1        True  \n",
      "3998              c             R1        True  \n",
      "3999              c             R1        True  \n",
      "\n",
      "[4000 rows x 7 columns]:\n",
      "['run_name', 'model_display_name', 'question_file', 'answer', 'correct_answer', 'prompting_tech', 'is_failure']\n",
      "No successful runs for group ('DeepSeek R1:1.5b', 'question_04.txt', 'R1')\n",
      "No successful runs for group ('DeepSeek R1:1.5b', 'question_08.txt', 'R1')\n",
      "No successful runs for group ('DeepSeek R1:1.5b', 'question_09.txt', 'R1')\n",
      "No successful runs for group ('DeepSeek R1:1.5b', 'question_10.txt', 'R1')\n",
      "No successful runs for group ('DeepSeek R1:1.5b', 'question_10.txt', 'R2')\n",
      "No successful runs for group ('DeepSeek R1:1.5b', 'question_10.txt', 'R3')\n",
      "No successful runs for group ('DeepSeek R1:1.5b', 'question_10.txt', 'R4')\n",
      "No successful runs for group ('Gemma3:1b', 'question_08.txt', 'R1')\n",
      "No successful runs for group ('Gemma3:4b', 'question_06.txt', 'R1')\n",
      "No successful runs for group ('Gemma3:4b', 'question_07.txt', 'R1')\n",
      "No successful runs for group ('Moondream 2', 'question_01.txt', 'R1')\n",
      "No successful runs for group ('Moondream 2', 'question_01.txt', 'R2')\n",
      "No successful runs for group ('Moondream 2', 'question_01.txt', 'R3')\n",
      "No successful runs for group ('Moondream 2', 'question_01.txt', 'R4')\n",
      "No successful runs for group ('Moondream 2', 'question_03.txt', 'R1')\n",
      "No successful runs for group ('Moondream 2', 'question_03.txt', 'R2')\n",
      "No successful runs for group ('Moondream 2', 'question_05.txt', 'R1')\n",
      "No successful runs for group ('Moondream 2', 'question_05.txt', 'R2')\n",
      "No successful runs for group ('Moondream 2', 'question_05.txt', 'R3')\n",
      "No successful runs for group ('Moondream 2', 'question_05.txt', 'R4')\n",
      "No successful runs for group ('Moondream 2', 'question_06.txt', 'R1')\n",
      "No successful runs for group ('Moondream 2', 'question_06.txt', 'R2')\n",
      "No successful runs for group ('Moondream 2', 'question_07.txt', 'R1')\n",
      "No successful runs for group ('Moondream 2', 'question_07.txt', 'R2')\n",
      "No successful runs for group ('Moondream 2', 'question_07.txt', 'R3')\n",
      "No successful runs for group ('Moondream 2', 'question_07.txt', 'R4')\n",
      "No successful runs for group ('Moondream 2', 'question_09.txt', 'R1')\n",
      "No successful runs for group ('Moondream 2', 'question_09.txt', 'R2')\n",
      "No successful runs for group ('Moondream 2', 'question_09.txt', 'R3')\n",
      "No successful runs for group ('Moondream 2', 'question_09.txt', 'R4')\n",
      "No successful runs for group ('Moondream 2', 'question_10.txt', 'R1')\n",
      "No successful runs for group ('Moondream 2', 'question_10.txt', 'R2')\n",
      "No successful runs for group ('Moondream 2', 'question_10.txt', 'R3')\n",
      "No successful runs for group ('Moondream 2', 'question_10.txt', 'R4')\n",
      "No successful runs for group ('Qwen3:0.6b', 'question_04.txt', 'R1')\n",
      "No successful runs for group ('Qwen3:0.6b', 'question_04.txt', 'R3')\n",
      "No successful runs for group ('Qwen3:0.6b', 'question_04.txt', 'R4')\n",
      "No successful runs for group ('Qwen3:0.6b', 'question_07.txt', 'R1')\n",
      "No successful runs for group ('Qwen3:0.6b', 'question_07.txt', 'R3')\n",
      "No successful runs for group ('Qwen3:1.7b', 'question_10.txt', 'R4')\n",
      "No successful runs for group ('Qwen3:4b', 'question_01.txt', 'R1')\n",
      "No successful runs for group ('Qwen3:4b', 'question_04.txt', 'R1')\n",
      "No successful runs for group ('Qwen3:4b', 'question_10.txt', 'R1')\n",
      "\n",
      "--- Determinism Results Table ---\n",
      "    Model Display Name    Question File Prompting Tech  Determinism Score  \\\n",
      "0     DeepSeek R1:1.5b  question_01.txt             R1                1.0   \n",
      "1     DeepSeek R1:1.5b  question_01.txt             R2                0.8   \n",
      "2     DeepSeek R1:1.5b  question_01.txt             R3                0.8   \n",
      "3     DeepSeek R1:1.5b  question_01.txt             R4                0.6   \n",
      "4     DeepSeek R1:1.5b  question_02.txt             R1                0.9   \n",
      "..                 ...              ...            ...                ...   \n",
      "395       smollm2:1.7b  question_09.txt             R4                0.7   \n",
      "396       smollm2:1.7b  question_10.txt             R1                0.9   \n",
      "397       smollm2:1.7b  question_10.txt             R2                0.6   \n",
      "398       smollm2:1.7b  question_10.txt             R3                0.7   \n",
      "399       smollm2:1.7b  question_10.txt             R4                0.9   \n",
      "\n",
      "     Number of Runs  Number of Failures  \\\n",
      "0                10                   0   \n",
      "1                10                   5   \n",
      "2                10                   4   \n",
      "3                10                   1   \n",
      "4                10                   0   \n",
      "..              ...                 ...   \n",
      "395              10                   0   \n",
      "396              10                   0   \n",
      "397              10                   0   \n",
      "398              10                   0   \n",
      "399              10                   0   \n",
      "\n",
      "                                           All Answers  \n",
      "0                       [b, b, b, b, b, b, b, b, b, b]  \n",
      "1              [a, a, a, b, b, b, b, None, None, None]  \n",
      "2              [a, a, b, b, b, b, b, None, None, None]  \n",
      "3        [a, b, b, b, b, None, None, None, None, None]  \n",
      "4                    [d, d, d, d, d, d, d, d, d, None]  \n",
      "..                                                 ...  \n",
      "395                     [a, a, a, a, a, a, a, f, f, f]  \n",
      "396  [f, None, None, None, None, None, None, None, ...  \n",
      "397   [a, a, f, f, None, None, None, None, None, None]  \n",
      "398            [f, f, f, f, f, f, f, None, None, None]  \n",
      "399                  [a, a, a, a, a, a, a, a, a, None]  \n",
      "\n",
      "[400 rows x 7 columns]\n",
      "\n",
      "Determinism table saved to: ../../../data/determinism_tables/determinism_table_temp_04_mc.csv\n",
      "MODELS:  ['Llama3.2:1b' 'Moondream 2' 'DeepSeek R1:1.5b' 'Gemma3:1b'\n",
      " 'TinyLlama:1.1b' 'smollm2:1.7b' 'Gemma3:4b' 'Qwen3:4b' 'Qwen3:1.7b'\n",
      " 'Qwen3:0.6b']\n",
      "\n",
      "Columns in                           run_name model_display_name    question_file  \\\n",
      "0          run_008_llama3.2:1b_0.4        Llama3.2:1b  question_01.txt   \n",
      "1          run_008_llama3.2:1b_0.4        Llama3.2:1b  question_01.txt   \n",
      "2          run_008_llama3.2:1b_0.4        Llama3.2:1b  question_01.txt   \n",
      "3          run_008_llama3.2:1b_0.4        Llama3.2:1b  question_01.txt   \n",
      "4          run_008_llama3.2:1b_0.4        Llama3.2:1b  question_01.txt   \n",
      "...                            ...                ...              ...   \n",
      "3995  run_001_deepseek-r1:1.5b_0.4   DeepSeek R1:1.5b  question_10.txt   \n",
      "3996  run_001_deepseek-r1:1.5b_0.4   DeepSeek R1:1.5b  question_10.txt   \n",
      "3997  run_001_deepseek-r1:1.5b_0.4   DeepSeek R1:1.5b  question_10.txt   \n",
      "3998  run_001_deepseek-r1:1.5b_0.4   DeepSeek R1:1.5b  question_10.txt   \n",
      "3999  run_001_deepseek-r1:1.5b_0.4   DeepSeek R1:1.5b  question_10.txt   \n",
      "\n",
      "                                                 answer correct_answer  \\\n",
      "0     ## Step 1: Understand what natural binary is N...       00111101   \n",
      "1     ## Step 1: Understand what natural binary is N...       00111101   \n",
      "2     ## Step 1: Understand what natural binary is N...       00111101   \n",
      "3     ## Step 1: Understand what natural binary is N...       00111101   \n",
      "4     ## Step 1: Understand what natural binary is N...       00111101   \n",
      "...                                                 ...            ...   \n",
      "3995                                                NaN     1101000101   \n",
      "3996                                                NaN     1101000101   \n",
      "3997                                                NaN     1101000101   \n",
      "3998                                                NaN     1101000101   \n",
      "3999                                                NaN     1101000101   \n",
      "\n",
      "     prompting_tech  is_failure  \n",
      "0                R3       False  \n",
      "1                R3       False  \n",
      "2                R3       False  \n",
      "3                R3       False  \n",
      "4                R3       False  \n",
      "...             ...         ...  \n",
      "3995             R1        True  \n",
      "3996             R1        True  \n",
      "3997             R1        True  \n",
      "3998             R1        True  \n",
      "3999             R1        True  \n",
      "\n",
      "[4000 rows x 7 columns]:\n",
      "['run_name', 'model_display_name', 'question_file', 'answer', 'correct_answer', 'prompting_tech', 'is_failure']\n",
      "No successful runs for group ('DeepSeek R1:1.5b', 'question_10.txt', 'R1')\n",
      "No successful runs for group ('Gemma3:1b', 'question_06.txt', 'R1')\n",
      "No successful runs for group ('Gemma3:1b', 'question_08.txt', 'R1')\n",
      "No successful runs for group ('Gemma3:4b', 'question_10.txt', 'R1')\n",
      "No successful runs for group ('Moondream 2', 'question_02.txt', 'R1')\n",
      "No successful runs for group ('Moondream 2', 'question_02.txt', 'R2')\n",
      "No successful runs for group ('Moondream 2', 'question_02.txt', 'R3')\n",
      "No successful runs for group ('Moondream 2', 'question_03.txt', 'R1')\n",
      "No successful runs for group ('Moondream 2', 'question_03.txt', 'R2')\n",
      "No successful runs for group ('Moondream 2', 'question_07.txt', 'R1')\n",
      "No successful runs for group ('Moondream 2', 'question_07.txt', 'R2')\n",
      "No successful runs for group ('Moondream 2', 'question_07.txt', 'R3')\n",
      "No successful runs for group ('Qwen3:1.7b', 'question_04.txt', 'R3')\n",
      "No successful runs for group ('Qwen3:1.7b', 'question_04.txt', 'R4')\n",
      "No successful runs for group ('Qwen3:1.7b', 'question_10.txt', 'R1')\n",
      "No successful runs for group ('Qwen3:1.7b', 'question_10.txt', 'R2')\n",
      "No successful runs for group ('Qwen3:1.7b', 'question_10.txt', 'R3')\n",
      "No successful runs for group ('Qwen3:4b', 'question_04.txt', 'R1')\n",
      "No successful runs for group ('Qwen3:4b', 'question_09.txt', 'R1')\n",
      "No successful runs for group ('Qwen3:4b', 'question_10.txt', 'R1')\n",
      "No successful runs for group ('Qwen3:4b', 'question_10.txt', 'R4')\n",
      "\n",
      "--- Determinism Results Table ---\n",
      "    Model Display Name    Question File Prompting Tech  Determinism Score  \\\n",
      "0     DeepSeek R1:1.5b  question_01.txt             R1                0.1   \n",
      "1     DeepSeek R1:1.5b  question_01.txt             R2                0.1   \n",
      "2     DeepSeek R1:1.5b  question_01.txt             R3                0.1   \n",
      "3     DeepSeek R1:1.5b  question_01.txt             R4                0.1   \n",
      "4     DeepSeek R1:1.5b  question_02.txt             R1                0.1   \n",
      "..                 ...              ...            ...                ...   \n",
      "395       smollm2:1.7b  question_09.txt             R4                0.1   \n",
      "396       smollm2:1.7b  question_10.txt             R1                0.1   \n",
      "397       smollm2:1.7b  question_10.txt             R2                0.1   \n",
      "398       smollm2:1.7b  question_10.txt             R3                0.1   \n",
      "399       smollm2:1.7b  question_10.txt             R4                0.1   \n",
      "\n",
      "     Number of Runs  Number of Failures  \\\n",
      "0                10                   0   \n",
      "1                10                   0   \n",
      "2                10                   0   \n",
      "3                10                   2   \n",
      "4                10                   0   \n",
      "..              ...                 ...   \n",
      "395              10                   0   \n",
      "396              10                   0   \n",
      "397              10                   0   \n",
      "398              10                   0   \n",
      "399              10                   0   \n",
      "\n",
      "                                           All Answers  \n",
      "0    [The decimal number 61 in natural binary is re...  \n",
      "1    [The binary representation of the decimal numb...  \n",
      "2    [The binary representation of the decimal numb...  \n",
      "3    [<think> Okay, so I need to figure out how to ...  \n",
      "4    [The decimal number 61 converts to **3D** in h...  \n",
      "..                                                 ...  \n",
      "395  [[[ ## question ## ]] The question asks for th...  \n",
      "396  [[[ ## question ## ]] question  [[ ## thought ...  \n",
      "397  [[[ ## question ## ]] The question asks for th...  \n",
      "398  [[[ ## question ## ]] The question asks for th...  \n",
      "399  [[[ ## question ## ]] The question asks for th...  \n",
      "\n",
      "[400 rows x 7 columns]\n",
      "\n",
      "Determinism table saved to: ../../../data/determinism_tables/determinism_table_temp_04_oa.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TEMPERATURE = 0.0\n",
    "csv_filename=\"opik_determinism_data_temp_00.csv\"\n",
    "temp_00_df = get_dataframe_from_csv(csv_filename=csv_filename)\n",
    "# Multiple choice answers\n",
    "temp_00_mc_df = temp_00_df[temp_00_df['question_type'] == 'multiple_choice'].copy()\n",
    "output_filename = \"determinism_table_temp_00_mc.csv\"\n",
    "generate_determinism_table_mc(temp_00_mc_df, filename=output_filename)\n",
    "# Open answer questions\n",
    "temp_00_oa_df = temp_00_df[temp_00_df['question_type'] == 'open_answer'].copy()\n",
    "output_filename = \"determinism_table_temp_00_oa.csv\"\n",
    "generate_determinism_table_oa(temp_00_oa_df, filename=output_filename)\n",
    "\n",
    "\n",
    "# TEMPERATURE = 0.2\n",
    "csv_filename=\"opik_determinism_data_temp_02.csv\"\n",
    "temp_02_df = get_dataframe_from_csv(csv_filename=csv_filename)\n",
    "# Multiple choice answers\n",
    "temp_02_mc_df = temp_02_df[temp_02_df['question_type'] == 'multiple_choice'].copy()\n",
    "output_filename = \"determinism_table_temp_02_mc.csv\"\n",
    "generate_determinism_table_mc(temp_02_mc_df, filename=output_filename)\n",
    "# Open answer questions\n",
    "temp_02_oa_df = temp_02_df[temp_02_df['question_type'] == 'open_answer'].copy()\n",
    "output_filename = \"determinism_table_temp_02_oa.csv\"\n",
    "generate_determinism_table_oa(temp_02_oa_df, filename=output_filename)\n",
    "\n",
    "# TEMPERATURE = 0.4\n",
    "csv_filename=\"opik_determinism_data_temp_04.csv\"\n",
    "temp_04_df = get_dataframe_from_csv(csv_filename=csv_filename)\n",
    "# Multiple choice answers\n",
    "temp_04_mc_df = temp_04_df[temp_04_df['question_type'] == 'multiple_choice'].copy()\n",
    "output_filename = \"determinism_table_temp_04_mc.csv\"\n",
    "generate_determinism_table_mc(temp_04_mc_df, filename=output_filename)\n",
    "# Open answer questions\n",
    "temp_04_oa_df = temp_04_df[temp_04_df['question_type'] == 'open_answer'].copy()\n",
    "output_filename = \"determinism_table_temp_04_oa.csv\"\n",
    "generate_determinism_table_oa(temp_04_oa_df, filename=output_filename)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
