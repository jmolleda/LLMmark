{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "8b68a26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from typing import Dict, Any, List, Optional, Callable\n",
    "from opik import Opik\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "OPIK_DETERMINISM_PROJECT_NAME = \"LLMmark_determinism\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83a55c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def parse_model_info(model_name: str, tags: List[str]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Parses model ID, display name, and size from the model name and tags.\n",
    "    Adjust this function based on your actual model naming conventions.\n",
    "    \"\"\"\n",
    "    model_id = model_name\n",
    "    model_size = \"N/A\"\n",
    "    question_type = \"N/A\"\n",
    "\n",
    "    match = re.search(r':([\\d\\.]+)b', model_name)\n",
    "    if match:\n",
    "        model_size = match.group(1) + \"B\"\n",
    "\n",
    "    if \"multiple_choice\" in tags:\n",
    "        question_type = \"multiple_choice\"\n",
    "    elif \"open_answer\" in tags:\n",
    "        question_type = \"open_answer\"\n",
    "\n",
    "    return {\n",
    "        \"model_id\": model_id,\n",
    "        \"model_size\": model_size,\n",
    "        \"question_type\": question_type\n",
    "    }\n",
    "\n",
    "def get_opik_flat_data_for_csv(project_name: str = OPIK_DETERMINISM_PROJECT_NAME) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Fetches detailed trace and span data from Opik and flattens it for CSV export.\n",
    "    Each dictionary in the returned list represents a single span,\n",
    "    including its parent trace's metadata.\n",
    "    \"\"\"\n",
    "    client = Opik()\n",
    "    flat_data = []\n",
    "\n",
    "    print(f\"Fetching traces from project: {project_name}...\")\n",
    "\n",
    "    traces = client.search_traces(\n",
    "        project_name=project_name,\n",
    "        max_results=25000\n",
    "    )\n",
    "\n",
    "    if not traces:\n",
    "        print(f\"No traces found in project '{project_name}'. Please check the project name and your Opik configuration.\")\n",
    "        return []\n",
    "    \n",
    "    # Delete traces with None values\n",
    "    traces = [trace for trace in traces if trace.name is not None]\n",
    "\n",
    "    for i, trace in enumerate(traces):\n",
    "        \n",
    "        print(f\"Processing trace {i+1}/{len(traces)}: {trace.name} ({trace.id})\")\n",
    "\n",
    "        trace_content = client.get_trace_content(trace.id)\n",
    "        spans = client.search_spans(project_name=project_name, trace_id=trace.id)\n",
    "\n",
    "        if not spans:\n",
    "            print(f\"  No spans found for trace {trace.id}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        model_info = parse_model_info(trace.name, trace.tags)\n",
    "        \n",
    "        model_source = \"N/A\"\n",
    "        if \"local\" in trace.tags:\n",
    "            model_source = \"local\"\n",
    "        elif \"online\" in trace.tags:\n",
    "            model_source = \"online\"\n",
    "\n",
    "        trace_flat_metadata = {\n",
    "            \"trace_id\": trace.id,\n",
    "            \"run_name\": trace.name,\n",
    "            \"model_display_name\": trace_content.metadata.get(\"model_display_name\"),\n",
    "            \"language\": trace_content.metadata.get(\"language\", \"en\"),\n",
    "            \"prompting_tech\": trace_content.metadata.get(\"prompting_tech\", \"N/A\"),\n",
    "            \"num_runs_per_question\": trace_content.metadata.get(\"num_runs_per_question\", 1),\n",
    "            \"model_source\": model_source,\n",
    "            \"temperature\": trace_content.metadata.get(\"temperature\", \"N/A\"),\n",
    "            \"top_p\": trace_content.metadata.get(\"top_p\", 0.1),\n",
    "            \"exercise\": trace_content.metadata.get(\"exercise\", \"N/A\"),\n",
    "            \"question_type\": model_info[\"question_type\"],\n",
    "            **{f\"trace_meta_{k.replace('.', '_')}\": v for k, v in trace_content.metadata.items() # Replace '.' in keys for valid column names\n",
    "            if k not in [\"language\", \"prompting_tech\", \"num_runs_per_question\", \n",
    "                         \"model_source\", \"temperature\", \"top_p\", \"exercise\", \n",
    "                         \"prompt_tech\", \"question_type\", \"comments\", \"model_id\", \"model_display_name\", \"top-p\", \"run_name\"]}\n",
    "        }\n",
    "\n",
    "        # Process each span and combine with trace-level metadata\n",
    "        for j, span in enumerate(spans):\n",
    "            span_response_time_ms = span.output.get(\"response_time (s)\", \"N/A\")\n",
    "\n",
    "            span_input_question = span.input.get(\"question\", str(span.input)) if isinstance(span.input, dict) else str(span.input)\n",
    "            span_output_answer = span.output.get(\"answer\", str(span.output)) if isinstance(span.output, dict) else str(span.output)\n",
    "            span_output_raw_answer = span.output.get(\"raw_answer\", span_output_answer) if isinstance(span.output, dict) else span_output_answer\n",
    "\n",
    "            correct_answer = span.metadata.get(\"correct_answer\", \"PLACEHOLDER_CORRECT_ANSWER\")\n",
    "\n",
    "            span_data_row = {\n",
    "                \"span_id\": span.id,\n",
    "                \"span_name\": span.name,\n",
    "                \"response_time_ms\": span_response_time_ms,\n",
    "                \"span_input_question\": span_input_question,\n",
    "                \"span_output_answer\": span_output_answer,\n",
    "                \"span_output_raw_answer\": span_output_raw_answer,\n",
    "                \"span_correct_answer\": correct_answer,\n",
    "                \"question_file\": span.metadata.get(\"question_file\", \"N/A\"),\n",
    "            }\n",
    "            \n",
    "            combined_row = {**span_data_row, **trace_flat_metadata}\n",
    "            flat_data.append(combined_row)\n",
    "\n",
    "    return flat_data\n",
    "\n",
    "def filter_and_save_dataframe(\n",
    "    df: pd.DataFrame, \n",
    "    csv_filename: str = \"opik_determinism_data.csv\", \n",
    "    temperature_filter: Optional[float] = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Filters an existing DataFrame by temperature and saves it to a CSV file.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        print(\"\\nInput DataFrame is empty. No CSV file will be created.\")\n",
    "        return\n",
    "\n",
    "    filtered_df = df.copy()\n",
    "\n",
    "    \n",
    "    if temperature_filter is not None:\n",
    "        filtered_df['temperature'] = pd.to_numeric(filtered_df['temperature'], errors='coerce')\n",
    "        filtered_df = filtered_df[filtered_df['temperature'] == temperature_filter].copy()\n",
    "        print(f\"\\nFiltered DataFrame for temperature = {temperature_filter}:\")\n",
    "\n",
    "    if filtered_df.empty:\n",
    "        print(f\"No data after filtering for temperature = {temperature_filter}.\")\n",
    "        return\n",
    "\n",
    "    output_filename = csv_filename\n",
    "    output_dir = '../../../data/determinism'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "    \n",
    "    filtered_df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(\"DataFrame head:\")\n",
    "    print(filtered_df.head())\n",
    "    print(f\"\\nDataFrame shape: {filtered_df.shape}\")\n",
    "\n",
    "    print(f\"\\nSuccessfully extracted data and saved to {output_path}\")\n",
    "      \n",
    "def get_dataframe_from_csv(csv_filename: str = \"opik_determinism_data.csv\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads a CSV file into a DataFrame.\n",
    "    \"\"\"\n",
    "    output_dir = '../../../data/determinism'\n",
    "    output_path = os.path.join(output_dir, csv_filename)\n",
    "    \n",
    "    if not os.path.exists(output_path):\n",
    "        print(f\"CSV file {output_path} does not exist.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = pd.read_csv(output_path, encoding='utf-8')\n",
    "    print(f\"DataFrame loaded from {output_path} with shape: {df.shape}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Analyse multiple_choice answers\n",
    "def calculate_determinism_mc(answers):\n",
    "    \"\"\"Calculates the determinism of a list of answers (multiple choice).\n",
    "\n",
    "    Args:\n",
    "        answers (list): A list of answers to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        float: The determinism score for the given answers.\n",
    "    \"\"\"\n",
    "    unique_answers = set(answers)\n",
    "\n",
    "    if len(unique_answers) == 1:\n",
    "        return 1.0  # Completely deterministic\n",
    "    else:\n",
    "        # Calculate the proportion of the most frequent answer\n",
    "        counter = Counter(answers)\n",
    "        most_frequent_answer = counter.most_common(1)[0][1]\n",
    "        return most_frequent_answer / len(answers)\n",
    "\n",
    "def extract_answer_letter_mc(answer_text: str) -> Optional[str]: \n",
    "    \"\"\"\n",
    "    Extracts the answer letter from the given text.\n",
    "    The expected format is: [a]\n",
    "    If multiple bracketed letters follow, only the first one is returned.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Match the pattern [letter]\n",
    "        match = re.search(r'\\[([a-zA-Z])\\]', answer_text)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting answer letter: {e}\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def process_determinism_and_store(df: pd.DataFrame, determinism_function: Callable[[List[str]], float]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates determinism for each question_file, model_display_name, and prompting_tech,\n",
    "    and stores the results in a new DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df: The input DataFrame containing LLM experiment results.\n",
    "        determinism_function: The function used to calculate determinism.\n",
    "\n",
    "    Returns:\n",
    "        A new DataFrame summarizing the determinism results.\n",
    "    \"\"\"\n",
    "    results_list = []\n",
    "\n",
    "    required_columns = ['model_display_name', 'question_file', 'prompting_tech', 'answer']\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        raise ValueError(f\"Input DataFrame must contain all of these columns: {required_columns}\")\n",
    "\n",
    "    unique_models = df['model_display_name'].unique()\n",
    "\n",
    "    for model in unique_models:\n",
    "        model_df = df[df['model_display_name'] == model].copy()\n",
    "\n",
    "        # Group by question_file and prompting_tech\n",
    "        grouped = model_df.groupby(['question_file', 'prompting_tech'])\n",
    "\n",
    "        for (question_file, prompting_tech), group in grouped:\n",
    "            answers_for_determinism = group['answer'].tolist()\n",
    "            determinism_score = determinism_function(answers_for_determinism)\n",
    "\n",
    "            results_list.append({\n",
    "                'Model Display Name': model,\n",
    "                'Question File': question_file,\n",
    "                'Prompting Tech': prompting_tech,\n",
    "                'Determinism Score': determinism_score,\n",
    "                'Number of Runs': len(group),\n",
    "                'All Answers': answers_for_determinism\n",
    "            })\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    print(\"Determinism calculation complete.\")\n",
    "\n",
    "    results_df = results_df.sort_values(by=['Model Display Name', 'Question File', 'Prompting Tech'])\n",
    "    return results_df\n",
    "\n",
    "\n",
    "def create_determinism_table(df, filename=\"determinism_summary_table.csv\"):\n",
    "    \"\"\"Creates a summary table of determinism scores and saves it to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing LLM experiment results.\n",
    "        filename (str, optional): The name of the output CSV file. Defaults to \"determinism_summary_table.csv\".\n",
    "    \"\"\"\n",
    "    print(f\"\\nColumns in {df}:\")\n",
    "    print(df.columns.tolist())\n",
    "    \n",
    "    df = df.sort_values(by=['model_display_name', 'question_file', 'prompting_tech'])\n",
    "\n",
    "\n",
    "    determinism_table = process_determinism_and_store(df, calculate_determinism_mc)\n",
    "\n",
    "    print(\"\\n--- Determinism Results Table ---\")\n",
    "    print(determinism_table)\n",
    "    \n",
    "    # Save the determinism table to a CSV file\n",
    "    output_dir_tables = '../../../data/determinism_tables'\n",
    "    os.makedirs(output_dir_tables, exist_ok=True)\n",
    "    table_filepath = os.path.join(output_dir_tables, filename)\n",
    "    determinism_table.to_csv(table_filepath, index=False)\n",
    "    print(f\"\\nDeterminism table saved to: {table_filepath}\")\n",
    "    \n",
    "\n",
    "\n",
    "def generate_determinism_table_mc(mc_df, filename):\n",
    "    \"\"\"Generates a determinism table for multiple choice questions.\n",
    "\n",
    "    Args:\n",
    "        mc_df (pd.DataFrame): DataFrame containing multiple choice question data.\n",
    "        filename (str): Name of the output CSV file.\n",
    "    \"\"\"\n",
    "    # Filter by question_type\n",
    "    mc_df = mc_df[mc_df['question_type'] == 'multiple_choice'].copy()\n",
    "\n",
    "    #Extract the answer letter from multiple choice answers\n",
    "    mc_df['answer'] = mc_df['span_output_answer'].apply(extract_answer_letter_mc)\n",
    "\n",
    "    # Rename span_correct_answer to correct_answer\n",
    "    mc_df.rename(columns={'span_correct_answer': 'correct_answer'}, inplace=True)\n",
    "    # Drop all columns except run_name, question_file, answer, span_correct_answer, promtpting_tech\n",
    "    mc_df = mc_df[['run_name', 'model_display_name', 'question_file', 'answer', 'correct_answer', 'prompting_tech']]\n",
    "\n",
    "    # Order by question_file\n",
    "    mc_df = mc_df.sort_values(by=['question_file', 'answer']).reset_index(drop=True)\n",
    "\n",
    "    mc_df.head(5).style.set_table_styles(\n",
    "        [{'selector': 'th', 'props': [('background-color', '#f2f2f2'), ('color', 'black')]}]\n",
    "    ).set_properties(**{'text-align': 'center'})\n",
    "    \n",
    "    \n",
    "    # Get unique model_display_name values\n",
    "    unique_models = mc_df['model_display_name'].unique()\n",
    "    print(\"MODELS: \", unique_models)\n",
    "    \n",
    "    # Create CSV determinism table\n",
    "    create_determinism_table(mc_df, filename=filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "ff02533f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching traces from project: LLMmark_determinism...\n",
      "Processing trace 1/240: run_024_qwen3:4b (0197ad5f-3790-7b31-bc21-2c8b7dacf46e)\n",
      "Processing trace 2/240: run_024_qwen3:1.7b (0197ad53-5121-7526-994e-38e9837f24f4)\n",
      "Processing trace 3/240: run_024_qwen3:0.6b (0197ad49-38d9-7c0f-adaf-61faddb79b82)\n",
      "Processing trace 4/240: run_024_tinyllama:1.1b (0197ad45-8838-7a45-a637-c200af953229)\n",
      "Processing trace 5/240: run_024_smollm2:1.7b (0197ad41-62a9-71c5-a2ca-734c3a8caa0f)\n",
      "Processing trace 6/240: run_024_moondream:1.8b (0197ad41-5941-7945-8b7b-414d2b87fd21)\n",
      "Processing trace 7/240: run_024_llama3.2:1b (0197ad3f-b0ad-75af-b838-347ddaec3d84)\n",
      "Processing trace 8/240: run_024_gemma3:4b (0197ad39-5e59-78cd-a03f-d90595dd9047)\n",
      "Processing trace 9/240: run_024_gemma3:1b (0197ad35-a5f0-7605-9167-aa447c155de1)\n",
      "Processing trace 10/240: run_024_deepseek-r1:1.5b (0197ad27-3834-7027-88a3-4dea2d3aacde)\n",
      "Processing trace 11/240: run_023_qwen3:4b (0197ad15-8c3b-761c-bb22-ee65aee74ff7)\n",
      "Processing trace 12/240: run_023_qwen3:1.7b (0197ad09-a738-7773-95a5-dc5181cdb68b)\n",
      "Processing trace 13/240: run_023_qwen3:0.6b (0197acff-91e5-7c82-af23-826a43522038)\n",
      "Processing trace 14/240: run_023_tinyllama:1.1b (0197acfb-3cfe-7938-a72d-30345a48c6aa)\n",
      "Processing trace 15/240: run_023_smollm2:1.7b (0197acf7-17aa-7825-bae0-f743e4597ecd)\n",
      "Processing trace 16/240: run_023_moondream:1.8b (0197acf7-0c79-7606-8b44-8bdf877b5de6)\n",
      "Processing trace 17/240: run_023_llama3.2:1b (0197acf5-64eb-78a1-8509-b93c09f38cc4)\n",
      "Processing trace 18/240: run_023_gemma3:4b (0197acef-13de-7fd1-ac73-bd59b1875c10)\n",
      "Processing trace 19/240: run_023_gemma3:1b (0197aceb-5423-76b6-baf5-1a5cd7e812d5)\n",
      "Processing trace 20/240: run_023_deepseek-r1:1.5b (0197acdc-e6ae-7a68-812f-12aec2e822b4)\n",
      "Processing trace 21/240: run_022_qwen3:4b (0197accb-3a7c-7ecb-bb35-7bd1fdc63d6c)\n",
      "Processing trace 22/240: run_022_qwen3:1.7b (0197acbf-4ec2-70f3-a1a7-f2d2a253fe07)\n",
      "Processing trace 23/240: run_022_qwen3:0.6b (0197aca9-2846-7a61-9c41-7e5fd8a2305e)\n",
      "Processing trace 24/240: run_021_qwen3:4b (0197aca0-fab4-7a53-8abc-a5a034c1d36e)\n",
      "Processing trace 25/240: run_022_tinyllama:1.1b (0197ac9d-6b08-77d3-9ca6-4557846f990c)\n",
      "Processing trace 26/240: run_022_smollm2:1.7b (0197ac95-e550-7b32-8be9-1c852681e2af)\n",
      "Processing trace 27/240: run_022_moondream:1.8b (0197ac95-d7b7-7731-8bc1-89b52fcb5d5f)\n",
      "Processing trace 28/240: run_022_llama3.2:1b (0197ac92-a7a3-737e-a2de-c2e933aea596)\n",
      "Processing trace 29/240: run_021_qwen3:1.7b (0197ac8f-67f6-705c-b97e-123ab83906f1)\n",
      "Processing trace 30/240: run_022_gemma3:4b (0197ac8a-3214-7e13-8f27-903669ebe657)\n",
      "Processing trace 31/240: run_022_gemma3:1b (0197ac84-62ec-7543-817e-60afb71ac83a)\n",
      "Processing trace 32/240: run_021_qwen3:0.6b (0197ac82-6959-7d8d-8a0b-5076887fe828)\n",
      "Processing trace 33/240: run_021_tinyllama:1.1b (0197ac7c-4cf3-7bd2-bbe1-4e841e55c174)\n",
      "Processing trace 34/240: run_021_smollm2:1.7b (0197ac77-74fe-70f3-a48f-7045ea1f2717)\n",
      "Processing trace 35/240: run_021_moondream:1.8b (0197ac77-5ea6-7a51-bd91-43978d69d694)\n",
      "Processing trace 36/240: run_021_llama3.2:1b (0197ac73-f189-7589-8de0-5b9d60b19ba3)\n",
      "Processing trace 37/240: run_021_gemma3:4b (0197ac6b-ee77-7c4d-9b00-e26903d0972f)\n",
      "Processing trace 38/240: run_022_deepseek-r1:1.5b (0197ac6a-391d-7d12-85f0-4325de425642)\n",
      "Processing trace 39/240: run_021_gemma3:1b (0197ac66-0b65-712a-9d74-c643ef5d62f6)\n",
      "Processing trace 40/240: run_020_qwen3:4b (0197ac50-a7ef-7527-9cb6-bf9bb610f222)\n",
      "Processing trace 41/240: run_020_qwen3:1.7b (0197ac4e-ffb0-7d30-aeae-71cedb205adb)\n",
      "Processing trace 42/240: run_021_deepseek-r1:1.5b (0197ac44-2e24-7f24-8bb6-be95d4a342b0)\n",
      "Processing trace 43/240: run_020_qwen3:0.6b (0197ac32-bea7-79e7-8371-55b9e090aef8)\n",
      "Processing trace 44/240: run_019_qwen3:4b (0197ac2a-fb88-741a-87ad-8e80dc332f32)\n",
      "Processing trace 45/240: run_020_tinyllama:1.1b (0197ac28-9922-7df1-82f9-c1108ab2baa4)\n",
      "Processing trace 46/240: run_020_smollm2:1.7b (0197ac23-81db-7515-a369-7e025cba0827)\n",
      "Processing trace 47/240: run_020_moondream:1.8b (0197ac23-759b-762b-b47f-33a0438ae038)\n",
      "Processing trace 48/240: run_020_llama3.2:1b (0197ac1e-7be7-73b7-9d50-ae54389a2966)\n",
      "Processing trace 49/240: run_019_qwen3:1.7b (0197ac19-0bc4-743a-a5b8-5af7373a889e)\n",
      "Processing trace 50/240: run_020_gemma3:4b (0197ac13-5ead-7983-89c4-848884a0a638)\n",
      "Processing trace 51/240: run_020_gemma3:1b (0197ac0f-fae9-7b8c-a9b7-2800778bedba)\n",
      "Processing trace 52/240: run_019_qwen3:0.6b (0197ac0b-b3b5-7332-a7e7-07833f99970a)\n",
      "Processing trace 53/240: run_019_tinyllama:1.1b (0197ac05-94c4-76c6-85a1-8b2e8afa2d37)\n",
      "Processing trace 54/240: run_019_smollm2:1.7b (0197ac00-b471-7c69-a804-b7c4f44654d8)\n",
      "Processing trace 55/240: run_019_moondream:1.8b (0197ac00-9ddb-75a0-8988-c3b730761ce3)\n",
      "Processing trace 56/240: run_019_llama3.2:1b (0197abfd-3576-7b1b-8c5b-995f4538fb62)\n",
      "Processing trace 57/240: run_019_gemma3:4b (0197abf5-33af-73f1-be94-745b090979ec)\n",
      "Processing trace 58/240: run_020_deepseek-r1:1.5b (0197abf4-c2b9-7e5e-8411-43b86be0b3f1)\n",
      "Processing trace 59/240: run_019_gemma3:1b (0197abee-acde-7e92-a448-52d8e3cce3d8)\n",
      "Processing trace 60/240: run_018_qwen3:4b (0197abdb-3b42-728a-9773-18a5a7aea5c4)\n",
      "Processing trace 61/240: run_018_qwen3:1.7b (0197abd9-924d-7d6f-9fd0-c223347d5692)\n",
      "Processing trace 62/240: run_019_deepseek-r1:1.5b (0197abcd-983b-7397-82f0-65907e2b4bc2)\n",
      "Processing trace 63/240: run_018_qwen3:0.6b (0197abbd-ef81-70c5-9a5b-b9f7f94914fa)\n",
      "Processing trace 64/240: run_017_qwen3:4b (0197abb4-8482-7607-ae7f-090250110243)\n",
      "Processing trace 65/240: run_018_tinyllama:1.1b (0197abb4-3427-7499-a08d-9d8cdfa29758)\n",
      "Processing trace 66/240: run_018_smollm2:1.7b (0197abaf-1995-7020-8b54-9e138fe7d4a0)\n",
      "Processing trace 67/240: run_018_moondream:1.8b (0197abaf-0d99-754e-a982-173f7f970a54)\n",
      "Processing trace 68/240: run_018_llama3.2:1b (0197abaa-0259-7e59-b670-457566ca9ed6)\n",
      "Processing trace 69/240: run_017_qwen3:1.7b (0197aba1-39d5-71d4-ab6c-28297ce9acac)\n",
      "Processing trace 70/240: run_018_gemma3:4b (0197ab9e-bb3f-70b2-8d80-c5d54967ff91)\n",
      "Processing trace 71/240: run_018_gemma3:1b (0197ab9b-6a1d-7ab7-a70c-af6f782d1363)\n",
      "Processing trace 72/240: run_017_qwen3:0.6b (0197ab95-a51e-7803-8d9d-858386808a3b)\n",
      "Processing trace 73/240: run_017_tinyllama:1.1b (0197ab8f-8f01-708d-9545-dd3e45e8eef6)\n",
      "Processing trace 74/240: run_017_smollm2:1.7b (0197ab8a-ad48-7478-ae06-638634849080)\n",
      "Processing trace 75/240: run_017_moondream:1.8b (0197ab8a-9546-735c-b754-fce47d9278be)\n",
      "Processing trace 76/240: run_017_llama3.2:1b (0197ab87-25e1-79f4-9bbb-f07228937fc7)\n",
      "Processing trace 77/240: run_018_deepseek-r1:1.5b (0197ab80-b1c8-7225-9a61-eb06e633cc1d)\n",
      "Processing trace 78/240: run_017_gemma3:4b (0197ab7e-bc2e-7ae9-b8ed-c3dea5c573bb)\n",
      "Processing trace 79/240: run_017_gemma3:1b (0197ab77-d98b-7a4f-8dd4-7c588e2abd7c)\n",
      "Processing trace 80/240: run_016_qwen3:4b (0197ab66-cf3d-75f8-b5af-f36a48f4e115)\n",
      "Processing trace 81/240: run_016_qwen3:1.7b (0197ab65-26bc-7e88-8dd2-61587050470c)\n",
      "Processing trace 82/240: run_017_deepseek-r1:1.5b (0197ab57-c59b-71d4-8fd1-581833cda7e5)\n",
      "Processing trace 83/240: run_016_qwen3:0.6b (0197ab4a-7b37-727d-ad5e-350510aa3b69)\n",
      "Processing trace 84/240: run_016_tinyllama:1.1b (0197ab3f-2157-79f0-932e-d39a38843070)\n",
      "Processing trace 85/240: run_015_qwen3:4b (0197ab3b-decd-7378-81d4-170e0a809240)\n",
      "Processing trace 86/240: run_016_smollm2:1.7b (0197ab39-4546-774e-a011-34a727efa8bd)\n",
      "Processing trace 87/240: run_016_moondream:1.8b (0197ab39-38d5-7e48-8447-276ce1925aa9)\n",
      "Processing trace 88/240: run_016_llama3.2:1b (0197ab34-2cbf-76fd-90b8-d655ea54a6fc)\n",
      "Processing trace 89/240: run_016_gemma3:4b (0197ab28-87c0-78af-880d-0da76d879b38)\n",
      "Processing trace 90/240: run_015_qwen3:1.7b (0197ab27-9045-78cb-83e0-320f8fea225d)\n",
      "Processing trace 91/240: run_016_gemma3:1b (0197ab24-bb03-7161-ae3f-b8be327e9f02)\n",
      "Processing trace 92/240: run_015_qwen3:0.6b (0197ab1c-efb5-77bb-ad56-7f33c78e5530)\n",
      "Processing trace 93/240: run_015_tinyllama:1.1b (0197ab18-d200-7123-be3d-69e325f17676)\n",
      "Processing trace 94/240: run_015_smollm2:1.7b (0197ab12-08e6-7bd9-a52a-c2f7dd1447d9)\n",
      "Processing trace 95/240: run_015_moondream:1.8b (0197ab11-f40e-760f-88c4-5b055a400770)\n",
      "Processing trace 96/240: run_016_deepseek-r1:1.5b (0197ab0b-7f87-7aeb-84a5-c06e339838f3)\n",
      "Processing trace 97/240: run_015_llama3.2:1b (0197ab09-5910-7f77-8faa-26cc22d3d5ad)\n",
      "Processing trace 98/240: run_015_gemma3:4b (0197ab03-26c6-7f62-ae59-21e06ba7501d)\n",
      "Processing trace 99/240: run_015_gemma3:1b (0197aaf6-bf86-7ce0-86e1-7f7983e18093)\n",
      "Processing trace 100/240: run_014_qwen3:4b (0197aaf3-b22a-7a3e-9642-fc3db8fc58cd)\n",
      "Processing trace 101/240: run_014_qwen3:1.7b (0197aadf-59eb-746f-b871-6f6eca015ced)\n",
      "Processing trace 102/240: run_015_deepseek-r1:1.5b (0197aadf-5722-79da-bd0d-aeb255fa3704)\n",
      "Processing trace 103/240: run_014_qwen3:0.6b (0197aac5-20e5-7c96-8e01-2a551da3e006)\n",
      "Processing trace 104/240: run_013_qwen3:4b (0197aac3-5a12-7e2a-aa97-7f48654ae1fc)\n",
      "Processing trace 105/240: run_014_tinyllama:1.1b (0197aab9-83d6-7275-91fd-56a800ec3173)\n",
      "Processing trace 106/240: run_013_qwen3:1.7b (0197aab3-4e88-7811-aec7-cfeaf296b3de)\n",
      "Processing trace 107/240: run_014_smollm2:1.7b (0197aab0-6f08-7d7c-aabf-d015dbb67e6d)\n",
      "Processing trace 108/240: run_014_moondream:1.8b (0197aab0-60f2-78d5-b2fc-4a9cb84dac09)\n",
      "Processing trace 109/240: run_014_llama3.2:1b (0197aaad-5871-7c69-91d9-5e1b2bbb4144)\n",
      "Processing trace 110/240: run_013_qwen3:0.6b (0197aaa6-93cd-76c4-bfba-8bac5f8b67ed)\n",
      "Processing trace 111/240: run_014_gemma3:4b (0197aaa2-4a01-7eef-a7de-f1089b297d2d)\n",
      "Processing trace 112/240: run_013_tinyllama:1.1b (0197aaa1-3791-78c2-a735-0c44b7d9a665)\n",
      "Processing trace 113/240: run_013_smollm2:1.7b (0197aa9b-41af-7dd7-9631-d80236f31eee)\n",
      "Processing trace 114/240: run_013_moondream:1.8b (0197aa9b-2bec-7e06-b641-53529fbb5697)\n",
      "Processing trace 115/240: run_014_gemma3:1b (0197aa99-6d0f-7abb-9df2-9bf7235a631c)\n",
      "Processing trace 116/240: run_013_llama3.2:1b (0197aa92-b8a1-7bf0-b30b-7fa889571992)\n",
      "Processing trace 117/240: run_013_gemma3:4b (0197aa8d-f3a0-74c1-8297-5f56524dd6eb)\n",
      "Processing trace 118/240: run_013_gemma3:1b (0197aa84-d78f-7840-847a-e4c9fa587fb8)\n",
      "Processing trace 119/240: run_014_deepseek-r1:1.5b (0197aa7b-e45c-7ac3-b735-71935632f85a)\n",
      "Processing trace 120/240: run_013_deepseek-r1:1.5b (0197aa66-ac62-7caf-a45b-221474000c2b)\n",
      "Processing trace 121/240: run_011_qwen3:4b (0197aa5c-5a4c-7c59-b54f-1b63ad22fa8c)\n",
      "Processing trace 122/240: run_012_qwen3:4b (0197aa46-27ad-7752-84c1-0e266446e6a3)\n",
      "Processing trace 123/240: run_011_qwen3:1.7b (0197aa42-5ac3-7cfc-851d-26c10659227d)\n",
      "Processing trace 124/240: run_012_qwen3:1.7b (0197aa35-8d5b-74bc-ba96-663d085b7b82)\n",
      "Processing trace 125/240: run_012_qwen3:0.6b (0197aa2c-0b76-7b25-b74d-dde480219710)\n",
      "Processing trace 126/240: run_011_qwen3:0.6b (0197aa2b-885a-76a2-ad5e-318263c4238d)\n",
      "Processing trace 127/240: run_012_tinyllama:1.1b (0197aa28-fa42-793c-882a-4c7e8229f4c2)\n",
      "Processing trace 128/240: run_011_tinyllama:1.1b (0197aa21-d341-7e33-80b9-e397437c9840)\n",
      "Processing trace 129/240: run_012_smollm2:1.7b (0197aa20-183a-7b99-954b-f463d4aad4e9)\n",
      "Processing trace 130/240: run_012_moondream:1.8b (0197aa20-047f-7e08-844a-ae60d4763f24)\n",
      "Processing trace 131/240: run_011_smollm2:1.7b (0197aa1d-64d2-7316-bca9-a68e059f9316)\n",
      "Processing trace 132/240: run_011_moondream:1.8b (0197aa1d-5945-79b9-a2b1-13453a78ba40)\n",
      "Processing trace 133/240: run_011_llama3.2:1b (0197aa1a-baa8-7200-ae44-6323ba0b65d1)\n",
      "Processing trace 134/240: run_012_llama3.2:1b (0197aa15-30be-7e70-8ebd-a4adeb4f76be)\n",
      "Processing trace 135/240: run_012_gemma3:4b (0197aa10-6b4b-75f9-b254-819978f0d479)\n",
      "Processing trace 136/240: run_011_gemma3:4b (0197aa0f-7476-7e74-8c8b-7f9c882f81ec)\n",
      "Processing trace 137/240: run_011_gemma3:1b (0197aa0a-4bc3-7a37-ad6a-f16ae34d3956)\n",
      "Processing trace 138/240: run_012_gemma3:1b (0197aa09-f1e3-7ea8-8b17-be2121345c3a)\n",
      "Processing trace 139/240: run_012_deepseek-r1:1.5b (0197a9f4-b25e-79a8-8ded-e2beeaba5861)\n",
      "Processing trace 140/240: run_011_deepseek-r1:1.5b (0197a9ec-e782-7a04-ba71-e0194ec5817d)\n",
      "Processing trace 141/240: run_010_qwen3:4b (0197a9d6-6153-78eb-8a65-47a60d5e1cc3)\n",
      "Processing trace 142/240: run_009_qwen3:4b (0197a9d2-05ed-746c-ba9f-d4caaad0d78a)\n",
      "Processing trace 143/240: run_010_qwen3:1.7b (0197a9c5-5fb0-76be-b4d8-b3e69bde4a07)\n",
      "Processing trace 144/240: run_009_qwen3:1.7b (0197a9c0-6c44-712f-8c26-b4a3bb387f3c)\n",
      "Processing trace 145/240: run_010_qwen3:0.6b (0197a9bb-6a98-744f-9063-57394a3b6616)\n",
      "Processing trace 146/240: run_010_tinyllama:1.1b (0197a9b7-9289-7f89-9564-6355f50d98c2)\n",
      "Processing trace 147/240: run_010_smollm2:1.7b (0197a9b3-5a5d-7270-b097-b31dab895b5b)\n",
      "Processing trace 148/240: run_010_moondream:1.8b (0197a9b3-4bf9-7b56-b1af-774b1eb706e8)\n",
      "Processing trace 149/240: run_009_qwen3:0.6b (0197a9b2-5a86-7098-989e-dd360a6cb9eb)\n",
      "Processing trace 150/240: run_010_llama3.2:1b (0197a9b1-3a44-76b7-ba54-10ac8592756f)\n",
      "Processing trace 151/240: run_010_gemma3:4b (0197a9ac-9449-732c-bd84-74efec6c1435)\n",
      "Processing trace 152/240: run_009_tinyllama:1.1b (0197a9a7-a601-7877-928b-999ec217bbbc)\n",
      "Processing trace 153/240: run_010_gemma3:1b (0197a9a4-edf9-7a9c-afdd-36951c81b24e)\n",
      "Processing trace 154/240: run_009_smollm2:1.7b (0197a99e-ad74-79b4-b63a-b8201fbf5783)\n",
      "Processing trace 155/240: run_009_moondream:1.8b (0197a99e-9f9a-7774-ae9f-e7e0ab01598a)\n",
      "Processing trace 156/240: run_009_llama3.2:1b (0197a99b-389f-7013-89e4-17740b2ff227)\n",
      "Processing trace 157/240: run_009_gemma3:4b (0197a98e-fb60-794e-8172-b1f0c8de1449)\n",
      "Processing trace 158/240: run_010_deepseek-r1:1.5b (0197a989-ac39-7949-a35d-933638ea008f)\n",
      "Processing trace 159/240: run_009_gemma3:1b (0197a985-3387-7f31-9d7a-88f1abebe547)\n",
      "Processing trace 160/240: run_008_qwen3:4b (0197a96d-9d19-7c92-991a-ed6dec71f253)\n",
      "Processing trace 161/240: run_009_deepseek-r1:1.5b (0197a962-1e71-7ec6-8fe1-501008254d62)\n",
      "Processing trace 162/240: run_008_qwen3:1.7b (0197a958-b16c-720c-bbc9-9d218cd28a75)\n",
      "Processing trace 163/240: run_008_qwen3:0.6b (0197a94d-4f28-7313-8a5f-ecc12e3c4b76)\n",
      "Processing trace 164/240: run_008_tinyllama:1.1b (0197a945-74c5-76fb-bdc1-54b35ab062db)\n",
      "Processing trace 165/240: run_007_qwen3:4b (0197a943-ee33-75b3-bda8-ba408c647c38)\n",
      "Processing trace 166/240: run_007_qwen3:1.7b (0197a941-0b96-7506-ba12-dd78949ab737)\n",
      "Processing trace 167/240: run_008_smollm2:1.7b (0197a940-6f9f-7efa-bbc0-ec086377d459)\n",
      "Processing trace 168/240: run_008_moondream:1.8b (0197a940-5fc5-774a-ac48-383f7a40c192)\n",
      "Processing trace 169/240: run_008_llama3.2:1b (0197a93e-32a1-7078-b923-6176614def35)\n",
      "Processing trace 170/240: run_008_gemma3:4b (0197a939-3cfd-7935-97a6-246a687a6c13)\n",
      "Processing trace 171/240: run_008_gemma3:1b (0197a931-c332-7eff-a837-ca9978b2f18e)\n",
      "Processing trace 172/240: run_007_qwen3:0.6b (0197a929-d096-7769-ae09-d5a9fad0d101)\n",
      "Processing trace 173/240: run_007_tinyllama:1.1b (0197a91f-370a-7be6-8b25-59b6a2e49def)\n",
      "Processing trace 174/240: run_008_deepseek-r1:1.5b (0197a91b-8c34-7cc7-a188-59329eb6fd03)\n",
      "Processing trace 175/240: run_007_smollm2:1.7b (0197a90a-617b-75d9-80f9-371a762c0943)\n",
      "Processing trace 176/240: run_007_moondream:1.8b (0197a90a-53c7-7bbc-9268-90d956c09fea)\n",
      "Processing trace 177/240: run_007_llama3.2:1b (0197a905-3e53-7f1f-b0af-2b1968e9c1e2)\n",
      "Processing trace 178/240: run_006_qwen3:4b (0197a8fa-dae8-70d4-9779-61bf38a5ec38)\n",
      "Processing trace 179/240: run_007_gemma3:4b (0197a8f1-c2af-75b2-bb8d-abf765481b48)\n",
      "Processing trace 180/240: run_007_gemma3:1b (0197a8e9-9938-7865-948f-d1d39dea9a54)\n",
      "Processing trace 181/240: run_006_qwen3:1.7b (0197a8e7-283d-7915-8ffa-eaae2c70b7da)\n",
      "Processing trace 182/240: run_006_qwen3:0.6b (0197a8e0-35d6-718a-84bc-7a0a79cb7d95)\n",
      "Processing trace 183/240: run_006_tinyllama:1.1b (0197a8db-63b9-7645-8678-d5803bcff48f)\n",
      "Processing trace 184/240: run_006_smollm2:1.7b (0197a8d6-c99e-70fc-b8d2-174b0f5d6fdc)\n",
      "Processing trace 185/240: run_006_moondream:1.8b (0197a8d6-ba82-729c-afaa-9b89018d3500)\n",
      "Processing trace 186/240: run_006_llama3.2:1b (0197a8d4-4edb-7b56-b760-cdc2538c448d)\n",
      "Processing trace 187/240: run_007_deepseek-r1:1.5b (0197a8d0-1049-7962-89dd-7234f93f46e2)\n",
      "Processing trace 188/240: run_006_gemma3:4b (0197a8ce-c5e6-7f74-8f15-2ba0b420ab45)\n",
      "Processing trace 189/240: run_006_gemma3:1b (0197a8c2-3b1c-7b38-a8c9-0f71eec91ed0)\n",
      "Processing trace 190/240: run_005_qwen3:4b (0197a8b0-6c27-76ac-89a6-dcde531ede78)\n",
      "Processing trace 191/240: run_005_qwen3:1.7b (0197a8ad-f594-7ca6-b169-669c8abab5b4)\n",
      "Processing trace 192/240: run_006_deepseek-r1:1.5b (0197a8a4-d3cc-798d-9586-f3b98379cb97)\n",
      "Processing trace 193/240: run_005_qwen3:0.6b (0197a88f-5c9b-7057-91c7-5aa2e16c9096)\n",
      "Processing trace 194/240: run_004_qwen3:4b (0197a88a-01d2-7b8c-a3ff-316ea696aadb)\n",
      "Processing trace 195/240: run_005_tinyllama:1.1b (0197a882-94eb-71bf-b17f-603e241dc9af)\n",
      "Processing trace 196/240: run_004_qwen3:1.7b (0197a877-3a6a-7254-9faa-9b6616191366)\n",
      "Processing trace 197/240: run_005_smollm2:1.7b (0197a872-8071-7af3-96a4-843d1143dc5f)\n",
      "Processing trace 198/240: run_005_moondream:1.8b (0197a872-7442-716b-b429-d86d8a25ba4b)\n",
      "Processing trace 199/240: run_005_llama3.2:1b (0197a86f-820f-732a-a461-2e5b7f2beda2)\n",
      "Processing trace 200/240: run_004_qwen3:0.6b (0197a86a-5a37-7251-9836-7fe4a038bff5)\n",
      "Processing trace 201/240: run_004_tinyllama:1.1b (0197a85f-865a-7d3b-8872-b010749dd254)\n",
      "Processing trace 202/240: run_005_gemma3:4b (0197a85f-099e-722a-8483-333c714054e4)\n",
      "Processing trace 203/240: run_004_smollm2:1.7b (0197a85b-9fe4-758c-b63d-7f0211a7fe9d)\n",
      "Processing trace 204/240: run_004_moondream:1.8b (0197a85b-9114-7a5c-bbed-011b726ae2ad)\n",
      "Processing trace 205/240: run_004_llama3.2:1b (0197a857-8942-777c-b5df-5bcffb8331c9)\n",
      "Processing trace 206/240: run_005_gemma3:1b (0197a857-0b4f-7695-82a5-c7c9bf3f3bd8)\n",
      "Processing trace 207/240: run_004_gemma3:4b (0197a852-7a7a-7a18-a092-f9c9467124c8)\n",
      "Processing trace 208/240: run_004_gemma3:1b (0197a84d-c47b-7330-b553-a679e274c778)\n",
      "Processing trace 209/240: run_005_deepseek-r1:1.5b (0197a839-6852-782c-ba1d-408fad0eeca5)\n",
      "Processing trace 210/240: run_004_deepseek-r1:1.5b (0197a836-58a5-7abd-a7d4-8f5f5419a68f)\n",
      "Processing trace 211/240: run_002_qwen3:4b (0197a81f-0667-7d00-af6c-a89adb689fce)\n",
      "Processing trace 212/240: run_002_qwen3:1.7b (0197a81a-d620-7b9c-9588-84230aab273a)\n",
      "Processing trace 213/240: run_003_qwen3:4b (0197a818-5af4-7dd4-8528-8e30aae366cc)\n",
      "Processing trace 214/240: run_003_qwen3:1.7b (0197a805-abb9-7540-975b-17c7229db0f8)\n",
      "Processing trace 215/240: run_002_qwen3:0.6b (0197a7fd-67d8-7d96-af9c-decc879d4168)\n",
      "Processing trace 216/240: run_003_qwen3:0.6b (0197a7fb-e9a2-79c1-b139-437ffd806189)\n",
      "Processing trace 217/240: run_003_tinyllama:1.1b (0197a7f7-f7f6-728a-b091-774f373d68be)\n",
      "Processing trace 218/240: run_002_tinyllama:1.1b (0197a7f7-873d-7607-95cd-468fc2d5f7c7)\n",
      "Processing trace 219/240: run_003_smollm2:1.7b (0197a7f3-a33c-7549-afeb-3ab48ca7b16e)\n",
      "Processing trace 220/240: run_003_moondream:1.8b (0197a7f3-9233-7f6e-b6b3-99e6909adffd)\n",
      "Processing trace 221/240: run_002_smollm2:1.7b (0197a7ec-6440-767d-8893-0a3a224da2a9)\n",
      "Processing trace 222/240: run_002_moondream:1.8b (0197a7ec-592b-7666-a734-207b8935123f)\n",
      "Processing trace 223/240: run_003_llama3.2:1b (0197a7eb-f921-725f-87df-cb66dcc43eb2)\n",
      "Processing trace 224/240: run_002_llama3.2:1b (0197a7e7-ef13-7c03-a6b3-3ac8efe173cf)\n",
      "Processing trace 225/240: run_003_gemma3:4b (0197a7e5-1005-74c9-8cb6-635b3d3185b4)\n",
      "Processing trace 226/240: run_003_gemma3:1b (0197a7de-f5c3-7f57-9dcb-7f76d9e3ffa3)\n",
      "Processing trace 227/240: run_002_gemma3:4b (0197a7d8-0ed3-727f-b572-c35cd4cf519f)\n",
      "Processing trace 228/240: run_002_gemma3:1b (0197a7cf-a563-795c-850c-8a721c7d1549)\n",
      "Processing trace 229/240: run_003_deepseek-r1:1.5b (0197a7c6-b010-7e6f-80fb-59cad618752e)\n",
      "Processing trace 230/240: run_002_deepseek-r1:1.5b (0197a7ac-ab4d-7b24-8c23-b8f7c6fbc86a)\n",
      "Processing trace 231/240: run_001_qwen3:4b (0197a7aa-eac2-72ff-a479-575ad448aee2)\n",
      "Processing trace 232/240: run_001_qwen3:1.7b (0197a7a1-3889-74c0-97ff-3fd3c18c1068)\n",
      "Processing trace 233/240: run_001_qwen3:0.6b (0197a79c-15a9-716f-a48c-041941c8a1fc)\n",
      "Processing trace 234/240: run_001_tinyllama:1.1b (0197a799-1151-781d-8fc5-3a47b9511a62)\n",
      "Processing trace 235/240: run_001_smollm2:1.7b (0197a796-cee9-77ef-be86-259760c9c5da)\n",
      "Processing trace 236/240: run_001_moondream:1.8b (0197a796-c547-7415-99a1-59ffaea33a2a)\n",
      "Processing trace 237/240: run_001_llama3.2:1b (0197a794-5c4d-701f-a21d-065174a3778e)\n",
      "Processing trace 238/240: run_001_gemma3:4b (0197a790-f02a-7019-a786-2df1d42ae2a5)\n",
      "Processing trace 239/240: run_001_gemma3:1b (0197a78e-7f43-7f9d-a5b4-ac7ed98d16f3)\n",
      "Processing trace 240/240: run_001_deepseek-r1:1.5b (0197a780-ab91-791b-9107-b1c8462c0f71)\n",
      "\n",
      "Full DataFrame loaded with shape: (24000, 19)\n",
      "DataFrame head:\n",
      "                                span_id span_name  response_time_ms  \\\n",
      "0  0197ad70-d09c-7292-bc99-cb017e7d2742   q10_r10            28.095   \n",
      "1  0197ad70-62dd-76e8-b694-d3a4d4f984da    q10_r9            28.102   \n",
      "2  0197ad6f-f517-7b18-b750-d5115bd695b3    q10_r8            28.119   \n",
      "3  0197ad6f-873f-780b-87f4-d055a8e903ac    q10_r7            28.105   \n",
      "4  0197ad6f-1976-7fc1-bdc7-e2822814d0e3    q10_r6            28.067   \n",
      "\n",
      "                                 span_input_question     span_output_answer  \\\n",
      "0  What is the sum of the numbers 111001011b and ...  [a][b][c][d][e][f][c]   \n",
      "1  What is the sum of the numbers 111001011b and ...  [a][b][c][d][e][f][c]   \n",
      "2  What is the sum of the numbers 111001011b and ...  [a][b][c][d][e][f][c]   \n",
      "3  What is the sum of the numbers 111001011b and ...  [a][b][c][d][e][f][c]   \n",
      "4  What is the sum of the numbers 111001011b and ...  [a][b][c][d][e][f][c]   \n",
      "\n",
      "                              span_output_raw_answer span_correct_answer  \\\n",
      "0  <think> Okay, let's see. The question is askin...                   c   \n",
      "1  <think> Okay, let's see. The question is askin...                   c   \n",
      "2  <think> Okay, let's see. The question is askin...                   c   \n",
      "3  <think> Okay, let's see. The question is askin...                   c   \n",
      "4  <think> Okay, let's see. The question is askin...                   c   \n",
      "\n",
      "     question_file                              trace_id          run_name  \\\n",
      "0  question_10.txt  0197ad5f-3790-7b31-bc21-2c8b7dacf46e  run_024_qwen3:4b   \n",
      "1  question_10.txt  0197ad5f-3790-7b31-bc21-2c8b7dacf46e  run_024_qwen3:4b   \n",
      "2  question_10.txt  0197ad5f-3790-7b31-bc21-2c8b7dacf46e  run_024_qwen3:4b   \n",
      "3  question_10.txt  0197ad5f-3790-7b31-bc21-2c8b7dacf46e  run_024_qwen3:4b   \n",
      "4  question_10.txt  0197ad5f-3790-7b31-bc21-2c8b7dacf46e  run_024_qwen3:4b   \n",
      "\n",
      "  model_display_name language prompting_tech  num_runs_per_question  \\\n",
      "0           Qwen3:4b       en             R4                     10   \n",
      "1           Qwen3:4b       en             R4                     10   \n",
      "2           Qwen3:4b       en             R4                     10   \n",
      "3           Qwen3:4b       en             R4                     10   \n",
      "4           Qwen3:4b       en             R4                     10   \n",
      "\n",
      "  model_source  temperature  top_p    exercise    question_type  \n",
      "0        local          0.4    0.1  exam_01_mc  multiple_choice  \n",
      "1        local          0.4    0.1  exam_01_mc  multiple_choice  \n",
      "2        local          0.4    0.1  exam_01_mc  multiple_choice  \n",
      "3        local          0.4    0.1  exam_01_mc  multiple_choice  \n",
      "4        local          0.4    0.1  exam_01_mc  multiple_choice  \n",
      "\n",
      "DataFrame shape: (24000, 19)\n",
      "\n",
      "Successfully extracted data and saved to ../../../data/determinism/opik_determinism_data.csv\n",
      "\n",
      "Filtered DataFrame for temperature = 0.0:\n",
      "DataFrame head:\n",
      "                                   span_id span_name  response_time_ms  \\\n",
      "2000  0197acdc-d028-796f-ad01-22ef66d179d1   q10_r10            28.122   \n",
      "2001  0197acdc-624e-77c2-93a8-ccf0643c8803    q10_r9            28.081   \n",
      "2002  0197acdb-f49d-79b2-b0c7-72086fd6bc94    q10_r8            28.085   \n",
      "2003  0197acdb-86e7-7d4b-a25f-5ce98e0160d9    q10_r7            28.105   \n",
      "2004  0197acdb-191f-7685-b69e-0fb5f0f2bda9    q10_r6            28.074   \n",
      "\n",
      "                                    span_input_question  \\\n",
      "2000  What is the sum of the numbers 111001011b and ...   \n",
      "2001  What is the sum of the numbers 111001011b and ...   \n",
      "2002  What is the sum of the numbers 111001011b and ...   \n",
      "2003  What is the sum of the numbers 111001011b and ...   \n",
      "2004  What is the sum of the numbers 111001011b and ...   \n",
      "\n",
      "         span_output_answer  \\\n",
      "2000  [a][b][c][d][e][f][c]   \n",
      "2001  [a][b][c][d][e][f][c]   \n",
      "2002  [a][b][c][d][e][f][c]   \n",
      "2003  [a][b][c][d][e][f][c]   \n",
      "2004  [a][b][c][d][e][f][c]   \n",
      "\n",
      "                                 span_output_raw_answer span_correct_answer  \\\n",
      "2000  <think> Okay, let's see. The question is askin...                   c   \n",
      "2001  <think> Okay, let's see. The question is askin...                   c   \n",
      "2002  <think> Okay, let's see. The question is askin...                   c   \n",
      "2003  <think> Okay, let's see. The question is askin...                   c   \n",
      "2004  <think> Okay, let's see. The question is askin...                   c   \n",
      "\n",
      "        question_file                              trace_id          run_name  \\\n",
      "2000  question_10.txt  0197accb-3a7c-7ecb-bb35-7bd1fdc63d6c  run_022_qwen3:4b   \n",
      "2001  question_10.txt  0197accb-3a7c-7ecb-bb35-7bd1fdc63d6c  run_022_qwen3:4b   \n",
      "2002  question_10.txt  0197accb-3a7c-7ecb-bb35-7bd1fdc63d6c  run_022_qwen3:4b   \n",
      "2003  question_10.txt  0197accb-3a7c-7ecb-bb35-7bd1fdc63d6c  run_022_qwen3:4b   \n",
      "2004  question_10.txt  0197accb-3a7c-7ecb-bb35-7bd1fdc63d6c  run_022_qwen3:4b   \n",
      "\n",
      "     model_display_name language prompting_tech  num_runs_per_question  \\\n",
      "2000           Qwen3:4b       en             R4                     10   \n",
      "2001           Qwen3:4b       en             R4                     10   \n",
      "2002           Qwen3:4b       en             R4                     10   \n",
      "2003           Qwen3:4b       en             R4                     10   \n",
      "2004           Qwen3:4b       en             R4                     10   \n",
      "\n",
      "     model_source  temperature  top_p    exercise    question_type  \n",
      "2000        local          0.0    0.1  exam_01_mc  multiple_choice  \n",
      "2001        local          0.0    0.1  exam_01_mc  multiple_choice  \n",
      "2002        local          0.0    0.1  exam_01_mc  multiple_choice  \n",
      "2003        local          0.0    0.1  exam_01_mc  multiple_choice  \n",
      "2004        local          0.0    0.1  exam_01_mc  multiple_choice  \n",
      "\n",
      "DataFrame shape: (8000, 19)\n",
      "\n",
      "Successfully extracted data and saved to ../../../data/determinism/opik_determinism_data_temp_00.csv\n",
      "\n",
      "Filtered DataFrame for temperature = 0.2:\n",
      "DataFrame head:\n",
      "                                   span_id span_name  response_time_ms  \\\n",
      "1000  0197ad27-220e-7c1b-9d73-210dd234a4e4   q10_r10            28.059   \n",
      "1001  0197ad26-b472-7c6c-a9f6-ce88e153cc14    q10_r9            28.067   \n",
      "1002  0197ad26-46d0-770f-bf9c-7273e61b08bc    q10_r8            28.093   \n",
      "1003  0197ad25-d912-7dcf-a559-0c5646d9ab79    q10_r7            28.060   \n",
      "1004  0197ad25-6b76-766f-861e-cf125797c30d    q10_r6            28.083   \n",
      "\n",
      "                                    span_input_question  \\\n",
      "1000  What is the sum of the numbers 111001011b and ...   \n",
      "1001  What is the sum of the numbers 111001011b and ...   \n",
      "1002  What is the sum of the numbers 111001011b and ...   \n",
      "1003  What is the sum of the numbers 111001011b and ...   \n",
      "1004  What is the sum of the numbers 111001011b and ...   \n",
      "\n",
      "         span_output_answer  \\\n",
      "1000  [a][b][c][d][e][f][c]   \n",
      "1001  [a][b][c][d][e][f][c]   \n",
      "1002  [a][b][c][d][e][f][c]   \n",
      "1003  [a][b][c][d][e][f][c]   \n",
      "1004  [a][b][c][d][e][f][c]   \n",
      "\n",
      "                                 span_output_raw_answer span_correct_answer  \\\n",
      "1000  <think> Okay, let's see. The question is askin...                   c   \n",
      "1001  <think> Okay, let's see. The question is askin...                   c   \n",
      "1002  <think> Okay, let's see. The question is askin...                   c   \n",
      "1003  <think> Okay, let's see. The question is askin...                   c   \n",
      "1004  <think> Okay, let's see. The question is askin...                   c   \n",
      "\n",
      "        question_file                              trace_id          run_name  \\\n",
      "1000  question_10.txt  0197ad15-8c3b-761c-bb22-ee65aee74ff7  run_023_qwen3:4b   \n",
      "1001  question_10.txt  0197ad15-8c3b-761c-bb22-ee65aee74ff7  run_023_qwen3:4b   \n",
      "1002  question_10.txt  0197ad15-8c3b-761c-bb22-ee65aee74ff7  run_023_qwen3:4b   \n",
      "1003  question_10.txt  0197ad15-8c3b-761c-bb22-ee65aee74ff7  run_023_qwen3:4b   \n",
      "1004  question_10.txt  0197ad15-8c3b-761c-bb22-ee65aee74ff7  run_023_qwen3:4b   \n",
      "\n",
      "     model_display_name language prompting_tech  num_runs_per_question  \\\n",
      "1000           Qwen3:4b       en             R4                     10   \n",
      "1001           Qwen3:4b       en             R4                     10   \n",
      "1002           Qwen3:4b       en             R4                     10   \n",
      "1003           Qwen3:4b       en             R4                     10   \n",
      "1004           Qwen3:4b       en             R4                     10   \n",
      "\n",
      "     model_source  temperature  top_p    exercise    question_type  \n",
      "1000        local          0.2    0.1  exam_01_mc  multiple_choice  \n",
      "1001        local          0.2    0.1  exam_01_mc  multiple_choice  \n",
      "1002        local          0.2    0.1  exam_01_mc  multiple_choice  \n",
      "1003        local          0.2    0.1  exam_01_mc  multiple_choice  \n",
      "1004        local          0.2    0.1  exam_01_mc  multiple_choice  \n",
      "\n",
      "DataFrame shape: (8000, 19)\n",
      "\n",
      "Successfully extracted data and saved to ../../../data/determinism/opik_determinism_data_temp_02.csv\n",
      "\n",
      "Filtered DataFrame for temperature = 0.4:\n",
      "DataFrame head:\n",
      "                                span_id span_name  response_time_ms  \\\n",
      "0  0197ad70-d09c-7292-bc99-cb017e7d2742   q10_r10            28.095   \n",
      "1  0197ad70-62dd-76e8-b694-d3a4d4f984da    q10_r9            28.102   \n",
      "2  0197ad6f-f517-7b18-b750-d5115bd695b3    q10_r8            28.119   \n",
      "3  0197ad6f-873f-780b-87f4-d055a8e903ac    q10_r7            28.105   \n",
      "4  0197ad6f-1976-7fc1-bdc7-e2822814d0e3    q10_r6            28.067   \n",
      "\n",
      "                                 span_input_question     span_output_answer  \\\n",
      "0  What is the sum of the numbers 111001011b and ...  [a][b][c][d][e][f][c]   \n",
      "1  What is the sum of the numbers 111001011b and ...  [a][b][c][d][e][f][c]   \n",
      "2  What is the sum of the numbers 111001011b and ...  [a][b][c][d][e][f][c]   \n",
      "3  What is the sum of the numbers 111001011b and ...  [a][b][c][d][e][f][c]   \n",
      "4  What is the sum of the numbers 111001011b and ...  [a][b][c][d][e][f][c]   \n",
      "\n",
      "                              span_output_raw_answer span_correct_answer  \\\n",
      "0  <think> Okay, let's see. The question is askin...                   c   \n",
      "1  <think> Okay, let's see. The question is askin...                   c   \n",
      "2  <think> Okay, let's see. The question is askin...                   c   \n",
      "3  <think> Okay, let's see. The question is askin...                   c   \n",
      "4  <think> Okay, let's see. The question is askin...                   c   \n",
      "\n",
      "     question_file                              trace_id          run_name  \\\n",
      "0  question_10.txt  0197ad5f-3790-7b31-bc21-2c8b7dacf46e  run_024_qwen3:4b   \n",
      "1  question_10.txt  0197ad5f-3790-7b31-bc21-2c8b7dacf46e  run_024_qwen3:4b   \n",
      "2  question_10.txt  0197ad5f-3790-7b31-bc21-2c8b7dacf46e  run_024_qwen3:4b   \n",
      "3  question_10.txt  0197ad5f-3790-7b31-bc21-2c8b7dacf46e  run_024_qwen3:4b   \n",
      "4  question_10.txt  0197ad5f-3790-7b31-bc21-2c8b7dacf46e  run_024_qwen3:4b   \n",
      "\n",
      "  model_display_name language prompting_tech  num_runs_per_question  \\\n",
      "0           Qwen3:4b       en             R4                     10   \n",
      "1           Qwen3:4b       en             R4                     10   \n",
      "2           Qwen3:4b       en             R4                     10   \n",
      "3           Qwen3:4b       en             R4                     10   \n",
      "4           Qwen3:4b       en             R4                     10   \n",
      "\n",
      "  model_source  temperature  top_p    exercise    question_type  \n",
      "0        local          0.4    0.1  exam_01_mc  multiple_choice  \n",
      "1        local          0.4    0.1  exam_01_mc  multiple_choice  \n",
      "2        local          0.4    0.1  exam_01_mc  multiple_choice  \n",
      "3        local          0.4    0.1  exam_01_mc  multiple_choice  \n",
      "4        local          0.4    0.1  exam_01_mc  multiple_choice  \n",
      "\n",
      "DataFrame shape: (8000, 19)\n",
      "\n",
      "Successfully extracted data and saved to ../../../data/determinism/opik_determinism_data_temp_04.csv\n"
     ]
    }
   ],
   "source": [
    "# Get all the data of Opik determinism project\n",
    "all_opik_data = get_opik_flat_data_for_csv()\n",
    "\n",
    "if all_opik_data:\n",
    "    full_df = pd.DataFrame(all_opik_data)\n",
    "    print(f\"\\nFull DataFrame loaded with shape: {full_df.shape}\")\n",
    "    \n",
    "    # Save full dataframe\n",
    "    filter_and_save_dataframe(full_df)\n",
    "\n",
    "    # Dataframe with temperature=0.0\n",
    "    filter_and_save_dataframe(full_df, csv_filename=\"opik_determinism_data_temp_00.csv\", temperature_filter=0.0)\n",
    "\n",
    "    # Dataframe with temperature=0.2\n",
    "    filter_and_save_dataframe(full_df, csv_filename=\"opik_determinism_data_temp_02.csv\", temperature_filter=0.2)\n",
    "    \n",
    "    # Dataframe with temperature=0.4\n",
    "    filter_and_save_dataframe(full_df, csv_filename=\"opik_determinism_data_temp_04.csv\", temperature_filter=0.4)\n",
    "\n",
    "else:\n",
    "    print(\"No data fetched from Opik to create any CSV files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f01819",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected '(' (1006903490.py, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[208], line 17\u001b[0;36m\u001b[0m\n\u001b[0;31m    def generate determinism_table(df, filename):\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected '('\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TEMPERATURE = 0.0\n",
    "csv_filename=\"opik_determinism_data_temp_00.csv\"\n",
    "temp_00_df = get_dataframe_from_csv(csv_filename=csv_filename)\n",
    "temp_00_mc_df = temp_00_df[temp_00_df['question_type'] == 'multiple_choice'].copy()\n",
    "temp_00_oa_df = temp_00_df[temp_00_df['question_type'] == 'open_answer'].copy()\n",
    "output_filename = \"determinism_table_temp_00_mc.csv\"\n",
    "generate_determinism_table_mc(temp_00_mc_df, filename=output_filename)\n",
    "\n",
    "# TEMPERATURE = 0.2\n",
    "csv_filename=\"opik_determinism_data_temp_02.csv\"\n",
    "temp_02_df = get_dataframe_from_csv(csv_filename=csv_filename)\n",
    "temp_02_mc_df = temp_02_df[temp_02_df['question_type'] == 'multiple_choice'].copy()\n",
    "temp_02_oa_df = temp_02_df[temp_02_df['question_type'] == 'open_answer'].copy()\n",
    "output_filename = \"determinism_table_temp_02_mc.csv\"\n",
    "generate_determinism_table_mc(temp_02_mc_df, filename=output_filename)\n",
    "\n",
    "# TEMPERATURE = 0.4\n",
    "csv_filename=\"opik_determinism_data_temp_04.csv\"\n",
    "temp_04_df = get_dataframe_from_csv(csv_filename=csv_filename)\n",
    "temp_04_mc_df = temp_04_df[temp_04_df['question_type'] == 'multiple_choice'].copy()\n",
    "temp_04_oa_df = temp_04_df[temp_04_df['question_type'] == 'open_answer'].copy()\n",
    "output_filename = \"determinism_table_temp_04_mc.csv\"\n",
    "generate_determinism_table_mc(temp_04_mc_df, filename=output_filename)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
