import os
import json
from collections import defaultdict
from ..settings import Settings

from rouge_score import rouge_scorer


def calculate_rouge(generated_text: str, reference_text: str) -> dict:
    """
    Calculate ROUGE scores between generated text and reference text.
    Returns a dictionary with ROUGE-1, ROUGE-2, and ROUGE-L F1 scores.
    Args:
        generated_text (str): The text generated by the model.
        reference_text (str): The reference text to compare against.
    Returns:
        dict: A dictionary containing ROUGE-1, ROUGE-2, and ROUGE-L F1 scores.
    """
    if not generated_text or not reference_text:
        return {'rouge1_f1': 0.0, 'rouge2_f1': 0.0, 'rougeL_f1': 0.0}

    # use_stemmer to help with stemming and improve matching
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    scores = scorer.score(reference_text, generated_text)

    return {
        'rouge1_f1': scores['rouge1'].fmeasure,
        'rouge2_f1': scores['rouge2'].fmeasure,
        'rougeL_f1': scores['rougeL'].fmeasure,
    }

def check_contains(generated_text: str, required_phrases: list[str], case_sensitive: bool = False) -> dict:
    """
    Verify if the generated text contains all required phrases.
    Returns a dictionary with the percentage of phrases contained and whether all phrases are present.
    Args:
        generated_text (str): The text generated by the model.
        required_phrases (list[str]): A list of phrases that must be contained in the generated text.
        case_sensitive (bool): Whether the check should be case-sensitive. Defaults to False.
    Returns:
        dict: A dictionary with 'contains_percentage' and 'all_contained' keys.
            - 'contains_percentage': Percentage of required phrases contained in the generated text.
            - 'all_contained': Boolean indicating if all required phrases are present.
    """
    if not generated_text or not required_phrases:
        return {"contains_percentage": 0.0, "all_contained": 0}

    contained_count = 0
    total_phrases = len(required_phrases)
    

    for phrase in required_phrases:
        if case_sensitive:
            is_contained = phrase in generated_text
        else:
            is_contained = phrase.lower() in generated_text.lower()
        if is_contained:
            contained_count += 1

    percentage = (contained_count / total_phrases) * 100 if total_phrases > 0 else 0
    all_contained = (contained_count == total_phrases)

    return {
        "contains_percentage": percentage,
        "all_contained": bool(all_contained)
    }



def load_model_answers(run_folder, model_id, question_file_prefix):
    model_path = os.path.join(run_folder, model_id)
    all_items = []
    for fname in sorted(os.listdir(model_path)):
        if fname.startswith(question_file_prefix) and fname.endswith(".json"):
            with open(os.path.join(model_path, fname), "r") as f:
                data = json.load(f)
                all_items.append((fname, data[1:]))
    return all_items

def group_by_question(data):
    grouped = defaultdict(list)
    for _, items in data:
        for entry in items:
            grouped[entry["question"]].append(entry)
    return grouped

def select_run(settings):
    base_folder = settings.experiments_folder
    print (f"Available runs in \033[1m{base_folder}\033[0m:")
    available_runs = sorted([
        d for d in os.listdir(base_folder)
        if os.path.isdir(os.path.join(base_folder, d))
    ])
    if not available_runs:
        print("No runs found.")
        return None, None
    print("Available runs:")
    for idx, run in enumerate(available_runs, 1):
        print(f"{idx}. {run}")
    run_choice = input("Enter a run number (default [1]): ").strip()
    run_idx = int(run_choice) - 1 if run_choice.isdigit() and 1 <= int(run_choice) <= len(available_runs) else 0
    selected_run = available_runs[run_idx]
    run_path = os.path.join(base_folder, selected_run)
    return selected_run, run_path

def select_model(selected_run, run_path):
    available_models = sorted([
        d for d in os.listdir(run_path)
        if os.path.isdir(os.path.join(run_path, d))
    ])
    if not available_models:
        print("No models found in this run.")
        return None, None
    print(f"\nAvailable models in \033[1m{selected_run}\033[0m:")
    for idx, model in enumerate(available_models, 1):
        print(f"{idx}. {model}")
    model_choice = input("Enter model number or 'A' to run all models (default [1]): ").strip()
    if model_choice.upper() == 'A':
        print("Selecting the first model for evaluation as 'A' option is not fully implemented yet.")
        model_idx = 0
    elif model_choice.isdigit() and 1 <= int(model_choice) <= len(available_models):
        model_idx = int(model_choice) - 1
    else:
        model_idx = 0
    model_id = available_models[model_idx]
    model_path = os.path.join(run_path, model_id)
    print(f"\nSelected model: \033[92m{model_id}\033[0m")
    return model_id, model_path



def main():
    settings = Settings()

    selected_run, run_path = select_run(settings)
    if not selected_run:
        return

    model_id, model_path = select_model(selected_run, run_path)
    if not model_id:
        return

    raw_data = load_model_answers(run_path, model_id, settings.question_file_prefix)
    grouped = group_by_question(raw_data)

    print(f"\n=== Evaluating model responses using ROUGE and Contains metrics ===")

    results = []
    for q_idx, (question, answers) in enumerate(grouped.items(), 1):
        print(f"\n{q_idx:02d}. \033[1mQuestion:\033[0m {question.strip()}")
        expected_answer = answers[0]["correct_answer"]
        print(f"    \033[96mExpected Answer:\033[0m {expected_answer.strip()}")

        required_phrases_for_contains = []
        for word in expected_answer.split():
            cleaned_word = word.strip()
            if cleaned_word:
                required_phrases_for_contains.append(cleaned_word)

        if len(required_phrases_for_contains) > 5:
            required_phrases_for_contains = required_phrases_for_contains[:5]


        for run_idx, run in enumerate(answers[:settings.num_runs_per_question], 1):
            model_output = run["raw_answer"]

            rouge_scores = calculate_rouge(model_output, expected_answer)
            contains_results = check_contains(model_output, required_phrases_for_contains)


            print(f"  Run {run_idx}:")
            print(f"    \033[93mModel Answer:\033[0m {model_output.strip()}")

            print("    --- ROUGE Scores ---")
            for metric_name, score_value in rouge_scores.items():
                print(f"      \033[92m{metric_name.replace('_', ' ').title()}:\033[0m {score_value:.4f}")

            print("    --- Contains Results ---")
            if "contains_percentage" in contains_results:
                print(f"      \033[92mContains Percentage:\033[0m {contains_results['contains_percentage']:.2f}")
            if "all_contained" in contains_results:
                print(f"      \033[92mAll Contained:\033[0m {bool(contains_results['all_contained'])}")


            overall_grade = "Pass"
            if rouge_scores.get("rougeL_f1", 0) < 0.2:
                overall_grade = "Fail (ROUGE-L Low)"
            elif not bool(contains_results.get("all_contained", 0)):
                overall_grade = "Fail (Contains Missing)"


            results.append({
                "question": question,
                "run": run_idx,
                "model_output": model_output,
                "expected_answer": expected_answer,
                "rouge_scores": rouge_scores,
                "contains_results": contains_results,
                "overall_grade": overall_grade
            })


    output_path = os.path.join(model_path, settings.evaluation_output_file)
    with open(output_path, "w") as f:
        json.dump(results, f, indent=2)

    print(f"\nEvaluation results saved to: {output_path}")

if __name__ == "__main__":
    main()